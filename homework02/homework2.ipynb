{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: CS344\n",
    "#### Ian Park"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam:\n",
      "0.9998979800040808\n",
      "Ham:\n",
      "0.00010201999591920018\n",
      "Mix Ham and Spam:\n",
      "0.4999999999999997\n",
      "Unseen word:\n",
      "0.4\n",
      "Unseen word with ham:\n",
      "0.25\n",
      "Unseen word with spam:\n",
      "0.9850746268656716\n",
      "Spam message 1:\n",
      "0.9999999895897965\n",
      "Spam message 2:\n",
      "0.999995877576386\n",
      "Non-Spam message 1:\n",
      "2.6025508824397714e-09\n",
      "Non-Spam message 2:\n",
      "0.3333333333333333\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "# Tokenized corpus of spam and nonspam mail. Each list in the lists represent tokens in a message.\n",
    "spam_corpus = [[\"i\", \"am\", \"spam\", \"spam\", \"i\", \"am\"], [\"i\", \"do\", \"not\", \"like\", \"that\", \"spamiam\"]]\n",
    "ham_corpus = [[\"do\", \"i\", \"like\", \"green\", \"eggs\", \"and\", \"ham\"], [\"i\", \"do\"]]\n",
    "\n",
    "# Receives corpora of spam and nonspam mails respectively\n",
    "# Returns a hashtable (dictionary) with probabilities given a corpus\n",
    "def spam_filter(spam_mail_corpus, nonspam_mail_corpus):\n",
    "    # Number of non-spam and spam messages respectively\n",
    "    # aka sizes of each corpus\n",
    "    ngood = len(nonspam_mail_corpus)\n",
    "    nbad = len(spam_mail_corpus)\n",
    "\n",
    "    # List of words of each corpus\n",
    "    # Iterate through each list of lists of words\n",
    "    good_tokens = [token for message in nonspam_mail_corpus for token in message]\n",
    "    bad_tokens = [token for message in spam_mail_corpus for token in message]\n",
    "\n",
    "    # Complete Tokens(Words)\n",
    "    # Get unique tokens through making the list a set\n",
    "    # Get the union between the sets\n",
    "    tokens = list(set(good_tokens) | set(bad_tokens))\n",
    "\n",
    "    # First Hashtable:\n",
    "    # A mapping of nonspam tokens to its number of occurrences\n",
    "    # Map from ham_corpus tokens(words) to number of occurrences (good)\n",
    "    good = get_token_occurrences_map(nonspam_mail_corpus, good_tokens, tokens)\n",
    "\n",
    "    # Second Hashtable:\n",
    "    # A mapping of spam tokens to its number of occurrences\n",
    "    # Map from spam_corpus tokens(words) to number of occurrences (bad)\n",
    "    bad = get_token_occurrences_map(spam_mail_corpus, bad_tokens, tokens)\n",
    "\n",
    "    # Third Hashtable:\n",
    "    # A mapping of each token to the probability that an\n",
    "    # email containing it is spam (algorithm given by Graham)\n",
    "    return get_token_spam_probability_map(good, bad, ngood, nbad, tokens)\n",
    "\n",
    "\n",
    "# Helper function to get the first and second hashtables\n",
    "# Returns a dictionary with each token(word) mapping to occurrences\n",
    "def get_token_occurrences_map(mail_corpus, message_tokens, tokens):\n",
    "    token_occurrences_map = {}\n",
    "    for token in tokens:\n",
    "       token_occurrences_map[token] = message_tokens.count(token)\n",
    "\n",
    "    return token_occurrences_map\n",
    "\n",
    "# Helper function to get the third hashtable\n",
    "# Returns a dictionary mapping each token to the probability that an email containing it is spam\n",
    "def get_token_spam_probability_map(good_token_occurrences_map, bad_token_occurrences_map, ngood, nbad, tokens):\n",
    "    token_spam_probability_map = {}\n",
    "    for token in tokens:\n",
    "        # Double good map to slightly avoid false-positives\n",
    "        g = float(2 * good_token_occurrences_map[token])\n",
    "        b = float(bad_token_occurrences_map[token])\n",
    "        # Check against min threshold\n",
    "        if g + b > 0.9:\n",
    "            # Divide by number of emails\n",
    "            token_spam_probability_map[token] = max(0.01, min(0.99, min(1.0, b/nbad) / (min(1.0, g/ngood) + min(1.0, b/nbad))))\n",
    "        else:\n",
    "            token_spam_probability_map[token] = 0\n",
    "\n",
    "    return token_spam_probability_map\n",
    "\n",
    "# Do the actual filtering given a probability map of words\n",
    "def is_spam(message, probability_map):\n",
    "    # Probability of tokens never seen previously\n",
    "    never_seen_probability = 0.4\n",
    "    # The product of the elements of probability\n",
    "    product = 1.0\n",
    "    # Complement product\n",
    "    complement_product = 1.0\n",
    "\n",
    "    # Algorithm given by Paul Graham\n",
    "    for token in message:\n",
    "        if token in probability_map:\n",
    "            probability = probability_map[token]\n",
    "        else:\n",
    "            probability = never_seen_probability\n",
    "        product *= probability\n",
    "        complement_product *= (1.0 - probability)\n",
    "\n",
    "    return product / (product + complement_product)\n",
    "\n",
    "# Get the probability map\n",
    "prob_map = spam_filter(spam_corpus, ham_corpus)\n",
    "\n",
    "# See probability of mail being spam\n",
    "print(\"Spam:\")\n",
    "print(is_spam([\"spam\", \"spamiam\"], prob_map)) # Spam\n",
    "print(\"Ham:\")\n",
    "print(is_spam([\"green\", \"eggs\"], prob_map)) # Ham\n",
    "print(\"Mix Ham and Spam:\")\n",
    "print(is_spam([\"spam\", \"ham\"], prob_map)) # Mix Ham and Spam\n",
    "print(\"Unseen word:\")\n",
    "print(is_spam([\"hehe\"], prob_map)) # Unseen word\n",
    "print(\"Unseen word with ham:\")\n",
    "print(is_spam([\"do\", \"hehe\"], prob_map)) # Unseen word with ham\n",
    "print(\"Unseen word with spam:\")\n",
    "\n",
    "print(is_spam([\"spam\", \"hehe\"], prob_map)) # Unseen word with spam\n",
    "print(\"Spam message 1:\")\n",
    "print(is_spam(spam_corpus[0], prob_map)) # Spam message 1\n",
    "print(\"Spam message 2:\")\n",
    "print(is_spam(spam_corpus[1], prob_map)) # Spam message 2\n",
    "print(\"Non-Spam message 1:\")\n",
    "print(is_spam(ham_corpus[0], prob_map)) # Non-spam message 1\n",
    "print(\"Non-Spam message 2:\")\n",
    "print(is_spam(ham_corpus[1], prob_map)) # Non-spam message 2\n",
    "\n",
    "print(\"DONE\")\n",
    "\n",
    "# TODO\n",
    "# Actual spam mail message with data from my spam inbox (including a token filter)\n",
    "# Actual working system (with aggregate filter additions and filter reruns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What makes this approach to SPAM Bayesian?**\n",
    "\n",
    "We are approaching this problem with probabilities and data (of initial frequencies and each resulting data) instead of approaching it with good-old fashioned programming that uses a set of rules to get our deterministic answer. As Graham explains, by representing our data(words) according to probabilities, we have a much more accuracy in knowing whether or not the mail containing the probabilities of each word is spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False: 0.5, True: 0.5\n",
      "False: 0.9, True: 0.1\n",
      "False: 0.952, True: 0.0476\n",
      "False: 0.01, True: 0.99\n",
      "False: 0.639, True: 0.361\n"
     ]
    }
   ],
   "source": [
    "from aima.probability import BayesNet, enumeration_ask, elimination_ask, gibbs_ask\n",
    "\n",
    "T, F = True, False\n",
    "\n",
    "grass = BayesNet([\n",
    "    ('Cloudy', '', 0.5),\n",
    "    ('Sprinkler', 'Cloudy', {T:0.1, F:0.5}),\n",
    "    ('Rain', 'Cloudy', {T:0.8, F:0.2}),\n",
    "    ('WetGrass', 'Sprinkler Rain', {(T, T):0.99, (T,F):0.90, (F,T):0.90, (F,F):0.00}),\n",
    "    ])\n",
    "\n",
    "# P(Cloudy)\n",
    "print(enumeration_ask('Cloudy', dict(), grass).show_approx())\n",
    "\n",
    "# P(Sprinker | cloudy)\n",
    "print(enumeration_ask('Sprinkler', dict(Cloudy=T), grass).show_approx())\n",
    "\n",
    "# P(Cloudy| the sprinkler is running and it’s not raining)\n",
    "print(enumeration_ask('Cloudy', dict(Sprinkler=T, Rain=F), grass).show_approx())\n",
    "\n",
    "# P(WetGrass | it’s cloudy, the sprinkler is running and it’s raining)\n",
    "print(enumeration_ask('WetGrass', dict(Cloudy=T, Sprinkler=T, Rain=T), grass).show_approx())\n",
    "\n",
    "# P(Cloudy | the grass is not wet)\n",
    "print(enumeration_ask('Cloudy', dict(WetGrass=F), grass).show_approx())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.b\n",
    "$2^4 = 16$ independent values.\n",
    "We have four different variables that can be either $True$ or $False$.\n",
    "\n",
    "## 2.c\n",
    "$9$ independent values.\n",
    "This is attained by looking at the Bayesian network figure and counting the number of independent values.\n",
    "\n",
    "## 2.d\n",
    "\n",
    "### i.\n",
    "From the Graph\n",
    "\n",
    "$P(\\text{Cloudy}) = <0.5, 0.5>$\n",
    "\n",
    "### ii. \n",
    "From the Graph\n",
    "\n",
    "$P(\\text{Sprinker  |  Cloudy}) = <0.1, 0.9>$\n",
    "\n",
    "### iii. \n",
    "\\begin{align}\n",
    "\\ P(\\text{Cloudy  |  the sprinkler is running and it’s not raining}) & = P(C | s \\land \\neg r) \\\\\n",
    "\\ & = \\alpha  \\times \\langle 0.50.10.2, 0.50.50.8 \\rangle \\\\\n",
    "\\ & = \\alpha \\times \\langle 0.01, 0.2 \\rangle \\\\\n",
    "\\ & = \\langle 0.0476, 0.9524 \\rangle \\\\\n",
    "\\end{align}\n",
    "\n",
    "### iv. \n",
    "\\begin{align}\n",
    "\\ P(\\text{WetGrass  |  it’s cloudy, the sprinkler is running and it’s raining}) & = \\alpha \\times \\langle P(W \\land c \\land s \\land r), P(\\neg W \\land c \\land s \\land r) \\rangle \\\\\n",
    "\\ & = \\alpha \\times \\langle 0.5 \\times 0.1 \\times 0.8 \\times 0.99, 0.5 \\times 0.1 \\times 0.8 \\times 0.01 \\rangle \\\\\n",
    "\\ & = \\alpha \\times \\langle 0.0396, 0.0004 \\rangle \\\\\n",
    "\\ & = \\langle 0.99, 0.01 \\rangle \\\\\n",
    "\\end{align}\n",
    "\n",
    "### v. \n",
    "\\begin{align}\n",
    "\\ P(\\text{Cloudy  |  the grass is not wet}) & = P(C | \\neg w) \\\\\n",
    "\\ & = \\alpha \\times \\langle \\sum_s ( \\sum_r ( P(C) \\times P(s \\land r) \\times P(g | s \\land r) ) ) \\rangle \\\\\n",
    "\\ & = \\alpha \\times \\langle 0.5 \\times 0.08 \\times 0.01 + 0.5 \\times 0.02 \\times 0.10 + 0.5 \\times 0.72 \\times 0.10 + 0.5 \\times 0.18 \\times 1.00,0.5 \\times 0.10 \\times 0.01 + 0.5 \\times 0.40 \\times 0.10 + 0.5 \\times 0.10 \\times 0.10 + 0.5 \\times 0.40 \\times 1.00 \\rangle \\\\\n",
    "\\ & = \\alpha \\times \\langle 0.1274, 0.2255 \\rangle \\\\\n",
    "\\ & = \\langle 0.361, 0.639 \\rangle \\\\\n",
    "\\end{align}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
