{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n",
      "0.99\n",
      "0.25\n"
     ]
    }
   ],
   "source": [
    "# Tokenized corpus of spam and nonspam mail. Each list in the lists represent tokens in a message.\n",
    "spam_corpus = [[\"i\", \"am\", \"spam\", \"spam\", \"i\", \"am\"], [\"i\", \"do\", \"not\", \"like\", \"that\", \"spamiam\"]]\n",
    "ham_corpus = [[\"do\", \"i\", \"like\", \"green\", \"eggs\", \"and\", \"ham\"], [\"i\", \"do\"]]\n",
    "\n",
    "# Receives corpora of spam and nonspam mails respectively\n",
    "# Returns a hashtable (dictionary) with probabilities given a corpus\n",
    "def spam_filter(spam_mail_corpus, nonspam_mail_corpus):\n",
    "    # Number of non-spam and spam messages respectively\n",
    "    # aka sizes of each corpus\n",
    "    ngood = len(nonspam_mail_corpus)\n",
    "    nbad = len(spam_mail_corpus)\n",
    "\n",
    "    # List of words of each corpus\n",
    "    # Iterate through each list of lists of words\n",
    "    good_tokens = [token for message in nonspam_mail_corpus for token in message]\n",
    "    bad_tokens = [token for message in spam_mail_corpus for token in message]\n",
    "\n",
    "    # Complete Tokens(Words)\n",
    "    # Get unique tokens through making the list a set\n",
    "    # Get the union between the sets\n",
    "    tokens = list(set(good_tokens) | set(bad_tokens))\n",
    "\n",
    "    # First Hashtable:\n",
    "    # A mapping of nonspam tokens to its number of occurrences\n",
    "    # Map from ham_corpus tokens(words) to number of occurrences (good)\n",
    "    good = get_token_occurrences_map(nonspam_mail_corpus, good_tokens, tokens)\n",
    "\n",
    "    # Second Hashtable:\n",
    "    # A mapping of spam tokens to its number of occurrences\n",
    "    # Map from spam_corpus tokens(words) to number of occurrences (bad)\n",
    "    bad = get_token_occurrences_map(spam_mail_corpus, bad_tokens, tokens)\n",
    "\n",
    "    # Third Hashtable:\n",
    "    # A mapping of each token to the probability that an\n",
    "    # email containing it is spam (algorithm given by Graham)\n",
    "    return get_token_spam_probability_map(good, bad, ngood, nbad, tokens)\n",
    "\n",
    "\n",
    "# Helper function to get the first and second hashtables\n",
    "# Returns a dictionary with each token(word) mapping to occurrences\n",
    "def get_token_occurrences_map(mail_corpus, message_tokens, tokens):\n",
    "    token_occurrences_map = {}\n",
    "    for token in tokens:\n",
    "       token_occurrences_map[token] = message_tokens.count(token)\n",
    "\n",
    "    return token_occurrences_map\n",
    "\n",
    "# Helper function to get the third hashtable\n",
    "# Returns a dictionary mapping each token to the probability that an email containing it is spam\n",
    "def get_token_spam_probability_map(good_token_occurrences_map, bad_token_occurrences_map, ngood, nbad, tokens):\n",
    "    token_spam_probability_map = {}\n",
    "    for token in tokens:\n",
    "        # Double good map to slightly avoid false-positives\n",
    "        g = float(2 * good_token_occurrences_map[token])\n",
    "        b = float(bad_token_occurrences_map[token])\n",
    "        # Check against min threshold\n",
    "        if g + b > 1:\n",
    "            # Divide by number of emails\n",
    "            token_spam_probability_map[token] = max(0.01, min(0.99, min(1.0, b/nbad) / (min(1.0, g/ngood) + min(1.0, b/nbad))))\n",
    "        else:\n",
    "            token_spam_probability_map[token] = 0\n",
    "\n",
    "    return token_spam_probability_map\n",
    "\n",
    "# Do the actual filtering given a probability map of words\n",
    "def is_spam(message, probability_map):\n",
    "    # Probability of tokens never seen previously\n",
    "    never_seen_probability = 0.4\n",
    "    # The product of the elements of probability\n",
    "    product = 1.0\n",
    "    # Complement product\n",
    "    complement_product = 1.0\n",
    "\n",
    "    # Algorithm given by Paul Graham\n",
    "    for token in message:\n",
    "        if token in probability_map:\n",
    "            probability = probability_map[token]\n",
    "        else:\n",
    "            probability = never_seen_probability\n",
    "        product *= probability\n",
    "        complement_product *= (1.0 - probability)\n",
    "\n",
    "    return product / (product + complement_product)\n",
    "\n",
    "# Get the probability map\n",
    "prob_map = spam_filter(spam_corpus, ham_corpus)\n",
    "\n",
    "# See probability of mail being spam\n",
    "print(is_spam([\"hehe\"], prob_map))\n",
    "print(is_spam([\"spam\"], prob_map))\n",
    "print(is_spam([\"do\", \"it\"], prob_map))\n",
    "\n",
    "# TODO\n",
    "# Actual spam mail message with data from my spam inbox (including a tokenizer)\n",
    "# Actual working system (with aggregate filter additions and filter reruns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False: 0.5, True: 0.5\n",
      "False: 0.9, True: 0.1\n",
      "False: 0.952, True: 0.0476\n",
      "False: 0.01, True: 0.99\n",
      "False: 0.639, True: 0.361\n"
     ]
    }
   ],
   "source": [
    "from aima.probability import BayesNet, enumeration_ask, elimination_ask, gibbs_ask\n",
    "\n",
    "T, F = True, False\n",
    "\n",
    "grass = BayesNet([\n",
    "    ('Cloudy', '', 0.5),\n",
    "    ('Sprinkler', 'Cloudy', {T:0.1, F:0.5}),\n",
    "    ('Rain', 'Cloudy', {T:0.8, F:0.2}),\n",
    "    ('WetGrass', 'Sprinkler Rain', {(T, T):0.99, (T,F):0.9, (F,T):0.9, (F,F):0.0}),\n",
    "    ])\n",
    "\n",
    "# P(Cloudy)\n",
    "print(enumeration_ask('Cloudy', dict(), grass).show_approx())\n",
    "\n",
    "# P(Sprinker | cloudy)\n",
    "print(enumeration_ask('Sprinkler', dict(Cloudy=T), grass).show_approx())\n",
    "\n",
    "# P(Cloudy| the sprinkler is running and it’s not raining)\n",
    "print(enumeration_ask('Cloudy', dict(Sprinkler=T, Rain=F), grass).show_approx())\n",
    "\n",
    "# P(WetGrass | it’s cloudy, the sprinkler is running and it’s raining)\n",
    "print(enumeration_ask('WetGrass', dict(Cloudy=T, Sprinkler=T, Rain=T), grass).show_approx())\n",
    "\n",
    "# P(Cloudy | the grass is not wet)\n",
    "print(enumeration_ask('Cloudy', dict(WetGrass=F), grass).show_approx())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
