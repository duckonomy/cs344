{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Probabilities: \n",
      "{'am': 0.99, 'spamiam': 0.99, 'ham': 0.01, 'eggs': 0.01, 'green': 0.01, 'like': 0.3333333333333333, 'not': 0.99, 'and': 0.01, 'that': 0.99, 'spam': 0.99, 'do': 0.3333333333333333, 'i': 0.5}\n",
      "Spam:\n",
      "0.9998979800040808\n",
      "Ham:\n",
      "0.00010201999591920018\n",
      "Mix Ham and Spam:\n",
      "0.4999999999999997\n",
      "Unseen word:\n",
      "0.4\n",
      "Unseen word with ham:\n",
      "0.25\n",
      "Unseen word with spam:\n",
      "0.9850746268656716\n",
      "Spam message 1:\n",
      "0.9999999895897965\n",
      "Spam message 2:\n",
      "0.999995877576386\n",
      "Non-Spam message 1:\n",
      "2.6025508824397714e-09\n",
      "Non-Spam message 2:\n",
      "0.3333333333333333\n",
      "Spam corpus:\n",
      "{'text0': 0.9999999895897965, 'text1': 0.999995877576386}\n",
      "Ham corpus:\n",
      "{'text0': 2.6025508824397714e-09, 'text1': 0.3333333333333333}\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "# Tokenized corpus of spam and nonspam mail. Each list in the lists represent tokens in a message.\n",
    "spam_corpus = [[\"i\", \"am\", \"spam\", \"spam\", \"i\", \"am\"], [\"i\", \"do\", \"not\", \"like\", \"that\", \"spamiam\"]]\n",
    "ham_corpus = [[\"do\", \"i\", \"like\", \"green\", \"eggs\", \"and\", \"ham\"], [\"i\", \"do\"]]\n",
    "\n",
    "# Receives corpora of spam and nonspam mails respectively\n",
    "# Returns a hashtable (dictionary) with probabilities given a corpus\n",
    "def spam_filter(spam_mail_corpus, nonspam_mail_corpus):\n",
    "    # Number of non-spam and spam messages respectively\n",
    "    # aka sizes of each corpus\n",
    "    ngood = len(nonspam_mail_corpus)\n",
    "    nbad = len(spam_mail_corpus)\n",
    "\n",
    "    # List of words of each corpus\n",
    "    # Iterate through each list of lists of words\n",
    "    good_tokens = [token for message in nonspam_mail_corpus for token in message]\n",
    "    bad_tokens = [token for message in spam_mail_corpus for token in message]\n",
    "\n",
    "    # Complete Tokens(Words)\n",
    "    # Get unique tokens through making the list a set\n",
    "    # Get the union between the sets\n",
    "    tokens = list(set(good_tokens) | set(bad_tokens))\n",
    "\n",
    "    # First Hashtable:\n",
    "    # A mapping of nonspam tokens to its number of occurrences\n",
    "    # Map from ham_corpus tokens(words) to number of occurrences (good)\n",
    "    good = get_token_occurrences_map(nonspam_mail_corpus, good_tokens, tokens)\n",
    "\n",
    "    # Second Hashtable:\n",
    "    # A mapping of spam tokens to its number of occurrences\n",
    "    # Map from spam_corpus tokens(words) to number of occurrences (bad)\n",
    "    bad = get_token_occurrences_map(spam_mail_corpus, bad_tokens, tokens)\n",
    "\n",
    "    # Third Hashtable:\n",
    "    # A mapping of each token to the probability that an\n",
    "    # email containing it is spam (algorithm given by Graham)\n",
    "    return get_token_spam_probability_map(good, bad, ngood, nbad, tokens)\n",
    "\n",
    "\n",
    "# Helper function to get the first and second hashtables\n",
    "# Returns a dictionary with each token(word) mapping to occurrences\n",
    "def get_token_occurrences_map(mail_corpus, message_tokens, tokens):\n",
    "    token_occurrences_map = {}\n",
    "    for token in tokens:\n",
    "       token_occurrences_map[token] = message_tokens.count(token)\n",
    "\n",
    "    return token_occurrences_map\n",
    "\n",
    "# Helper function to get the third hashtable\n",
    "# Returns a dictionary mapping each token to the probability that an email containing it is spam\n",
    "def get_token_spam_probability_map(good_token_occurrences_map, bad_token_occurrences_map, ngood, nbad, tokens):\n",
    "    token_spam_probability_map = {}\n",
    "    for token in tokens:\n",
    "        # Double good map to slightly avoid false-positives\n",
    "        g = float(2 * good_token_occurrences_map[token])\n",
    "        b = float(bad_token_occurrences_map[token])\n",
    "        # Check against min threshold\n",
    "        if g + b > 0.9:\n",
    "            # Divide by number of emails\n",
    "            token_spam_probability_map[token] = max(0.01, min(0.99, min(1.0, b/nbad) / (min(1.0, g/ngood) + min(1.0, b/nbad))))\n",
    "        else:\n",
    "            token_spam_probability_map[token] = 0\n",
    "\n",
    "    return token_spam_probability_map\n",
    "\n",
    "# Do the actual filtering given a probability map of words\n",
    "def is_spam(message, probability_map):\n",
    "    # Probability of tokens never seen previously\n",
    "    never_seen_probability = 0.4\n",
    "    # The product of the elements of probability\n",
    "    product = 1.0\n",
    "    # Complement product\n",
    "    complement_product = 1.0\n",
    "\n",
    "    # Algorithm given by Paul Graham\n",
    "    for token in message:\n",
    "        if token in probability_map:\n",
    "            probability = probability_map[token]\n",
    "        else:\n",
    "            probability = never_seen_probability\n",
    "        product *= probability\n",
    "        complement_product *= (1.0 - probability)\n",
    "\n",
    "    return product / (product + complement_product)\n",
    "\n",
    "prob_map = spam_filter(spam_corpus, ham_corpus)\n",
    "\n",
    "### ADDED THIS:\n",
    "# Print word probabilities\n",
    "print(\"Word Probabilities: \")\n",
    "print(prob_map)\n",
    "\n",
    "### ADDED THIS:\n",
    "# Wrapper for filter\n",
    "def run_this(message):\n",
    "    # Check if message is an entire corpus\n",
    "    if (isinstance(message[0], list)):\n",
    "        # Display probability for each message\n",
    "        message_probs = {}\n",
    "        for i in range(len(message)):\n",
    "            message_probs[\"Message \" + str(i)] = is_spam(message[i], prob_map)\n",
    "        return message_probs\n",
    "    else:\n",
    "        # Get the probability map\n",
    "        return is_spam(message, prob_map)\n",
    "\n",
    "\n",
    "# See probability of mail being spam\n",
    "print(\"Spam:\")\n",
    "print(run_this([\"spam\", \"spamiam\"])) # Spam\n",
    "\n",
    "print(\"Ham:\")\n",
    "print(run_this([\"green\", \"eggs\"])) # Ham\n",
    "\n",
    "print(\"Mix Ham and Spam:\")\n",
    "print(run_this([\"spam\", \"ham\"])) # Mix Ham and Spam\n",
    "\n",
    "print(\"Unseen word:\")\n",
    "print(run_this([\"hehe\"])) # Unseen word\n",
    "\n",
    "print(\"Unseen word with ham:\")\n",
    "print(run_this([\"do\", \"hehe\"])) # Unseen word with ham\n",
    "\n",
    "print(\"Unseen word with spam:\")\n",
    "print(run_this([\"spam\", \"hehe\"])) # Unseen word with spam\n",
    "\n",
    "print(\"Spam message 1:\")\n",
    "print(run_this(spam_corpus[0])) # Spam message 1\n",
    "\n",
    "print(\"Spam message 2:\")\n",
    "print(run_this(spam_corpus[1])) # Spam message 2\n",
    "\n",
    "print(\"Non-Spam message 1:\")\n",
    "print(run_this(ham_corpus[0])) # Non-spam message 1\n",
    "\n",
    "print(\"Non-Spam message 2:\")\n",
    "print(run_this(ham_corpus[1])) # Non-spam message 2\n",
    "\n",
    "### ADDED THIS:\n",
    "print(\"Spam corpus:\")\n",
    "print(run_this(spam_corpus))\n",
    "print(\"Ham corpus:\")\n",
    "print(run_this(ham_corpus))\n",
    "\n",
    "print(\"DONE\")\n",
    "\n",
    "# TODO\n",
    "# Actual spam mail message with data from my spam inbox (including a token filter)\n",
    "# Actual working system (with aggregate filter additions and filter reruns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
