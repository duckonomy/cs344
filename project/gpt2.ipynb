{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "10kR8Tt95mrM"
   },
   "source": [
    "# GPT-2 Model Fine-tuning for Korean Community Website Posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DISCLAIMER**: *I have primarily used a paid instance of Google Colab (https://colab.research.google.com/signup) for training my model. I have not tested this on my local system. So I cannot guarantee that this will run on any system. However, I did modify the code for weight import to account for running on a local system. On Colab you have to load it from a Google Drive to retain persistence*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nuK86kCaUATs"
   },
   "source": [
    "## Import and Use Data from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "colab_type": "code",
    "id": "QrnV2za0QWZY",
    "outputId": "fd92c754-4c7d-4803-f78d-a5ad41de09cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KTXIdKO2QtyQ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append('drive/My Drive/Colab Notebooks/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 964
    },
    "colab_type": "code",
    "id": "jkMqR6srQZYA",
    "outputId": "9983244e-10f4-4ccf-dfdf-5d07b752ef3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gluonnlp>=0.8.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c6/27/07b57d22496ed6c98b247e578712122402487f5c265ec70a747900f97060/gluonnlp-0.9.1.tar.gz (252kB)\n",
      "\u001b[K     |████████████████████████████████| 256kB 3.4MB/s \n",
      "\u001b[?25hCollecting mxnet\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/f5/d79b5b40735086ff1100c680703e0f3efc830fa455e268e9e96f3c857e93/mxnet-1.6.0-py2.py3-none-any.whl (68.7MB)\n",
      "\u001b[K     |████████████████████████████████| 68.7MB 49kB/s \n",
      "\u001b[?25hCollecting sentencepiece>=0.1.6\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/88/49e772d686088e1278766ad68a463513642a2a877487decbd691dec02955/sentencepiece-0.1.90-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 50.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/Colab Notebooks/KoGPT2/requirements.txt (line 4)) (1.5.0+cu101)\n",
      "Collecting transformers>=2.4.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/97/7db72a0beef1825f82188a4b923e62a146271ac2ced7928baa4d47ef2467/transformers-2.9.1-py3-none-any.whl (641kB)\n",
      "\u001b[K     |████████████████████████████████| 645kB 66.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from gluonnlp>=0.8.3->-r drive/My Drive/Colab Notebooks/KoGPT2/requirements.txt (line 1)) (1.18.4)\n",
      "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from gluonnlp>=0.8.3->-r drive/My Drive/Colab Notebooks/KoGPT2/requirements.txt (line 1)) (0.29.17)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from gluonnlp>=0.8.3->-r drive/My Drive/Colab Notebooks/KoGPT2/requirements.txt (line 1)) (20.3)\n",
      "Collecting graphviz<0.9.0,>=0.8.1\n",
      "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet->-r drive/My Drive/Colab Notebooks/KoGPT2/requirements.txt (line 2)) (2.23.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->-r drive/My Drive/Colab Notebooks/KoGPT2/requirements.txt (line 4)) (0.16.0)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 60.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.4.1->-r drive/My Drive/Colab Notebooks/KoGPT2/requirements.txt (line 5)) (4.41.1)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=2.4.1->-r drive/My Drive/Colab Notebooks/KoGPT2/requirements.txt (line 5)) (0.7)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.4.1->-r drive/My Drive/Colab Notebooks/KoGPT2/requirements.txt (line 5)) (2019.12.20)\n",
      "Collecting tokenizers==0.7.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8MB 56.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.4.1->-r drive/My Drive/Colab Notebooks/KoGPT2/requirements.txt (line 5)) (3.0.12)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp>=0.8.3->-r drive/My Drive/Colab Notebooks/KoGPT2/requirements.txt (line 1)) (1.12.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp>=0.8.3->-r drive/My Drive/Colab Notebooks/KoGPT2/requirements.txt (line 1)) (2.4.7)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r drive/My Drive/Colab Notebooks/KoGPT2/requirements.txt (line 2)) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r drive/My Drive/Colab Notebooks/KoGPT2/requirements.txt (line 2)) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r drive/My Drive/Colab Notebooks/KoGPT2/requirements.txt (line 2)) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r drive/My Drive/Colab Notebooks/KoGPT2/requirements.txt (line 2)) (2020.4.5.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.4.1->-r drive/My Drive/Colab Notebooks/KoGPT2/requirements.txt (line 5)) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.4.1->-r drive/My Drive/Colab Notebooks/KoGPT2/requirements.txt (line 5)) (0.15.0)\n",
      "Building wheels for collected packages: gluonnlp, sacremoses\n",
      "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for gluonnlp: filename=gluonnlp-0.9.1-cp36-cp36m-linux_x86_64.whl size=471079 sha256=8719a330469240ce444cfa609de67a32724daaf33f8253b491a1e2b94c0989ac\n",
      "  Stored in directory: /root/.cache/pip/wheels/af/60/16/1f8a40e68b85bd9bd7960e91830bca5e40cd113f3220b7e231\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=c54c9aef352b1f5c9234c888271513a1cdc4303d59ef28542f6e8873dbfe1588\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built gluonnlp sacremoses\n",
      "Installing collected packages: gluonnlp, graphviz, mxnet, sentencepiece, sacremoses, tokenizers, transformers\n",
      "  Found existing installation: graphviz 0.10.1\n",
      "    Uninstalling graphviz-0.10.1:\n",
      "      Successfully uninstalled graphviz-0.10.1\n",
      "Successfully installed gluonnlp-0.9.1 graphviz-0.8.4 mxnet-1.6.0 sacremoses-0.0.43 sentencepiece-0.1.90 tokenizers-0.7.0 transformers-2.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -r drive/'My Drive'/'Colab Notebooks'/KoGPT2/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "colab_type": "code",
    "id": "kV6xxdECm9qY",
    "outputId": "d8119d87-b5b2-4ab2-8bdd-0906049eff9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./drive/My Drive/Colab Notebooks/KoGPT2\n",
      "Building wheels for collected packages: kogpt2\n",
      "  Building wheel for kogpt2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for kogpt2: filename=kogpt2-0.1.0-cp36-none-any.whl size=22259 sha256=ca65d740256b3ec1fcb19d9dec42aeef94dd7af7c71cae9e3948336399cdda5f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-olv18w6m/wheels/13/fe/32/c11ad824f0076b67cb83a05ac071ca46ad18d8105c9ce3b1d1\n",
      "Successfully built kogpt2\n",
      "Installing collected packages: kogpt2\n",
      "Successfully installed kogpt2-0.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install drive/'My Drive'/'Colab Notebooks'/KoGPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "19JkW2ruSXcw"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import TensorDataset # 텐서데이터셋\n",
    "from torch.utils.data import Dataset,DataLoader # 데이터로더\n",
    "\n",
    "from kogpt2.utils import download, tokenizer, get_tokenizer\n",
    "from kogpt2.model.torch_gpt2 import GPT2Config, GPT2LMHeadModel\n",
    "\n",
    "from gluonnlp.data import SentencepieceTokenizer\n",
    "import gluonnlp\n",
    "import numpy as np\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RplgA9X2uF78"
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DIGVuIKQWVZ4"
   },
   "outputs": [],
   "source": [
    "def cleanup_data(data, data_type):\n",
    "    data_new = {}\n",
    "    data_title = {}\n",
    "    data_content = {}\n",
    "    j = 0\n",
    "\n",
    "    for i in data:\n",
    "        data_title[str(j)] = i['title']\n",
    "        content = i['content']\n",
    "        content = [c for c in content if 'http' not in c]\n",
    "        content = [c for c in content if '\\xa0' not in c]\n",
    "        content = [c for c in content if '\\n' not in c]\n",
    "        content = [c for c in content if '- dc official App' not in c]\n",
    "        content_str = ''.join(map(str, content))\n",
    "        data_content[str(j)] = content_str.strip()\n",
    "        j += 1\n",
    "\n",
    "    data_new['title'] = data_title\n",
    "    data_new['content'] = data_content\n",
    "\n",
    "    json_final = json.dumps(data_new, ensure_ascii=False)\n",
    "    df = pd.read_json(json_final)\n",
    "\n",
    "    if data_type == \"content\":\n",
    "        return df['content'].to_numpy()\n",
    "    else:\n",
    "        return df['title'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u6s09bA7Rl0M"
   },
   "outputs": [],
   "source": [
    "class PostDataset(Dataset):\n",
    "  \"\"\"\n",
    "  Community Post Dataset\n",
    "  \"\"\"\n",
    "  def __init__(self, file_path, vocab, tokenizer):\n",
    "    self.file_path = file_path\n",
    "    self.data = []\n",
    "    self.vocab = vocab\n",
    "    self.tokenizer = tokenizer\n",
    "\n",
    "    url = self.file_path\n",
    "    json_data = urlopen(url).read().decode('utf-8')\n",
    "    json_data = json.loads(json_data)\n",
    "\n",
    "    clean_data = cleanup_data(json_data, \"content\")\n",
    "\n",
    "\n",
    "    # Ensure utf-8 for Korean to load properly\n",
    "    for text in clean_data:\n",
    "      tokenized_line = tokenizer(text[:-1])\n",
    "      index_of_words = [vocab[vocab.bos_token],] + vocab[tokenized_line]+ [vocab[vocab.eos_token]]\n",
    "      self.data.append(index_of_words)\n",
    "\n",
    "    print(np.shape(self.data))\n",
    "\n",
    "    # file.close()\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "  def __getitem__(self, index):\n",
    "    item = self.data[index]\n",
    "    return self.data[index]\n",
    "    # print(item)\n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T3BsVBjPQyQM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kUtjUcsbSRyQ"
   },
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_08aa56yaDLE"
   },
   "source": [
    "## KoGPT2 Configuration (Code derived from Official KoGPT2 code [tweaked for fine-tuning])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ItHYQrb_bHlM"
   },
   "source": [
    "#### Basic Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EG923kddQ2_U"
   },
   "outputs": [],
   "source": [
    "ctx= 'cuda'\n",
    "cachedir='~/kogpt2/'\n",
    "save_path = 'drive/My Drive/Colab Notebooks/korean-post-generator/checkpoint/'\n",
    "\n",
    "pytorch_kogpt2 = {\n",
    "    'url':\n",
    "    'https://kobert.blob.core.windows.net/models/kogpt2/pytorch/pytorch_kogpt2_676e9bcfa7.params',\n",
    "    'fname': 'pytorch_kogpt2_676e9bcfa7.params',\n",
    "    'chksum': '676e9bcfa7'\n",
    "}\n",
    "kogpt2_config = {\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"layer_norm_epsilon\": 1e-05,\n",
    "    \"n_ctx\": 1024,\n",
    "    \"n_embd\": 768,\n",
    "    \"n_head\": 12,\n",
    "    \"n_layer\": 12,\n",
    "    \"n_positions\": 1024,\n",
    "    \"vocab_size\": 50000\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "On4aE0rEai9c"
   },
   "source": [
    "#### Download the Model and Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "-mWE1MW4Q6PI",
    "outputId": "c3e4e734-6feb-4bee-ae2c-06ca8870ab20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[██████████████████████████████████████████████████]\n",
      "[██████████████████████████████████████████████████]\n"
     ]
    }
   ],
   "source": [
    "model_info = pytorch_kogpt2\n",
    "model_path = download(model_info['url'],\n",
    "                       model_info['fname'],\n",
    "                       model_info['chksum'],\n",
    "                       cachedir=cachedir)\n",
    "\n",
    "vocab_info = tokenizer\n",
    "vocab_path = download(vocab_info['url'],\n",
    "                       vocab_info['fname'],\n",
    "                       vocab_info['chksum'],\n",
    "                       cachedir=cachedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uEDAl_BpaZqI"
   },
   "source": [
    "#### Model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i6cIw3KbQ8JA"
   },
   "outputs": [],
   "source": [
    "kogpt2model = GPT2LMHeadModel(config=GPT2Config.from_dict(kogpt2_config))\n",
    "kogpt2model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "device = torch.device(ctx)\n",
    "kogpt2model.to(device)\n",
    "\n",
    "kogpt2model.train()\n",
    "vocab_b_obj = gluonnlp.vocab.BERTVocab.from_sentencepiece(vocab_path,\n",
    "                                                     mask_token=None,\n",
    "                                                     sep_token=None,\n",
    "                                                     cls_token=None,\n",
    "                                                     unknown_token='<unk>',\n",
    "                                                     padding_token='<pad>',\n",
    "                                                     bos_token='<s>',\n",
    "                                                     eos_token='</s>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y_7qb9oWbUQI"
   },
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VnKlTondZVsI"
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "learning_rate = 1e-5\n",
    "epoch=200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "414OdP8kbW_I"
   },
   "source": [
    "#### Load Batch Data using DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "cgUlXIjqQ9_w",
    "outputId": "3dcc1610-4ab6-4efd-855a-ec35db7bf4cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n",
      "(14040,)\n"
     ]
    }
   ],
   "source": [
    "tok_path = get_tokenizer()\n",
    "model, vocab = kogpt2model, vocab_b_obj\n",
    "sentencepieceTokenizer = SentencepieceTokenizer(tok_path)\n",
    "\n",
    "data_path = 'https://raw.githubusercontent.com/duckonomy/cs344/master/project/api/dcinside.json'\n",
    "\n",
    "post_dataset = PostDataset(data_path, vocab, sentencepieceTokenizer)\n",
    "post_data_loader = DataLoader(post_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3blxrt7mbgb0"
   },
   "source": [
    "#### Compile Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VlmaDdOnRDZE"
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "tmp_post_tens = None\n",
    "models_folder = \"trained_models\"\n",
    "if not os.path.exists(models_folder):\n",
    "    os.mkdir(models_folder)\n",
    "\n",
    "proc_seq_count = 0\n",
    "sum_loss = 0.0\n",
    "batch_count = 0\n",
    "\n",
    "avg_loss = (0.0, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-fIOQ2docA4Y"
   },
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X1byuyuquEa0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def top_k_logits(logits, k):\n",
    "    if k == 0:\n",
    "        return logits\n",
    "    values, _ = torch.topk(logits, k)\n",
    "    min_values = values[:, -1]\n",
    "    return torch.where(logits < min_values, torch.ones_like(logits, dtype=logits.dtype) * -1e10, logits)\n",
    "\n",
    "\n",
    "def top_p_logits(logits, top_p=0.0, filter_value=-float('Inf')):\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        sorted_indices_to_remove = cumulative_probs >= top_p\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[:, indices_to_remove] = filter_value\n",
    "    return logits\n",
    "\n",
    "\n",
    "def sample_sequence(model, tok, vocab, sent, text_size, temperature, top_p, top_k):\n",
    "    toked = tok(sent)\n",
    "    count = 0\n",
    "    generated_text = ''\n",
    "\n",
    "    if len(toked) > 1022:\n",
    "        return 0\n",
    "\n",
    "    while 1:\n",
    "        input_ids = torch.tensor([vocab[vocab.bos_token], ] + vocab[toked]).unsqueeze(0)\n",
    "        predicts = model(input_ids)\n",
    "        pred = predicts[0]\n",
    "\n",
    "        logits = pred\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        logits = top_k_logits(logits, top_k)\n",
    "        logits = top_p_logits(logits, top_p=top_p)\n",
    "\n",
    "        log_probs = F.softmax(logits, dim=-1)\n",
    "        prev = torch.multinomial(log_probs, num_samples=1)\n",
    "\n",
    "        gen = vocab.to_tokens(prev.squeeze().tolist())\n",
    "\n",
    "        if gen == '</s>' or gen == '|' or count > text_size:\n",
    "            print('to_tokens:', vocab.to_tokens(torch.argmax(pred, axis=-1).squeeze().tolist()))\n",
    "            sent += gen.replace('▁', ' ')\n",
    "            generated_text += gen.replace('▁', ' ')\n",
    "            sent += '\\n'\n",
    "            generated_text += '\\n'\n",
    "            toked = tok(sent)\n",
    "            count = 0\n",
    "            break\n",
    "\n",
    "        sent += gen.replace('▁', ' ')\n",
    "        generated_text += gen.replace('▁', ' ')\n",
    "        toked = tok(sent)\n",
    "        count += 1\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nkrJWqGNvJSo"
   },
   "outputs": [],
   "source": [
    "def auto_enter(text):\n",
    "    text = (text.replace(\"   \", \"\\n\"))\n",
    "    text = text.split(\"\\n\")\n",
    "\n",
    "    text = [t.lstrip() for t in text if t != '']\n",
    "    return \"\\n\\n\".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "tvYtTbxmRHyU",
    "outputId": "48709d6f-5269-4632-c5c6-fc7b183e83e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "epoch no.7 train no.860  loss = 5.99233 avg_loss = 5.71201\n",
      "epoch no.7 train no.870  loss = 5.31178 avg_loss = 5.73766\n",
      "epoch no.7 train no.880  loss = 5.30597 avg_loss = 5.71235\n",
      "epoch no.7 train no.890  loss = 5.94274 avg_loss = 5.74801\n",
      "epoch no.7 train no.900  loss = 5.68206 avg_loss = 5.77670\n",
      "epoch no.7 train no.910  loss = 5.66289 avg_loss = 5.76023\n",
      "epoch no.7 train no.920  loss = 7.31908 avg_loss = 5.75084\n",
      "epoch no.7 train no.930  loss = 4.66240 avg_loss = 5.73119\n",
      "epoch no.7 train no.940  loss = 4.58455 avg_loss = 5.74912\n",
      "epoch no.7 train no.950  loss = 5.36464 avg_loss = 5.73247\n",
      "epoch no.7 train no.960  loss = 5.59571 avg_loss = 5.75186\n",
      "epoch no.7 train no.970  loss = 5.85894 avg_loss = 5.73124\n",
      "epoch no.7 train no.980  loss = 6.44532 avg_loss = 5.75419\n",
      "epoch no.7 train no.990  loss = 5.43130 avg_loss = 5.76800\n",
      "epoch no.7 train no.1000  loss = 4.95532 avg_loss = 5.76362\n",
      "to_tokens: ['</s>', '</s>', '</s>']\n",
      "뭐함</s>\n",
      "epoch no.7 train no.1010  loss = 5.00181 avg_loss = 5.73578\n",
      "epoch no.7 train no.1020  loss = 5.69627 avg_loss = 5.73567\n",
      "epoch no.7 train no.1030  loss = 6.51837 avg_loss = 5.73985\n",
      "epoch no.7 train no.1040  loss = 5.54145 avg_loss = 5.72518\n",
      "epoch no.7 train no.1050  loss = 4.33665 avg_loss = 5.73485\n",
      "epoch no.7 train no.1060  loss = 5.27198 avg_loss = 5.74641\n",
      "epoch no.7 train no.1070  loss = 5.20780 avg_loss = 5.78231\n",
      "epoch no.7 train no.1080  loss = 5.59814 avg_loss = 5.71368\n",
      "epoch no.7 train no.1090  loss = 6.12175 avg_loss = 5.72062\n",
      "epoch no.7 train no.1100  loss = 5.76057 avg_loss = 5.73706\n",
      "epoch no.7 train no.1110  loss = 6.35539 avg_loss = 5.72387\n",
      "epoch no.7 train no.1120  loss = 6.08669 avg_loss = 5.77230\n",
      "epoch no.7 train no.1130  loss = 4.48032 avg_loss = 5.76386\n",
      "epoch no.7 train no.1140  loss = 5.34522 avg_loss = 5.74549\n",
      "epoch no.7 train no.1150  loss = 5.63970 avg_loss = 5.72769\n",
      "epoch no.7 train no.1160  loss = 6.33403 avg_loss = 5.78286\n",
      "epoch no.7 train no.1170  loss = 4.87332 avg_loss = 5.78336\n",
      "epoch no.7 train no.1180  loss = 6.50870 avg_loss = 5.75987\n",
      "epoch no.7 train no.1190  loss = 6.64988 avg_loss = 5.79270\n",
      "epoch no.7 train no.1200  loss = 5.44365 avg_loss = 5.79247\n",
      "epoch no.7 train no.1210  loss = 5.29396 avg_loss = 5.79933\n",
      "epoch no.7 train no.1220  loss = 5.69935 avg_loss = 5.83111\n",
      "epoch no.7 train no.1230  loss = 6.12794 avg_loss = 5.84160\n",
      "epoch no.7 train no.1240  loss = 6.40576 avg_loss = 5.81269\n",
      "epoch no.7 train no.1250  loss = 6.85063 avg_loss = 5.82614\n",
      "epoch no.7 train no.1260  loss = 7.50238 avg_loss = 5.81785\n",
      "epoch no.7 train no.1270  loss = 6.08255 avg_loss = 5.81954\n",
      "epoch no.7 train no.1280  loss = 5.43373 avg_loss = 5.84588\n",
      "epoch no.7 train no.1290  loss = 5.74148 avg_loss = 5.80812\n",
      "epoch no.7 train no.1300  loss = 4.98463 avg_loss = 5.78399\n",
      "epoch no.7 train no.1310  loss = 6.26032 avg_loss = 5.77513\n",
      "epoch no.7 train no.1320  loss = 6.25893 avg_loss = 5.77456\n",
      "epoch no.7 train no.1330  loss = 5.90961 avg_loss = 5.80067\n",
      "epoch no.7 train no.1340  loss = 6.05686 avg_loss = 5.77753\n",
      "epoch no.7 train no.1350  loss = 6.10621 avg_loss = 5.77978\n",
      "epoch no.7 train no.1360  loss = 7.32515 avg_loss = 5.79976\n",
      "epoch no.7 train no.1370  loss = 6.62918 avg_loss = 5.77557\n",
      "epoch no.7 train no.1380  loss = 6.32961 avg_loss = 5.77470\n",
      "epoch no.7 train no.1390  loss = 6.02429 avg_loss = 5.78412\n",
      "epoch no.7 train no.1400  loss = 6.15792 avg_loss = 5.77993\n",
      "epoch no.7 train no.1410  loss = 5.23740 avg_loss = 5.70350\n",
      "epoch no.7 train no.1420  loss = 5.65335 avg_loss = 5.68345\n",
      "epoch no.7 train no.1430  loss = 5.11959 avg_loss = 5.69785\n",
      "epoch no.7 train no.1440  loss = 5.37731 avg_loss = 5.66476\n",
      "epoch no.7 train no.1450  loss = 6.41719 avg_loss = 5.70174\n",
      "epoch no.7 train no.1460  loss = 5.15824 avg_loss = 5.69351\n",
      "epoch no.7 train no.1470  loss = 7.63285 avg_loss = 5.72071\n",
      "epoch no.7 train no.1480  loss = 5.82554 avg_loss = 5.74870\n",
      "epoch no.7 train no.1490  loss = 5.11766 avg_loss = 5.78689\n",
      "epoch no.7 train no.1500  loss = 4.37805 avg_loss = 5.78338\n",
      "epoch no.7 train no.1510  loss = 6.26448 avg_loss = 5.77450\n",
      "epoch no.7 train no.1520  loss = 5.40613 avg_loss = 5.84790\n",
      "epoch no.7 train no.1530  loss = 5.40627 avg_loss = 5.84359\n",
      "epoch no.7 train no.1540  loss = 6.36603 avg_loss = 5.85199\n",
      "epoch no.7 train no.1550  loss = 4.83677 avg_loss = 5.79498\n",
      "epoch no.7 train no.1560  loss = 5.49141 avg_loss = 5.77574\n",
      "epoch no.7 train no.1570  loss = 4.81724 avg_loss = 5.78797\n",
      "epoch no.7 train no.1580  loss = 5.02642 avg_loss = 5.79506\n",
      "epoch no.7 train no.1590  loss = 5.88208 avg_loss = 5.77840\n",
      "epoch no.7 train no.1600  loss = 5.99579 avg_loss = 5.76554\n",
      "epoch no.7 train no.1610  loss = 5.60000 avg_loss = 5.74214\n",
      "epoch no.7 train no.1620  loss = 5.55588 avg_loss = 5.69685\n",
      "epoch no.7 train no.1630  loss = 5.39620 avg_loss = 5.70972\n",
      "epoch no.7 train no.1640  loss = 5.63266 avg_loss = 5.72486\n",
      "epoch no.7 train no.1650  loss = 5.90716 avg_loss = 5.70629\n",
      "epoch no.7 train no.1660  loss = 6.71665 avg_loss = 5.71826\n",
      "epoch no.7 train no.1670  loss = 5.02591 avg_loss = 5.67738\n",
      "epoch no.7 train no.1680  loss = 5.97280 avg_loss = 5.68542\n",
      "epoch no.7 train no.1690  loss = 5.69317 avg_loss = 5.67567\n",
      "epoch no.7 train no.1700  loss = 6.38441 avg_loss = 5.72360\n",
      "epoch no.7 train no.1710  loss = 6.65756 avg_loss = 5.72937\n",
      "epoch no.7 train no.1720  loss = 5.62438 avg_loss = 5.74685\n",
      "epoch no.7 train no.1730  loss = 5.77074 avg_loss = 5.79660\n",
      "epoch no.7 train no.1740  loss = 6.79513 avg_loss = 5.82073\n",
      "epoch no.7 train no.1750  loss = 5.26894 avg_loss = 5.80142\n",
      "epoch no.8 train no.0  loss = 5.23295 avg_loss = 5.80934\n",
      "epoch no.8 train no.10  loss = 4.35960 avg_loss = 5.82389\n",
      "epoch no.8 train no.20  loss = 6.43748 avg_loss = 5.75724\n",
      "epoch no.8 train no.30  loss = 5.38323 avg_loss = 5.76665\n",
      "epoch no.8 train no.40  loss = 6.52885 avg_loss = 5.74674\n",
      "epoch no.8 train no.50  loss = 6.43972 avg_loss = 5.72231\n",
      "epoch no.8 train no.60  loss = 6.52812 avg_loss = 5.69620\n",
      "epoch no.8 train no.70  loss = 5.71530 avg_loss = 5.73313\n",
      "epoch no.8 train no.80  loss = 4.25649 avg_loss = 5.72891\n",
      "epoch no.8 train no.90  loss = 6.10313 avg_loss = 5.72089\n",
      "epoch no.8 train no.100  loss = 5.31939 avg_loss = 5.72300\n",
      "epoch no.8 train no.110  loss = 3.56350 avg_loss = 5.72725\n",
      "epoch no.8 train no.120  loss = 4.11368 avg_loss = 5.74412\n",
      "epoch no.8 train no.130  loss = 4.18168 avg_loss = 5.76511\n",
      "epoch no.8 train no.140  loss = 5.06741 avg_loss = 5.75458\n",
      "epoch no.8 train no.150  loss = 7.14856 avg_loss = 5.76005\n",
      "epoch no.8 train no.160  loss = 5.33086 avg_loss = 5.76972\n",
      "epoch no.8 train no.170  loss = 6.66487 avg_loss = 5.75584\n",
      "epoch no.8 train no.180  loss = 3.68026 avg_loss = 5.74589\n",
      "epoch no.8 train no.190  loss = 5.16729 avg_loss = 5.75792\n",
      "epoch no.8 train no.200  loss = 4.25046 avg_loss = 5.77744\n",
      "epoch no.8 train no.210  loss = 5.42553 avg_loss = 5.77967\n",
      "epoch no.8 train no.220  loss = 5.50733 avg_loss = 5.79830\n",
      "epoch no.8 train no.230  loss = 5.44679 avg_loss = 5.76981\n",
      "epoch no.8 train no.240  loss = 6.28537 avg_loss = 5.70597\n",
      "epoch no.8 train no.250  loss = 4.24574 avg_loss = 5.71863\n",
      "epoch no.8 train no.260  loss = 6.22686 avg_loss = 5.73752\n",
      "epoch no.8 train no.270  loss = 3.68850 avg_loss = 5.68466\n",
      "epoch no.8 train no.280  loss = 4.55566 avg_loss = 5.71109\n",
      "epoch no.8 train no.290  loss = 5.67889 avg_loss = 5.74484\n",
      "epoch no.8 train no.300  loss = 4.69108 avg_loss = 5.71853\n",
      "epoch no.8 train no.310  loss = 4.51465 avg_loss = 5.69818\n",
      "epoch no.8 train no.320  loss = 4.32781 avg_loss = 5.60309\n",
      "epoch no.8 train no.330  loss = 5.67840 avg_loss = 5.60778\n",
      "epoch no.8 train no.340  loss = 5.61850 avg_loss = 5.66360\n",
      "epoch no.8 train no.350  loss = 5.41587 avg_loss = 5.68817\n",
      "epoch no.8 train no.360  loss = 7.01655 avg_loss = 5.73194\n",
      "epoch no.8 train no.370  loss = 4.10706 avg_loss = 5.74494\n",
      "epoch no.8 train no.380  loss = 6.54103 avg_loss = 5.74730\n",
      "epoch no.8 train no.390  loss = 4.20831 avg_loss = 5.68653\n",
      "epoch no.8 train no.400  loss = 6.04836 avg_loss = 5.71540\n",
      "epoch no.8 train no.410  loss = 4.41301 avg_loss = 5.73735\n",
      "epoch no.8 train no.420  loss = 5.28206 avg_loss = 5.74721\n",
      "epoch no.8 train no.430  loss = 5.44975 avg_loss = 5.77366\n",
      "epoch no.8 train no.440  loss = 7.41783 avg_loss = 5.78172\n",
      "epoch no.8 train no.450  loss = 4.83171 avg_loss = 5.83495\n",
      "epoch no.8 train no.460  loss = 6.87639 avg_loss = 5.79873\n",
      "epoch no.8 train no.470  loss = 6.66888 avg_loss = 5.82214\n",
      "epoch no.8 train no.480  loss = 5.31360 avg_loss = 5.81893\n",
      "epoch no.8 train no.490  loss = 5.21063 avg_loss = 5.82372\n",
      "epoch no.8 train no.500  loss = 5.31226 avg_loss = 5.85005\n",
      "epoch no.8 train no.510  loss = 4.62258 avg_loss = 5.84545\n",
      "epoch no.8 train no.520  loss = 5.32797 avg_loss = 5.81388\n",
      "epoch no.8 train no.530  loss = 6.34322 avg_loss = 5.84003\n",
      "epoch no.8 train no.540  loss = 5.88425 avg_loss = 5.83525\n",
      "epoch no.8 train no.550  loss = 5.36672 avg_loss = 5.81675\n",
      "epoch no.8 train no.560  loss = 7.28015 avg_loss = 5.84883\n",
      "epoch no.8 train no.570  loss = 6.26248 avg_loss = 5.87150\n",
      "epoch no.8 train no.580  loss = 7.20162 avg_loss = 5.86381\n",
      "epoch no.8 train no.590  loss = 6.04550 avg_loss = 5.86537\n",
      "epoch no.8 train no.600  loss = 5.84495 avg_loss = 5.82781\n",
      "epoch no.8 train no.610  loss = 6.18737 avg_loss = 5.86766\n",
      "epoch no.8 train no.620  loss = 5.33598 avg_loss = 5.80509\n",
      "epoch no.8 train no.630  loss = 4.29289 avg_loss = 5.75696\n",
      "epoch no.8 train no.640  loss = 5.81093 avg_loss = 5.75041\n",
      "epoch no.8 train no.650  loss = 5.71992 avg_loss = 5.76043\n",
      "epoch no.8 train no.660  loss = 5.61887 avg_loss = 5.77736\n",
      "epoch no.8 train no.670  loss = 5.52144 avg_loss = 5.77367\n",
      "epoch no.8 train no.680  loss = 6.00419 avg_loss = 5.79415\n",
      "epoch no.8 train no.690  loss = 6.17062 avg_loss = 5.76301\n",
      "epoch no.8 train no.700  loss = 6.74582 avg_loss = 5.78121\n",
      "epoch no.8 train no.710  loss = 5.82954 avg_loss = 5.78429\n",
      "epoch no.8 train no.720  loss = 6.57319 avg_loss = 5.79735\n",
      "epoch no.8 train no.730  loss = 6.48589 avg_loss = 5.78207\n",
      "epoch no.8 train no.740  loss = 7.46459 avg_loss = 5.80196\n",
      "epoch no.8 train no.750  loss = 6.24912 avg_loss = 5.81065\n",
      "epoch no.8 train no.760  loss = 4.21720 avg_loss = 5.78257\n",
      "epoch no.8 train no.770  loss = 6.01502 avg_loss = 5.81660\n",
      "epoch no.8 train no.780  loss = 5.87001 avg_loss = 5.81592\n",
      "epoch no.8 train no.790  loss = 6.22305 avg_loss = 5.78870\n",
      "epoch no.8 train no.800  loss = 6.16397 avg_loss = 5.81204\n",
      "epoch no.8 train no.810  loss = 6.49034 avg_loss = 5.81068\n",
      "epoch no.8 train no.820  loss = 5.67963 avg_loss = 5.83472\n",
      "epoch no.8 train no.830  loss = 4.80843 avg_loss = 5.84142\n",
      "epoch no.8 train no.840  loss = 7.19987 avg_loss = 5.87610\n",
      "epoch no.8 train no.850  loss = 4.93353 avg_loss = 5.83394\n",
      "epoch no.8 train no.860  loss = 3.80008 avg_loss = 5.84131\n",
      "epoch no.8 train no.870  loss = 3.38572 avg_loss = 5.81182\n",
      "epoch no.8 train no.880  loss = 5.39300 avg_loss = 5.85056\n",
      "epoch no.8 train no.890  loss = 7.06450 avg_loss = 5.85937\n",
      "epoch no.8 train no.900  loss = 6.49960 avg_loss = 5.83280\n",
      "epoch no.8 train no.910  loss = 4.90561 avg_loss = 5.80544\n",
      "epoch no.8 train no.920  loss = 7.10106 avg_loss = 5.81603\n",
      "epoch no.8 train no.930  loss = 5.57270 avg_loss = 5.77546\n",
      "epoch no.8 train no.940  loss = 4.15654 avg_loss = 5.77561\n",
      "epoch no.8 train no.950  loss = 5.75499 avg_loss = 5.75204\n",
      "epoch no.8 train no.960  loss = 7.41614 avg_loss = 5.77301\n",
      "epoch no.8 train no.970  loss = 5.83900 avg_loss = 5.76580\n",
      "epoch no.8 train no.980  loss = 3.16390 avg_loss = 5.72583\n",
      "epoch no.8 train no.990  loss = 6.28759 avg_loss = 5.75951\n",
      "epoch no.8 train no.1000  loss = 6.15716 avg_loss = 5.78354\n",
      "to_tokens: ['</s>', '</s>', '</s>']\n",
      "뭐함</s>\n",
      "epoch no.8 train no.1010  loss = 5.39515 avg_loss = 5.80641\n",
      "epoch no.8 train no.1020  loss = 4.49607 avg_loss = 5.76596\n",
      "epoch no.8 train no.1030  loss = 5.85465 avg_loss = 5.75241\n",
      "epoch no.8 train no.1040  loss = 5.09898 avg_loss = 5.71653\n",
      "epoch no.8 train no.1050  loss = 6.98804 avg_loss = 5.73598\n",
      "epoch no.8 train no.1060  loss = 6.27089 avg_loss = 5.70354\n",
      "epoch no.8 train no.1070  loss = 5.49113 avg_loss = 5.72771\n",
      "epoch no.8 train no.1080  loss = 6.82892 avg_loss = 5.74437\n",
      "epoch no.8 train no.1090  loss = 5.82037 avg_loss = 5.75560\n",
      "epoch no.8 train no.1100  loss = 6.39410 avg_loss = 5.75696\n",
      "epoch no.8 train no.1110  loss = 5.87807 avg_loss = 5.74472\n",
      "epoch no.8 train no.1120  loss = 6.18322 avg_loss = 5.70421\n",
      "epoch no.8 train no.1130  loss = 7.63714 avg_loss = 5.76009\n",
      "epoch no.8 train no.1140  loss = 5.43900 avg_loss = 5.77848\n",
      "epoch no.8 train no.1150  loss = 6.07499 avg_loss = 5.77057\n",
      "epoch no.8 train no.1160  loss = 5.56252 avg_loss = 5.79870\n",
      "epoch no.8 train no.1170  loss = 6.58076 avg_loss = 5.74780\n",
      "epoch no.8 train no.1180  loss = 5.38606 avg_loss = 5.69780\n",
      "epoch no.8 train no.1190  loss = 5.83558 avg_loss = 5.67591\n",
      "epoch no.8 train no.1200  loss = 5.43931 avg_loss = 5.67755\n",
      "epoch no.8 train no.1210  loss = 5.62503 avg_loss = 5.61842\n",
      "epoch no.8 train no.1220  loss = 5.40113 avg_loss = 5.58502\n",
      "epoch no.8 train no.1230  loss = 4.94264 avg_loss = 5.58729\n",
      "epoch no.8 train no.1240  loss = 6.33932 avg_loss = 5.59605\n",
      "epoch no.8 train no.1250  loss = 6.42053 avg_loss = 5.63578\n",
      "epoch no.8 train no.1260  loss = 6.11484 avg_loss = 5.63407\n",
      "epoch no.8 train no.1270  loss = 6.03738 avg_loss = 5.63893\n",
      "epoch no.8 train no.1280  loss = 6.34517 avg_loss = 5.64427\n",
      "epoch no.8 train no.1290  loss = 5.33051 avg_loss = 5.62118\n",
      "epoch no.8 train no.1300  loss = 5.54610 avg_loss = 5.59987\n",
      "epoch no.8 train no.1310  loss = 4.63305 avg_loss = 5.60766\n",
      "epoch no.8 train no.1320  loss = 5.34879 avg_loss = 5.61305\n",
      "epoch no.8 train no.1330  loss = 5.64283 avg_loss = 5.61242\n",
      "epoch no.8 train no.1340  loss = 4.11258 avg_loss = 5.62230\n",
      "epoch no.8 train no.1350  loss = 6.75982 avg_loss = 5.61221\n",
      "epoch no.8 train no.1360  loss = 3.95367 avg_loss = 5.62709\n",
      "epoch no.8 train no.1370  loss = 5.66237 avg_loss = 5.58326\n",
      "epoch no.8 train no.1380  loss = 4.68950 avg_loss = 5.62803\n",
      "epoch no.8 train no.1390  loss = 5.78306 avg_loss = 5.68531\n",
      "epoch no.8 train no.1400  loss = 5.39181 avg_loss = 5.68370\n",
      "epoch no.8 train no.1410  loss = 5.73081 avg_loss = 5.67608\n",
      "epoch no.8 train no.1420  loss = 3.83519 avg_loss = 5.63705\n",
      "epoch no.8 train no.1430  loss = 5.92719 avg_loss = 5.65928\n",
      "epoch no.8 train no.1440  loss = 4.68538 avg_loss = 5.59780\n",
      "epoch no.8 train no.1450  loss = 6.29392 avg_loss = 5.58492\n",
      "epoch no.8 train no.1460  loss = 5.38862 avg_loss = 5.58936\n",
      "epoch no.8 train no.1470  loss = 5.91624 avg_loss = 5.59480\n",
      "epoch no.8 train no.1480  loss = 6.99588 avg_loss = 5.62254\n",
      "epoch no.8 train no.1490  loss = 6.02220 avg_loss = 5.65382\n",
      "epoch no.8 train no.1500  loss = 5.34876 avg_loss = 5.65961\n",
      "epoch no.8 train no.1510  loss = 6.09707 avg_loss = 5.66268\n",
      "epoch no.8 train no.1520  loss = 4.96942 avg_loss = 5.66580\n",
      "epoch no.8 train no.1530  loss = 6.59236 avg_loss = 5.64705\n",
      "epoch no.8 train no.1540  loss = 5.45695 avg_loss = 5.67117\n",
      "epoch no.8 train no.1550  loss = 6.00349 avg_loss = 5.71866\n",
      "epoch no.8 train no.1560  loss = 4.86111 avg_loss = 5.69230\n",
      "epoch no.8 train no.1570  loss = 4.68234 avg_loss = 5.64297\n",
      "epoch no.8 train no.1580  loss = 5.11378 avg_loss = 5.62926\n",
      "epoch no.8 train no.1590  loss = 6.09698 avg_loss = 5.59633\n",
      "epoch no.8 train no.1600  loss = 5.55696 avg_loss = 5.57060\n",
      "epoch no.8 train no.1610  loss = 6.30827 avg_loss = 5.58759\n",
      "epoch no.8 train no.1620  loss = 5.68852 avg_loss = 5.61071\n",
      "epoch no.8 train no.1630  loss = 4.89694 avg_loss = 5.64682\n",
      "epoch no.8 train no.1640  loss = 6.14561 avg_loss = 5.67491\n",
      "epoch no.8 train no.1650  loss = 6.01056 avg_loss = 5.70244\n",
      "epoch no.8 train no.1660  loss = 6.60887 avg_loss = 5.72000\n",
      "epoch no.8 train no.1670  loss = 4.50763 avg_loss = 5.73807\n",
      "epoch no.8 train no.1680  loss = 6.42053 avg_loss = 5.74941\n",
      "epoch no.8 train no.1690  loss = 6.38159 avg_loss = 5.75010\n",
      "epoch no.8 train no.1700  loss = 6.15663 avg_loss = 5.72881\n",
      "epoch no.8 train no.1710  loss = 7.11715 avg_loss = 5.76742\n",
      "epoch no.8 train no.1720  loss = 7.61245 avg_loss = 5.75370\n",
      "epoch no.8 train no.1730  loss = 6.40531 avg_loss = 5.76084\n",
      "epoch no.8 train no.1740  loss = 4.92657 avg_loss = 5.76795\n",
      "epoch no.8 train no.1750  loss = 4.61839 avg_loss = 5.74474\n",
      "epoch no.9 train no.0  loss = 6.49124 avg_loss = 5.76894\n",
      "epoch no.9 train no.10  loss = 5.66355 avg_loss = 5.75406\n",
      "epoch no.9 train no.20  loss = 5.04003 avg_loss = 5.71548\n",
      "epoch no.9 train no.30  loss = 4.74290 avg_loss = 5.72328\n",
      "epoch no.9 train no.40  loss = 5.49823 avg_loss = 5.71966\n",
      "epoch no.9 train no.50  loss = 4.90124 avg_loss = 5.71894\n",
      "epoch no.9 train no.60  loss = 3.31343 avg_loss = 5.72702\n",
      "epoch no.9 train no.70  loss = 6.66438 avg_loss = 5.77089\n",
      "epoch no.9 train no.80  loss = 5.09022 avg_loss = 5.72106\n",
      "epoch no.9 train no.90  loss = 5.83734 avg_loss = 5.71835\n",
      "epoch no.9 train no.100  loss = 7.37903 avg_loss = 5.75690\n",
      "epoch no.9 train no.110  loss = 4.14072 avg_loss = 5.75848\n",
      "epoch no.9 train no.120  loss = 6.99963 avg_loss = 5.76664\n",
      "epoch no.9 train no.130  loss = 5.30029 avg_loss = 5.79094\n",
      "epoch no.9 train no.140  loss = 5.97748 avg_loss = 5.83565\n",
      "epoch no.9 train no.150  loss = 4.60264 avg_loss = 5.83891\n",
      "epoch no.9 train no.160  loss = 5.42469 avg_loss = 5.82266\n",
      "epoch no.9 train no.170  loss = 5.74636 avg_loss = 5.84802\n",
      "epoch no.9 train no.180  loss = 4.27212 avg_loss = 5.83024\n",
      "epoch no.9 train no.190  loss = 4.45123 avg_loss = 5.82265\n",
      "epoch no.9 train no.200  loss = 5.73310 avg_loss = 5.79427\n",
      "epoch no.9 train no.210  loss = 6.49953 avg_loss = 5.81778\n",
      "epoch no.9 train no.220  loss = 4.31617 avg_loss = 5.78513\n",
      "epoch no.9 train no.230  loss = 6.64085 avg_loss = 5.80027\n",
      "epoch no.9 train no.240  loss = 4.33429 avg_loss = 5.73544\n",
      "epoch no.9 train no.250  loss = 4.15248 avg_loss = 5.74762\n",
      "epoch no.9 train no.260  loss = 5.94496 avg_loss = 5.78915\n",
      "epoch no.9 train no.270  loss = 6.32718 avg_loss = 5.80794\n",
      "epoch no.9 train no.280  loss = 6.52087 avg_loss = 5.78913\n",
      "epoch no.9 train no.290  loss = 6.91663 avg_loss = 5.75888\n",
      "epoch no.9 train no.300  loss = 5.23440 avg_loss = 5.80014\n",
      "epoch no.9 train no.310  loss = 5.90506 avg_loss = 5.81116\n",
      "epoch no.9 train no.320  loss = 4.78697 avg_loss = 5.77967\n",
      "epoch no.9 train no.330  loss = 7.17675 avg_loss = 5.82532\n",
      "epoch no.9 train no.340  loss = 6.53948 avg_loss = 5.80791\n",
      "epoch no.9 train no.350  loss = 5.65826 avg_loss = 5.82126\n",
      "epoch no.9 train no.360  loss = 5.98232 avg_loss = 5.81131\n",
      "epoch no.9 train no.370  loss = 6.57426 avg_loss = 5.83733\n",
      "epoch no.9 train no.380  loss = 3.55023 avg_loss = 5.79935\n",
      "epoch no.9 train no.390  loss = 6.10993 avg_loss = 5.80475\n",
      "epoch no.9 train no.400  loss = 6.19386 avg_loss = 5.84985\n",
      "epoch no.9 train no.410  loss = 5.52750 avg_loss = 5.89614\n",
      "epoch no.9 train no.420  loss = 5.22963 avg_loss = 5.83361\n",
      "epoch no.9 train no.430  loss = 6.10401 avg_loss = 5.84838\n",
      "epoch no.9 train no.440  loss = 4.80967 avg_loss = 5.83237\n",
      "epoch no.9 train no.450  loss = 6.78164 avg_loss = 5.83202\n",
      "epoch no.9 train no.460  loss = 6.08900 avg_loss = 5.84967\n",
      "epoch no.9 train no.470  loss = 7.17850 avg_loss = 5.85681\n",
      "epoch no.9 train no.480  loss = 6.19222 avg_loss = 5.85157\n",
      "epoch no.9 train no.490  loss = 5.22447 avg_loss = 5.77528\n",
      "epoch no.9 train no.500  loss = 6.73485 avg_loss = 5.73606\n",
      "epoch no.9 train no.510  loss = 5.18965 avg_loss = 5.74913\n",
      "epoch no.9 train no.520  loss = 5.08844 avg_loss = 5.74744\n",
      "epoch no.9 train no.530  loss = 7.14214 avg_loss = 5.75464\n",
      "epoch no.9 train no.540  loss = 4.43400 avg_loss = 5.71909\n",
      "epoch no.9 train no.550  loss = 4.44575 avg_loss = 5.70332\n",
      "epoch no.9 train no.560  loss = 6.38468 avg_loss = 5.72275\n",
      "epoch no.9 train no.570  loss = 6.44428 avg_loss = 5.71683\n",
      "epoch no.9 train no.580  loss = 5.18986 avg_loss = 5.72719\n",
      "epoch no.9 train no.590  loss = 6.72221 avg_loss = 5.74860\n",
      "epoch no.9 train no.600  loss = 6.17328 avg_loss = 5.73972\n",
      "epoch no.9 train no.610  loss = 6.38816 avg_loss = 5.74688\n",
      "epoch no.9 train no.620  loss = 5.80452 avg_loss = 5.74807\n",
      "epoch no.9 train no.630  loss = 6.35999 avg_loss = 5.74408\n",
      "epoch no.9 train no.640  loss = 5.44724 avg_loss = 5.76002\n",
      "epoch no.9 train no.650  loss = 5.04176 avg_loss = 5.76100\n",
      "epoch no.9 train no.660  loss = 6.56020 avg_loss = 5.77222\n",
      "epoch no.9 train no.670  loss = 5.71710 avg_loss = 5.78386\n",
      "epoch no.9 train no.680  loss = 5.81149 avg_loss = 5.81251\n",
      "epoch no.9 train no.690  loss = 6.52242 avg_loss = 5.82427\n",
      "epoch no.9 train no.700  loss = 6.00675 avg_loss = 5.81589\n",
      "epoch no.9 train no.710  loss = 4.33862 avg_loss = 5.78033\n",
      "epoch no.9 train no.720  loss = 5.31620 avg_loss = 5.76000\n",
      "epoch no.9 train no.730  loss = 6.09047 avg_loss = 5.76170\n",
      "epoch no.9 train no.740  loss = 4.99018 avg_loss = 5.78607\n",
      "epoch no.9 train no.750  loss = 5.58498 avg_loss = 5.82497\n",
      "epoch no.9 train no.760  loss = 6.62853 avg_loss = 5.83260\n",
      "epoch no.9 train no.770  loss = 5.89614 avg_loss = 5.79034\n",
      "epoch no.9 train no.780  loss = 5.45341 avg_loss = 5.77840\n",
      "epoch no.9 train no.790  loss = 5.88188 avg_loss = 5.79882\n",
      "epoch no.9 train no.800  loss = 6.54463 avg_loss = 5.78389\n",
      "epoch no.9 train no.810  loss = 6.11374 avg_loss = 5.73898\n",
      "epoch no.9 train no.820  loss = 5.03428 avg_loss = 5.73394\n",
      "epoch no.9 train no.830  loss = 6.20175 avg_loss = 5.73568\n",
      "epoch no.9 train no.840  loss = 4.81364 avg_loss = 5.70219\n",
      "epoch no.9 train no.850  loss = 4.95255 avg_loss = 5.69006\n",
      "epoch no.9 train no.860  loss = 6.41121 avg_loss = 5.73467\n",
      "epoch no.9 train no.870  loss = 5.65950 avg_loss = 5.73186\n",
      "epoch no.9 train no.880  loss = 5.63878 avg_loss = 5.74002\n",
      "epoch no.9 train no.890  loss = 4.83561 avg_loss = 5.72398\n",
      "epoch no.9 train no.900  loss = 6.44653 avg_loss = 5.73679\n",
      "epoch no.9 train no.910  loss = 5.95964 avg_loss = 5.74544\n",
      "epoch no.9 train no.920  loss = 5.64249 avg_loss = 5.75167\n",
      "epoch no.9 train no.930  loss = 6.27479 avg_loss = 5.69387\n",
      "epoch no.9 train no.940  loss = 6.20868 avg_loss = 5.68320\n",
      "epoch no.9 train no.950  loss = 2.82922 avg_loss = 5.61251\n",
      "epoch no.9 train no.960  loss = 4.80507 avg_loss = 5.65667\n",
      "epoch no.9 train no.970  loss = 4.88144 avg_loss = 5.67178\n",
      "epoch no.9 train no.980  loss = 4.98308 avg_loss = 5.63999\n",
      "epoch no.9 train no.990  loss = 5.08772 avg_loss = 5.65071\n",
      "epoch no.9 train no.1000  loss = 5.98898 avg_loss = 5.63391\n",
      "to_tokens: ['</s>', '</s>', '</s>']\n",
      "뭐함</s>\n",
      "epoch no.9 train no.1010  loss = 6.44304 avg_loss = 5.63675\n",
      "epoch no.9 train no.1020  loss = 6.67550 avg_loss = 5.70105\n",
      "epoch no.9 train no.1030  loss = 5.22671 avg_loss = 5.62927\n",
      "epoch no.9 train no.1040  loss = 7.71895 avg_loss = 5.65332\n",
      "epoch no.9 train no.1050  loss = 5.60333 avg_loss = 5.66747\n",
      "epoch no.9 train no.1060  loss = 5.94406 avg_loss = 5.66959\n",
      "epoch no.9 train no.1070  loss = 5.86436 avg_loss = 5.69369\n",
      "epoch no.9 train no.1080  loss = 6.92547 avg_loss = 5.70737\n",
      "epoch no.9 train no.1090  loss = 3.61656 avg_loss = 5.67309\n",
      "epoch no.9 train no.1100  loss = 7.81439 avg_loss = 5.73378\n",
      "epoch no.9 train no.1110  loss = 4.72038 avg_loss = 5.71386\n",
      "epoch no.9 train no.1120  loss = 5.06517 avg_loss = 5.69474\n",
      "epoch no.9 train no.1130  loss = 5.97072 avg_loss = 5.71947\n",
      "epoch no.9 train no.1140  loss = 6.74911 avg_loss = 5.75336\n",
      "epoch no.9 train no.1150  loss = 6.28317 avg_loss = 5.77390\n",
      "epoch no.9 train no.1160  loss = 5.30899 avg_loss = 5.80067\n",
      "epoch no.9 train no.1170  loss = 4.85359 avg_loss = 5.83668\n",
      "epoch no.9 train no.1180  loss = 5.51003 avg_loss = 5.79543\n",
      "epoch no.9 train no.1190  loss = 3.32262 avg_loss = 5.73191\n",
      "epoch no.9 train no.1200  loss = 5.83113 avg_loss = 5.73275\n",
      "epoch no.9 train no.1210  loss = 4.68652 avg_loss = 5.69973\n",
      "epoch no.9 train no.1220  loss = 6.14220 avg_loss = 5.71573\n",
      "epoch no.9 train no.1230  loss = 6.01597 avg_loss = 5.72218\n",
      "epoch no.9 train no.1240  loss = 6.69548 avg_loss = 5.76101\n",
      "epoch no.9 train no.1250  loss = 6.11783 avg_loss = 5.74539\n",
      "epoch no.9 train no.1260  loss = 6.09950 avg_loss = 5.77951\n",
      "epoch no.9 train no.1270  loss = 4.15358 avg_loss = 5.74507\n",
      "epoch no.9 train no.1280  loss = 5.60433 avg_loss = 5.75621\n",
      "epoch no.9 train no.1290  loss = 6.03349 avg_loss = 5.76679\n",
      "epoch no.9 train no.1300  loss = 3.85485 avg_loss = 5.68494\n",
      "epoch no.9 train no.1310  loss = 5.44154 avg_loss = 5.73513\n",
      "epoch no.9 train no.1320  loss = 6.06218 avg_loss = 5.73601\n",
      "epoch no.9 train no.1330  loss = 6.52482 avg_loss = 5.68619\n",
      "epoch no.9 train no.1340  loss = 6.11927 avg_loss = 5.69603\n",
      "epoch no.9 train no.1350  loss = 5.49945 avg_loss = 5.64279\n",
      "epoch no.9 train no.1360  loss = 4.95144 avg_loss = 5.58814\n",
      "epoch no.9 train no.1370  loss = 4.36592 avg_loss = 5.51642\n",
      "epoch no.9 train no.1380  loss = 4.10479 avg_loss = 5.46626\n",
      "epoch no.9 train no.1390  loss = 5.22419 avg_loss = 5.46514\n",
      "epoch no.9 train no.1400  loss = 6.96625 avg_loss = 5.49665\n",
      "epoch no.9 train no.1410  loss = 6.15155 avg_loss = 5.57297\n",
      "epoch no.9 train no.1420  loss = 6.09444 avg_loss = 5.59201\n",
      "epoch no.9 train no.1430  loss = 5.21181 avg_loss = 5.62736\n",
      "epoch no.9 train no.1440  loss = 6.90711 avg_loss = 5.67125\n",
      "epoch no.9 train no.1450  loss = 4.33493 avg_loss = 5.66488\n",
      "epoch no.9 train no.1460  loss = 5.78012 avg_loss = 5.66785\n",
      "epoch no.9 train no.1470  loss = 6.30901 avg_loss = 5.67963\n",
      "epoch no.9 train no.1480  loss = 5.09230 avg_loss = 5.65452\n",
      "epoch no.9 train no.1490  loss = 4.93719 avg_loss = 5.70663\n",
      "epoch no.9 train no.1500  loss = 6.17344 avg_loss = 5.71198\n",
      "epoch no.9 train no.1510  loss = 6.81662 avg_loss = 5.68066\n",
      "epoch no.9 train no.1520  loss = 3.87879 avg_loss = 5.61060\n",
      "epoch no.9 train no.1530  loss = 7.39679 avg_loss = 5.65573\n",
      "epoch no.9 train no.1540  loss = 6.26124 avg_loss = 5.66438\n",
      "epoch no.9 train no.1550  loss = 5.93313 avg_loss = 5.66621\n",
      "epoch no.9 train no.1560  loss = 5.28851 avg_loss = 5.68743\n",
      "epoch no.9 train no.1570  loss = 6.30809 avg_loss = 5.69883\n",
      "epoch no.9 train no.1580  loss = 7.73502 avg_loss = 5.67948\n",
      "epoch no.9 train no.1590  loss = 5.51497 avg_loss = 5.64544\n",
      "epoch no.9 train no.1600  loss = 5.06294 avg_loss = 5.63861\n",
      "epoch no.9 train no.1610  loss = 5.90200 avg_loss = 5.63403\n",
      "epoch no.9 train no.1620  loss = 3.29795 avg_loss = 5.55441\n",
      "epoch no.9 train no.1630  loss = 5.19325 avg_loss = 5.54090\n",
      "epoch no.9 train no.1640  loss = 6.22971 avg_loss = 5.54354\n",
      "epoch no.9 train no.1650  loss = 4.85494 avg_loss = 5.56550\n",
      "epoch no.9 train no.1660  loss = 6.79697 avg_loss = 5.54518\n",
      "epoch no.9 train no.1670  loss = 4.78216 avg_loss = 5.56961\n",
      "epoch no.9 train no.1680  loss = 4.51930 avg_loss = 5.60137\n",
      "epoch no.9 train no.1690  loss = 6.08320 avg_loss = 5.63966\n",
      "epoch no.9 train no.1700  loss = 5.95351 avg_loss = 5.65850\n",
      "epoch no.9 train no.1710  loss = 7.02241 avg_loss = 5.68667\n",
      "epoch no.9 train no.1720  loss = 5.64694 avg_loss = 5.69124\n",
      "epoch no.9 train no.1730  loss = 5.59797 avg_loss = 5.67588\n",
      "epoch no.9 train no.1740  loss = 5.82229 avg_loss = 5.71325\n",
      "epoch no.9 train no.1750  loss = 5.27334 avg_loss = 5.71386\n",
      "epoch no.10 train no.0  loss = 5.05068 avg_loss = 5.70907\n",
      "epoch no.10 train no.10  loss = 7.16659 avg_loss = 5.71412\n",
      "epoch no.10 train no.20  loss = 4.21010 avg_loss = 5.70240\n",
      "epoch no.10 train no.30  loss = 5.01157 avg_loss = 5.72663\n",
      "epoch no.10 train no.40  loss = 4.80571 avg_loss = 5.70734\n",
      "epoch no.10 train no.50  loss = 6.03572 avg_loss = 5.74494\n",
      "epoch no.10 train no.60  loss = 4.33607 avg_loss = 5.75672\n",
      "epoch no.10 train no.70  loss = 6.16008 avg_loss = 5.71432\n",
      "epoch no.10 train no.80  loss = 4.54665 avg_loss = 5.69776\n",
      "epoch no.10 train no.90  loss = 7.04402 avg_loss = 5.69698\n",
      "epoch no.10 train no.100  loss = 5.03327 avg_loss = 5.66279\n",
      "epoch no.10 train no.110  loss = 6.44485 avg_loss = 5.65715\n",
      "epoch no.10 train no.120  loss = 6.49792 avg_loss = 5.65796\n",
      "epoch no.10 train no.130  loss = 5.59008 avg_loss = 5.65255\n",
      "epoch no.10 train no.140  loss = 6.10831 avg_loss = 5.67769\n",
      "epoch no.10 train no.150  loss = 5.88912 avg_loss = 5.68629\n",
      "epoch no.10 train no.160  loss = 6.53578 avg_loss = 5.71415\n",
      "epoch no.10 train no.170  loss = 6.54283 avg_loss = 5.72233\n",
      "epoch no.10 train no.180  loss = 6.35027 avg_loss = 5.70105\n",
      "epoch no.10 train no.190  loss = 6.01925 avg_loss = 5.71161\n",
      "epoch no.10 train no.200  loss = 6.45514 avg_loss = 5.72312\n",
      "epoch no.10 train no.210  loss = 5.29306 avg_loss = 5.75536\n",
      "epoch no.10 train no.220  loss = 5.55471 avg_loss = 5.78877\n",
      "epoch no.10 train no.230  loss = 5.65077 avg_loss = 5.77118\n",
      "epoch no.10 train no.240  loss = 4.04815 avg_loss = 5.75297\n",
      "epoch no.10 train no.250  loss = 6.06317 avg_loss = 5.76292\n",
      "epoch no.10 train no.260  loss = 6.53911 avg_loss = 5.73698\n",
      "epoch no.10 train no.270  loss = 6.38291 avg_loss = 5.72879\n",
      "epoch no.10 train no.280  loss = 4.42346 avg_loss = 5.73187\n",
      "epoch no.10 train no.290  loss = 6.80593 avg_loss = 5.72157\n",
      "epoch no.10 train no.300  loss = 6.03615 avg_loss = 5.69562\n",
      "epoch no.10 train no.310  loss = 4.38572 avg_loss = 5.67535\n",
      "epoch no.10 train no.320  loss = 5.73620 avg_loss = 5.65977\n",
      "epoch no.10 train no.330  loss = 6.05160 avg_loss = 5.68049\n",
      "epoch no.10 train no.340  loss = 7.17881 avg_loss = 5.68019\n",
      "epoch no.10 train no.350  loss = 5.66104 avg_loss = 5.65734\n",
      "epoch no.10 train no.360  loss = 5.48814 avg_loss = 5.68711\n",
      "epoch no.10 train no.370  loss = 6.00190 avg_loss = 5.69706\n",
      "epoch no.10 train no.380  loss = 4.79253 avg_loss = 5.69417\n",
      "epoch no.10 train no.390  loss = 6.37078 avg_loss = 5.68626\n",
      "epoch no.10 train no.400  loss = 5.61021 avg_loss = 5.70732\n",
      "epoch no.10 train no.410  loss = 5.74225 avg_loss = 5.69654\n",
      "epoch no.10 train no.420  loss = 6.44510 avg_loss = 5.72219\n",
      "epoch no.10 train no.430  loss = 6.59180 avg_loss = 5.71307\n",
      "epoch no.10 train no.440  loss = 4.58903 avg_loss = 5.67229\n",
      "epoch no.10 train no.450  loss = 5.43420 avg_loss = 5.66346\n",
      "epoch no.10 train no.460  loss = 6.52266 avg_loss = 5.69528\n",
      "epoch no.10 train no.470  loss = 5.87702 avg_loss = 5.73165\n",
      "epoch no.10 train no.480  loss = 5.24704 avg_loss = 5.74513\n",
      "epoch no.10 train no.490  loss = 4.59831 avg_loss = 5.75450\n",
      "epoch no.10 train no.500  loss = 5.92666 avg_loss = 5.76510\n",
      "epoch no.10 train no.510  loss = 6.67387 avg_loss = 5.76644\n",
      "epoch no.10 train no.520  loss = 5.69986 avg_loss = 5.78565\n",
      "epoch no.10 train no.530  loss = 6.60500 avg_loss = 5.81805\n",
      "epoch no.10 train no.540  loss = 5.68995 avg_loss = 5.81633\n",
      "epoch no.10 train no.550  loss = 6.18837 avg_loss = 5.78776\n",
      "epoch no.10 train no.560  loss = 4.60814 avg_loss = 5.74873\n",
      "epoch no.10 train no.570  loss = 6.23408 avg_loss = 5.78915\n",
      "epoch no.10 train no.580  loss = 5.85547 avg_loss = 5.81043\n",
      "epoch no.10 train no.590  loss = 4.72798 avg_loss = 5.75232\n",
      "epoch no.10 train no.600  loss = 6.33453 avg_loss = 5.69567\n",
      "epoch no.10 train no.610  loss = 6.22232 avg_loss = 5.68939\n",
      "epoch no.10 train no.620  loss = 7.13113 avg_loss = 5.71475\n",
      "epoch no.10 train no.630  loss = 6.11946 avg_loss = 5.73175\n",
      "epoch no.10 train no.640  loss = 4.61522 avg_loss = 5.72325\n",
      "epoch no.10 train no.650  loss = 6.69559 avg_loss = 5.75243\n",
      "epoch no.10 train no.660  loss = 6.25667 avg_loss = 5.74407\n",
      "epoch no.10 train no.670  loss = 5.48438 avg_loss = 5.72855\n",
      "epoch no.10 train no.680  loss = 5.83391 avg_loss = 5.73441\n",
      "epoch no.10 train no.690  loss = 5.19425 avg_loss = 5.68014\n",
      "epoch no.10 train no.700  loss = 6.51937 avg_loss = 5.69340\n",
      "epoch no.10 train no.710  loss = 5.73077 avg_loss = 5.71405\n",
      "epoch no.10 train no.720  loss = 5.01517 avg_loss = 5.69696\n",
      "epoch no.10 train no.730  loss = 6.09173 avg_loss = 5.73781\n",
      "epoch no.10 train no.740  loss = 6.85171 avg_loss = 5.76112\n",
      "epoch no.10 train no.750  loss = 6.47113 avg_loss = 5.75068\n",
      "epoch no.10 train no.760  loss = 4.37829 avg_loss = 5.69172\n",
      "epoch no.10 train no.770  loss = 5.42653 avg_loss = 5.70031\n",
      "epoch no.10 train no.780  loss = 6.07458 avg_loss = 5.74015\n",
      "epoch no.10 train no.790  loss = 6.49646 avg_loss = 5.75804\n",
      "epoch no.10 train no.800  loss = 5.22084 avg_loss = 5.74838\n",
      "epoch no.10 train no.810  loss = 7.43007 avg_loss = 5.76640\n",
      "epoch no.10 train no.820  loss = 4.80034 avg_loss = 5.79663\n",
      "epoch no.10 train no.830  loss = 5.81486 avg_loss = 5.79945\n",
      "epoch no.10 train no.840  loss = 6.53165 avg_loss = 5.84401\n",
      "epoch no.10 train no.850  loss = 5.21157 avg_loss = 5.81640\n",
      "epoch no.10 train no.860  loss = 5.97319 avg_loss = 5.83936\n",
      "epoch no.10 train no.870  loss = 5.81412 avg_loss = 5.78939\n",
      "epoch no.10 train no.880  loss = 4.60594 avg_loss = 5.80806\n",
      "epoch no.10 train no.890  loss = 4.76747 avg_loss = 5.78185\n",
      "epoch no.10 train no.900  loss = 7.05966 avg_loss = 5.78024\n",
      "epoch no.10 train no.910  loss = 5.77480 avg_loss = 5.74752\n",
      "epoch no.10 train no.920  loss = 6.43628 avg_loss = 5.71302\n",
      "epoch no.10 train no.930  loss = 5.14818 avg_loss = 5.68077\n",
      "epoch no.10 train no.940  loss = 4.94261 avg_loss = 5.63107\n",
      "epoch no.10 train no.950  loss = 3.61407 avg_loss = 5.60543\n",
      "epoch no.10 train no.960  loss = 4.81441 avg_loss = 5.61893\n",
      "epoch no.10 train no.970  loss = 6.19726 avg_loss = 5.68427\n",
      "epoch no.10 train no.980  loss = 6.39918 avg_loss = 5.70243\n",
      "epoch no.10 train no.990  loss = 6.31010 avg_loss = 5.69363\n",
      "epoch no.10 train no.1000  loss = 5.89423 avg_loss = 5.72453\n",
      "to_tokens: ['</s>', '</s>', '</s>']\n",
      "뭐함</s>\n",
      "epoch no.10 train no.1010  loss = 5.02568 avg_loss = 5.72256\n",
      "epoch no.10 train no.1020  loss = 5.87682 avg_loss = 5.74892\n",
      "epoch no.10 train no.1030  loss = 7.02741 avg_loss = 5.72505\n",
      "epoch no.10 train no.1040  loss = 5.50578 avg_loss = 5.72903\n",
      "epoch no.10 train no.1050  loss = 5.51588 avg_loss = 5.68762\n",
      "epoch no.10 train no.1060  loss = 6.12289 avg_loss = 5.68832\n",
      "epoch no.10 train no.1070  loss = 4.28353 avg_loss = 5.70655\n",
      "epoch no.10 train no.1080  loss = 5.32870 avg_loss = 5.70984\n",
      "epoch no.10 train no.1090  loss = 5.53660 avg_loss = 5.68551\n",
      "epoch no.10 train no.1100  loss = 5.55187 avg_loss = 5.69982\n",
      "epoch no.10 train no.1110  loss = 5.70192 avg_loss = 5.67475\n",
      "epoch no.10 train no.1120  loss = 4.75079 avg_loss = 5.67146\n",
      "epoch no.10 train no.1130  loss = 4.98665 avg_loss = 5.64154\n",
      "epoch no.10 train no.1140  loss = 4.33269 avg_loss = 5.64063\n",
      "epoch no.10 train no.1150  loss = 5.33087 avg_loss = 5.63303\n",
      "epoch no.10 train no.1160  loss = 6.43967 avg_loss = 5.58891\n",
      "epoch no.10 train no.1170  loss = 5.15506 avg_loss = 5.59069\n",
      "epoch no.10 train no.1180  loss = 5.96853 avg_loss = 5.57759\n",
      "epoch no.10 train no.1190  loss = 4.00942 avg_loss = 5.55975\n",
      "epoch no.10 train no.1200  loss = 6.42947 avg_loss = 5.59715\n",
      "epoch no.10 train no.1210  loss = 5.69494 avg_loss = 5.61621\n",
      "epoch no.10 train no.1220  loss = 5.06322 avg_loss = 5.62582\n",
      "epoch no.10 train no.1230  loss = 5.22599 avg_loss = 5.62504\n",
      "epoch no.10 train no.1240  loss = 5.48554 avg_loss = 5.63862\n",
      "epoch no.10 train no.1250  loss = 5.59878 avg_loss = 5.64472\n",
      "epoch no.10 train no.1260  loss = 4.01862 avg_loss = 5.61806\n",
      "epoch no.10 train no.1270  loss = 7.42767 avg_loss = 5.64781\n",
      "epoch no.10 train no.1280  loss = 6.44798 avg_loss = 5.63366\n",
      "epoch no.10 train no.1290  loss = 5.14796 avg_loss = 5.63206\n",
      "epoch no.10 train no.1300  loss = 6.99619 avg_loss = 5.65871\n",
      "epoch no.10 train no.1310  loss = 6.71314 avg_loss = 5.72838\n",
      "epoch no.10 train no.1320  loss = 5.84348 avg_loss = 5.78680\n",
      "epoch no.10 train no.1330  loss = 6.21896 avg_loss = 5.78784\n",
      "epoch no.10 train no.1340  loss = 5.46832 avg_loss = 5.77478\n",
      "epoch no.10 train no.1350  loss = 5.27664 avg_loss = 5.73734\n",
      "epoch no.10 train no.1360  loss = 5.80438 avg_loss = 5.70703\n",
      "epoch no.10 train no.1370  loss = 6.19613 avg_loss = 5.64912\n",
      "epoch no.10 train no.1380  loss = 4.97323 avg_loss = 5.67846\n",
      "epoch no.10 train no.1390  loss = 4.53917 avg_loss = 5.68357\n",
      "epoch no.10 train no.1400  loss = 6.64884 avg_loss = 5.69148\n",
      "epoch no.10 train no.1410  loss = 6.13569 avg_loss = 5.67810\n",
      "epoch no.10 train no.1420  loss = 4.33487 avg_loss = 5.69364\n",
      "epoch no.10 train no.1430  loss = 5.05760 avg_loss = 5.71461\n",
      "epoch no.10 train no.1440  loss = 6.03200 avg_loss = 5.68818\n",
      "epoch no.10 train no.1450  loss = 6.68946 avg_loss = 5.73234\n",
      "epoch no.10 train no.1460  loss = 5.31408 avg_loss = 5.71234\n",
      "epoch no.10 train no.1470  loss = 5.10459 avg_loss = 5.69847\n",
      "epoch no.10 train no.1480  loss = 6.35960 avg_loss = 5.69561\n",
      "epoch no.10 train no.1490  loss = 5.54941 avg_loss = 5.68338\n",
      "epoch no.10 train no.1500  loss = 7.43808 avg_loss = 5.68242\n",
      "epoch no.10 train no.1510  loss = 5.74124 avg_loss = 5.71295\n",
      "epoch no.10 train no.1520  loss = 4.48435 avg_loss = 5.68239\n",
      "epoch no.10 train no.1530  loss = 5.20516 avg_loss = 5.71518\n",
      "epoch no.10 train no.1540  loss = 4.53743 avg_loss = 5.74428\n",
      "epoch no.10 train no.1550  loss = 5.54912 avg_loss = 5.74188\n",
      "epoch no.10 train no.1560  loss = 5.90837 avg_loss = 5.71107\n",
      "epoch no.10 train no.1570  loss = 6.33226 avg_loss = 5.69630\n",
      "epoch no.10 train no.1580  loss = 5.94775 avg_loss = 5.69344\n",
      "epoch no.10 train no.1590  loss = 6.02218 avg_loss = 5.67593\n",
      "epoch no.10 train no.1600  loss = 6.66306 avg_loss = 5.70189\n",
      "epoch no.10 train no.1610  loss = 6.53243 avg_loss = 5.70319\n",
      "epoch no.10 train no.1620  loss = 6.53435 avg_loss = 5.69838\n",
      "epoch no.10 train no.1630  loss = 6.82457 avg_loss = 5.70217\n",
      "epoch no.10 train no.1640  loss = 5.26994 avg_loss = 5.69028\n",
      "epoch no.10 train no.1650  loss = 6.30858 avg_loss = 5.72218\n",
      "epoch no.10 train no.1660  loss = 6.51831 avg_loss = 5.76473\n",
      "epoch no.10 train no.1670  loss = 5.93179 avg_loss = 5.72938\n",
      "epoch no.10 train no.1680  loss = 6.10728 avg_loss = 5.75590\n",
      "epoch no.10 train no.1690  loss = 6.66393 avg_loss = 5.72486\n",
      "epoch no.10 train no.1700  loss = 6.93540 avg_loss = 5.75544\n",
      "epoch no.10 train no.1710  loss = 4.21755 avg_loss = 5.75552\n",
      "epoch no.10 train no.1720  loss = 5.31283 avg_loss = 5.75703\n",
      "epoch no.10 train no.1730  loss = 6.49363 avg_loss = 5.76712\n",
      "epoch no.10 train no.1740  loss = 6.05555 avg_loss = 5.71673\n",
      "epoch no.10 train no.1750  loss = 4.48853 avg_loss = 5.70156\n",
      "epoch no.11 train no.0  loss = 4.78550 avg_loss = 5.67638\n",
      "epoch no.11 train no.10  loss = 6.23728 avg_loss = 5.66897\n",
      "epoch no.11 train no.20  loss = 5.60328 avg_loss = 5.60821\n",
      "epoch no.11 train no.30  loss = 7.19686 avg_loss = 5.61312\n",
      "epoch no.11 train no.40  loss = 4.62057 avg_loss = 5.61608\n",
      "epoch no.11 train no.50  loss = 5.94269 avg_loss = 5.60377\n",
      "epoch no.11 train no.60  loss = 5.21822 avg_loss = 5.63156\n",
      "epoch no.11 train no.70  loss = 3.87749 avg_loss = 5.64804\n",
      "epoch no.11 train no.80  loss = 6.61163 avg_loss = 5.64732\n",
      "epoch no.11 train no.90  loss = 5.91730 avg_loss = 5.61190\n",
      "epoch no.11 train no.100  loss = 6.26908 avg_loss = 5.61172\n",
      "epoch no.11 train no.110  loss = 4.44755 avg_loss = 5.61932\n",
      "epoch no.11 train no.120  loss = 5.02652 avg_loss = 5.60337\n",
      "epoch no.11 train no.130  loss = 3.55040 avg_loss = 5.55795\n",
      "epoch no.11 train no.140  loss = 4.55603 avg_loss = 5.55079\n",
      "epoch no.11 train no.150  loss = 6.39566 avg_loss = 5.59197\n",
      "epoch no.11 train no.160  loss = 5.38752 avg_loss = 5.62056\n",
      "epoch no.11 train no.170  loss = 3.42669 avg_loss = 5.59583\n",
      "epoch no.11 train no.180  loss = 3.77252 avg_loss = 5.56479\n",
      "epoch no.11 train no.190  loss = 6.56200 avg_loss = 5.55837\n",
      "epoch no.11 train no.200  loss = 6.09239 avg_loss = 5.59857\n",
      "epoch no.11 train no.210  loss = 6.87745 avg_loss = 5.62725\n",
      "epoch no.11 train no.220  loss = 5.49393 avg_loss = 5.56273\n",
      "epoch no.11 train no.230  loss = 7.06459 avg_loss = 5.58515\n",
      "epoch no.11 train no.240  loss = 6.22746 avg_loss = 5.62966\n",
      "epoch no.11 train no.250  loss = 7.33147 avg_loss = 5.64112\n",
      "epoch no.11 train no.260  loss = 5.41282 avg_loss = 5.63723\n",
      "epoch no.11 train no.270  loss = 6.36970 avg_loss = 5.66650\n",
      "epoch no.11 train no.280  loss = 5.47281 avg_loss = 5.63433\n",
      "epoch no.11 train no.290  loss = 5.42892 avg_loss = 5.63928\n",
      "epoch no.11 train no.300  loss = 5.40568 avg_loss = 5.64786\n",
      "epoch no.11 train no.310  loss = 5.41625 avg_loss = 5.65301\n",
      "epoch no.11 train no.320  loss = 5.35029 avg_loss = 5.68263\n",
      "epoch no.11 train no.330  loss = 6.12120 avg_loss = 5.71417\n",
      "epoch no.11 train no.340  loss = 6.46366 avg_loss = 5.71511\n",
      "epoch no.11 train no.350  loss = 5.11432 avg_loss = 5.72076\n",
      "epoch no.11 train no.360  loss = 4.86415 avg_loss = 5.68822\n",
      "epoch no.11 train no.370  loss = 5.12768 avg_loss = 5.63817\n",
      "epoch no.11 train no.380  loss = 6.16919 avg_loss = 5.60807\n",
      "epoch no.11 train no.390  loss = 3.55998 avg_loss = 5.60100\n",
      "epoch no.11 train no.400  loss = 5.45877 avg_loss = 5.60311\n",
      "epoch no.11 train no.410  loss = 4.55364 avg_loss = 5.62048\n",
      "epoch no.11 train no.420  loss = 4.94234 avg_loss = 5.59398\n",
      "epoch no.11 train no.430  loss = 5.12081 avg_loss = 5.62476\n",
      "epoch no.11 train no.440  loss = 6.15949 avg_loss = 5.62398\n",
      "epoch no.11 train no.450  loss = 6.37504 avg_loss = 5.63042\n",
      "epoch no.11 train no.460  loss = 5.93141 avg_loss = 5.62573\n",
      "epoch no.11 train no.470  loss = 4.97946 avg_loss = 5.61180\n",
      "epoch no.11 train no.480  loss = 6.71682 avg_loss = 5.61247\n",
      "epoch no.11 train no.490  loss = 5.66474 avg_loss = 5.64250\n",
      "epoch no.11 train no.500  loss = 5.91048 avg_loss = 5.62286\n",
      "epoch no.11 train no.510  loss = 5.89813 avg_loss = 5.64901\n",
      "epoch no.11 train no.520  loss = 4.39738 avg_loss = 5.65731\n",
      "epoch no.11 train no.530  loss = 7.03594 avg_loss = 5.68868\n",
      "epoch no.11 train no.540  loss = 6.70077 avg_loss = 5.74773\n",
      "epoch no.11 train no.550  loss = 5.09567 avg_loss = 5.76336\n",
      "epoch no.11 train no.560  loss = 5.06568 avg_loss = 5.77216\n",
      "epoch no.11 train no.570  loss = 5.60300 avg_loss = 5.77170\n",
      "epoch no.11 train no.580  loss = 5.44069 avg_loss = 5.79082\n",
      "epoch no.11 train no.590  loss = 5.89930 avg_loss = 5.78570\n",
      "epoch no.11 train no.600  loss = 7.30780 avg_loss = 5.81000\n",
      "epoch no.11 train no.610  loss = 5.79242 avg_loss = 5.81973\n",
      "epoch no.11 train no.620  loss = 4.36303 avg_loss = 5.78375\n",
      "epoch no.11 train no.630  loss = 5.38321 avg_loss = 5.79344\n",
      "epoch no.11 train no.640  loss = 4.03187 avg_loss = 5.78689\n",
      "epoch no.11 train no.650  loss = 4.24178 avg_loss = 5.79615\n",
      "epoch no.11 train no.660  loss = 4.20910 avg_loss = 5.80106\n",
      "epoch no.11 train no.670  loss = 5.63680 avg_loss = 5.82501\n",
      "epoch no.11 train no.680  loss = 5.65939 avg_loss = 5.80322\n",
      "epoch no.11 train no.690  loss = 6.91450 avg_loss = 5.84442\n",
      "epoch no.11 train no.700  loss = 5.36763 avg_loss = 5.83086\n",
      "epoch no.11 train no.710  loss = 4.96287 avg_loss = 5.84242\n",
      "epoch no.11 train no.720  loss = 5.99891 avg_loss = 5.82737\n",
      "epoch no.11 train no.730  loss = 6.74179 avg_loss = 5.80427\n",
      "epoch no.11 train no.740  loss = 6.42863 avg_loss = 5.79379\n",
      "epoch no.11 train no.750  loss = 6.23929 avg_loss = 5.77348\n",
      "epoch no.11 train no.760  loss = 6.10892 avg_loss = 5.78693\n",
      "epoch no.11 train no.770  loss = 7.22429 avg_loss = 5.74965\n",
      "epoch no.11 train no.780  loss = 6.15251 avg_loss = 5.78404\n",
      "epoch no.11 train no.790  loss = 6.84658 avg_loss = 5.78546\n",
      "epoch no.11 train no.800  loss = 6.99235 avg_loss = 5.79816\n",
      "epoch no.11 train no.810  loss = 5.77885 avg_loss = 5.82749\n",
      "epoch no.11 train no.820  loss = 5.56964 avg_loss = 5.84453\n",
      "epoch no.11 train no.830  loss = 4.66382 avg_loss = 5.77148\n",
      "epoch no.11 train no.840  loss = 6.72519 avg_loss = 5.80154\n",
      "epoch no.11 train no.850  loss = 6.32384 avg_loss = 5.79045\n",
      "epoch no.11 train no.860  loss = 5.33091 avg_loss = 5.76698\n",
      "epoch no.11 train no.870  loss = 5.28597 avg_loss = 5.74810\n",
      "epoch no.11 train no.880  loss = 5.81773 avg_loss = 5.76624\n",
      "epoch no.11 train no.890  loss = 5.45213 avg_loss = 5.78836\n",
      "epoch no.11 train no.900  loss = 4.86472 avg_loss = 5.79822\n",
      "epoch no.11 train no.910  loss = 5.28537 avg_loss = 5.79241\n",
      "epoch no.11 train no.920  loss = 7.17793 avg_loss = 5.77128\n",
      "epoch no.11 train no.930  loss = 4.61517 avg_loss = 5.78208\n",
      "epoch no.11 train no.940  loss = 6.41128 avg_loss = 5.76869\n",
      "epoch no.11 train no.950  loss = 5.78702 avg_loss = 5.78244\n",
      "epoch no.11 train no.960  loss = 6.79654 avg_loss = 5.75183\n",
      "epoch no.11 train no.970  loss = 6.22605 avg_loss = 5.72180\n",
      "epoch no.11 train no.980  loss = 6.89972 avg_loss = 5.75010\n",
      "epoch no.11 train no.990  loss = 6.03855 avg_loss = 5.73488\n",
      "epoch no.11 train no.1000  loss = 4.07467 avg_loss = 5.75116\n",
      "to_tokens: ['</s>', '</s>', '</s>']\n",
      "뭐함</s>\n",
      "epoch no.11 train no.1010  loss = 5.80982 avg_loss = 5.75293\n",
      "epoch no.11 train no.1020  loss = 4.86051 avg_loss = 5.73943\n",
      "epoch no.11 train no.1030  loss = 6.08977 avg_loss = 5.72477\n",
      "epoch no.11 train no.1040  loss = 6.60550 avg_loss = 5.75184\n",
      "epoch no.11 train no.1050  loss = 5.20582 avg_loss = 5.76882\n",
      "epoch no.11 train no.1060  loss = 6.65195 avg_loss = 5.78147\n",
      "epoch no.11 train no.1070  loss = 4.47731 avg_loss = 5.71980\n",
      "epoch no.11 train no.1080  loss = 6.16388 avg_loss = 5.71709\n",
      "epoch no.11 train no.1090  loss = 5.77671 avg_loss = 5.68301\n",
      "epoch no.11 train no.1100  loss = 6.46162 avg_loss = 5.63196\n",
      "epoch no.11 train no.1110  loss = 6.02391 avg_loss = 5.63145\n",
      "epoch no.11 train no.1120  loss = 4.28898 avg_loss = 5.60739\n",
      "epoch no.11 train no.1130  loss = 5.75166 avg_loss = 5.58584\n",
      "epoch no.11 train no.1140  loss = 5.40675 avg_loss = 5.61174\n",
      "epoch no.11 train no.1150  loss = 4.27448 avg_loss = 5.58964\n",
      "epoch no.11 train no.1160  loss = 6.21165 avg_loss = 5.53255\n",
      "epoch no.11 train no.1170  loss = 3.90427 avg_loss = 5.50414\n",
      "epoch no.11 train no.1180  loss = 5.50579 avg_loss = 5.54596\n",
      "epoch no.11 train no.1190  loss = 6.49076 avg_loss = 5.61108\n",
      "epoch no.11 train no.1200  loss = 4.69494 avg_loss = 5.61257\n",
      "epoch no.11 train no.1210  loss = 6.11134 avg_loss = 5.62186\n",
      "epoch no.11 train no.1220  loss = 5.41157 avg_loss = 5.65950\n",
      "epoch no.11 train no.1230  loss = 4.53064 avg_loss = 5.65347\n",
      "epoch no.11 train no.1240  loss = 6.23570 avg_loss = 5.65444\n",
      "epoch no.11 train no.1250  loss = 4.14924 avg_loss = 5.62016\n",
      "epoch no.11 train no.1260  loss = 3.79477 avg_loss = 5.54503\n",
      "epoch no.11 train no.1270  loss = 6.65780 avg_loss = 5.59550\n",
      "epoch no.11 train no.1280  loss = 6.69925 avg_loss = 5.63800\n",
      "epoch no.11 train no.1290  loss = 5.56057 avg_loss = 5.60387\n",
      "epoch no.11 train no.1300  loss = 3.85377 avg_loss = 5.63572\n",
      "epoch no.11 train no.1310  loss = 4.52994 avg_loss = 5.62372\n",
      "epoch no.11 train no.1320  loss = 3.89171 avg_loss = 5.58284\n",
      "epoch no.11 train no.1330  loss = 6.48520 avg_loss = 5.58151\n",
      "epoch no.11 train no.1340  loss = 6.29911 avg_loss = 5.59373\n",
      "epoch no.11 train no.1350  loss = 6.66554 avg_loss = 5.62056\n",
      "epoch no.11 train no.1360  loss = 3.92453 avg_loss = 5.60108\n",
      "epoch no.11 train no.1370  loss = 6.46766 avg_loss = 5.58797\n",
      "epoch no.11 train no.1380  loss = 5.56301 avg_loss = 5.58037\n",
      "epoch no.11 train no.1390  loss = 4.85205 avg_loss = 5.55762\n",
      "epoch no.11 train no.1400  loss = 5.25986 avg_loss = 5.58543\n",
      "epoch no.11 train no.1410  loss = 5.04691 avg_loss = 5.59525\n",
      "epoch no.11 train no.1420  loss = 5.97284 avg_loss = 5.64659\n",
      "epoch no.11 train no.1430  loss = 5.37937 avg_loss = 5.70368\n",
      "epoch no.11 train no.1440  loss = 6.37914 avg_loss = 5.72269\n",
      "epoch no.11 train no.1450  loss = 4.80274 avg_loss = 5.69110\n",
      "epoch no.11 train no.1460  loss = 5.93422 avg_loss = 5.73827\n",
      "epoch no.11 train no.1470  loss = 4.85312 avg_loss = 5.69188\n",
      "epoch no.11 train no.1480  loss = 5.24178 avg_loss = 5.70463\n",
      "epoch no.11 train no.1490  loss = 5.27412 avg_loss = 5.70988\n",
      "epoch no.11 train no.1500  loss = 6.10217 avg_loss = 5.70134\n",
      "epoch no.11 train no.1510  loss = 5.41148 avg_loss = 5.69065\n",
      "epoch no.11 train no.1520  loss = 7.12339 avg_loss = 5.70486\n",
      "epoch no.11 train no.1530  loss = 4.76615 avg_loss = 5.71336\n",
      "epoch no.11 train no.1540  loss = 5.21729 avg_loss = 5.71324\n",
      "epoch no.11 train no.1550  loss = 5.49184 avg_loss = 5.71985\n",
      "epoch no.11 train no.1560  loss = 5.54542 avg_loss = 5.75752\n",
      "epoch no.11 train no.1570  loss = 6.71277 avg_loss = 5.75071\n",
      "epoch no.11 train no.1580  loss = 5.72746 avg_loss = 5.71410\n",
      "epoch no.11 train no.1590  loss = 5.65082 avg_loss = 5.73552\n",
      "epoch no.11 train no.1600  loss = 6.56488 avg_loss = 5.75878\n",
      "epoch no.11 train no.1610  loss = 5.02210 avg_loss = 5.77221\n",
      "epoch no.11 train no.1620  loss = 6.61970 avg_loss = 5.78420\n",
      "epoch no.11 train no.1630  loss = 6.21798 avg_loss = 5.78600\n",
      "epoch no.11 train no.1640  loss = 4.66221 avg_loss = 5.73019\n",
      "epoch no.11 train no.1650  loss = 5.39337 avg_loss = 5.72573\n",
      "epoch no.11 train no.1660  loss = 7.46583 avg_loss = 5.75951\n",
      "epoch no.11 train no.1670  loss = 5.33363 avg_loss = 5.71899\n",
      "epoch no.11 train no.1680  loss = 6.69516 avg_loss = 5.74541\n",
      "epoch no.11 train no.1690  loss = 4.55976 avg_loss = 5.74383\n",
      "epoch no.11 train no.1700  loss = 5.45441 avg_loss = 5.77352\n",
      "epoch no.11 train no.1710  loss = 6.21182 avg_loss = 5.75662\n",
      "epoch no.11 train no.1720  loss = 5.67345 avg_loss = 5.77738\n",
      "epoch no.11 train no.1730  loss = 7.01556 avg_loss = 5.78536\n",
      "epoch no.11 train no.1740  loss = 6.56519 avg_loss = 5.76444\n",
      "epoch no.11 train no.1750  loss = 6.18436 avg_loss = 5.74388\n",
      "epoch no.12 train no.0  loss = 5.68102 avg_loss = 5.74703\n",
      "epoch no.12 train no.10  loss = 7.28506 avg_loss = 5.78257\n",
      "epoch no.12 train no.20  loss = 4.66429 avg_loss = 5.77898\n",
      "epoch no.12 train no.30  loss = 6.33456 avg_loss = 5.75189\n",
      "epoch no.12 train no.40  loss = 5.04678 avg_loss = 5.77263\n",
      "epoch no.12 train no.50  loss = 5.41115 avg_loss = 5.77105\n",
      "epoch no.12 train no.60  loss = 6.59986 avg_loss = 5.77912\n",
      "epoch no.12 train no.70  loss = 5.24735 avg_loss = 5.78867\n",
      "epoch no.12 train no.80  loss = 5.53850 avg_loss = 5.76000\n",
      "epoch no.12 train no.90  loss = 6.08005 avg_loss = 5.72693\n",
      "epoch no.12 train no.100  loss = 5.75005 avg_loss = 5.69502\n",
      "epoch no.12 train no.110  loss = 4.98999 avg_loss = 5.68890\n",
      "epoch no.12 train no.120  loss = 5.20760 avg_loss = 5.70290\n",
      "epoch no.12 train no.130  loss = 6.48424 avg_loss = 5.67514\n",
      "epoch no.12 train no.140  loss = 4.27112 avg_loss = 5.65244\n",
      "epoch no.12 train no.150  loss = 5.29986 avg_loss = 5.62136\n",
      "epoch no.12 train no.160  loss = 5.53992 avg_loss = 5.62939\n",
      "epoch no.12 train no.170  loss = 5.26993 avg_loss = 5.61871\n",
      "epoch no.12 train no.180  loss = 6.34111 avg_loss = 5.66102\n",
      "epoch no.12 train no.190  loss = 4.38995 avg_loss = 5.66655\n",
      "epoch no.12 train no.200  loss = 5.39072 avg_loss = 5.64266\n",
      "epoch no.12 train no.210  loss = 6.11022 avg_loss = 5.66522\n",
      "epoch no.12 train no.220  loss = 3.82256 avg_loss = 5.63399\n",
      "epoch no.12 train no.230  loss = 7.05593 avg_loss = 5.65170\n",
      "epoch no.12 train no.240  loss = 7.36450 avg_loss = 5.65906\n",
      "epoch no.12 train no.250  loss = 5.12356 avg_loss = 5.66193\n",
      "epoch no.12 train no.260  loss = 6.30227 avg_loss = 5.67918\n",
      "epoch no.12 train no.270  loss = 5.89178 avg_loss = 5.67871\n",
      "epoch no.12 train no.280  loss = 6.23113 avg_loss = 5.70654\n",
      "epoch no.12 train no.290  loss = 6.55910 avg_loss = 5.75638\n",
      "epoch no.12 train no.300  loss = 6.22089 avg_loss = 5.72645\n",
      "epoch no.12 train no.310  loss = 6.06106 avg_loss = 5.76193\n",
      "epoch no.12 train no.320  loss = 5.13452 avg_loss = 5.74067\n",
      "epoch no.12 train no.330  loss = 4.54962 avg_loss = 5.70501\n",
      "epoch no.12 train no.340  loss = 4.63543 avg_loss = 5.67467\n",
      "epoch no.12 train no.350  loss = 4.81617 avg_loss = 5.69082\n",
      "epoch no.12 train no.360  loss = 3.16077 avg_loss = 5.65440\n",
      "epoch no.12 train no.370  loss = 6.49324 avg_loss = 5.65923\n",
      "epoch no.12 train no.380  loss = 4.34774 avg_loss = 5.63955\n",
      "epoch no.12 train no.390  loss = 4.35820 avg_loss = 5.65594\n",
      "epoch no.12 train no.400  loss = 5.14901 avg_loss = 5.63878\n",
      "epoch no.12 train no.410  loss = 6.44813 avg_loss = 5.63714\n",
      "epoch no.12 train no.420  loss = 6.63013 avg_loss = 5.65897\n",
      "epoch no.12 train no.430  loss = 5.87837 avg_loss = 5.66316\n",
      "epoch no.12 train no.440  loss = 5.89769 avg_loss = 5.70840\n",
      "epoch no.12 train no.450  loss = 4.59881 avg_loss = 5.70912\n",
      "epoch no.12 train no.460  loss = 5.10713 avg_loss = 5.66909\n",
      "epoch no.12 train no.470  loss = 5.57501 avg_loss = 5.65709\n",
      "epoch no.12 train no.480  loss = 4.99005 avg_loss = 5.70729\n",
      "epoch no.12 train no.490  loss = 6.19776 avg_loss = 5.72551\n",
      "epoch no.12 train no.500  loss = 4.43082 avg_loss = 5.72952\n",
      "epoch no.12 train no.510  loss = 7.21938 avg_loss = 5.73930\n",
      "epoch no.12 train no.520  loss = 4.83273 avg_loss = 5.72169\n",
      "epoch no.12 train no.530  loss = 7.70922 avg_loss = 5.67831\n",
      "epoch no.12 train no.540  loss = 6.37844 avg_loss = 5.72034\n",
      "epoch no.12 train no.550  loss = 6.17091 avg_loss = 5.70719\n",
      "epoch no.12 train no.560  loss = 5.32348 avg_loss = 5.68912\n",
      "epoch no.12 train no.570  loss = 5.64543 avg_loss = 5.67299\n",
      "epoch no.12 train no.580  loss = 3.57729 avg_loss = 5.68049\n",
      "epoch no.12 train no.590  loss = 5.40588 avg_loss = 5.66708\n",
      "epoch no.12 train no.600  loss = 6.54487 avg_loss = 5.63776\n",
      "epoch no.12 train no.610  loss = 6.53555 avg_loss = 5.62989\n",
      "epoch no.12 train no.620  loss = 6.22189 avg_loss = 5.62086\n",
      "epoch no.12 train no.630  loss = 5.59368 avg_loss = 5.61948\n",
      "epoch no.12 train no.640  loss = 6.08481 avg_loss = 5.63318\n",
      "epoch no.12 train no.650  loss = 6.80376 avg_loss = 5.65101\n",
      "epoch no.12 train no.660  loss = 3.85347 avg_loss = 5.61416\n",
      "epoch no.12 train no.670  loss = 5.82959 avg_loss = 5.59567\n",
      "epoch no.12 train no.680  loss = 6.28152 avg_loss = 5.60976\n",
      "epoch no.12 train no.690  loss = 5.80709 avg_loss = 5.65857\n",
      "epoch no.12 train no.700  loss = 4.66174 avg_loss = 5.65482\n",
      "epoch no.12 train no.710  loss = 6.36508 avg_loss = 5.64863\n",
      "epoch no.12 train no.720  loss = 6.47414 avg_loss = 5.68009\n",
      "epoch no.12 train no.730  loss = 6.51901 avg_loss = 5.67283\n",
      "epoch no.12 train no.740  loss = 6.98351 avg_loss = 5.66440\n",
      "epoch no.12 train no.750  loss = 6.65240 avg_loss = 5.69581\n",
      "epoch no.12 train no.760  loss = 5.68757 avg_loss = 5.67858\n",
      "epoch no.12 train no.770  loss = 5.06621 avg_loss = 5.69031\n",
      "epoch no.12 train no.780  loss = 6.04880 avg_loss = 5.69465\n",
      "epoch no.12 train no.790  loss = 5.61522 avg_loss = 5.70925\n",
      "epoch no.12 train no.800  loss = 6.62239 avg_loss = 5.72670\n",
      "epoch no.12 train no.810  loss = 7.11518 avg_loss = 5.74559\n",
      "epoch no.12 train no.820  loss = 6.83166 avg_loss = 5.77197\n",
      "epoch no.12 train no.830  loss = 5.90273 avg_loss = 5.75633\n",
      "epoch no.12 train no.840  loss = 5.51038 avg_loss = 5.77158\n",
      "epoch no.12 train no.850  loss = 5.67863 avg_loss = 5.77602\n",
      "epoch no.12 train no.860  loss = 5.92526 avg_loss = 5.77024\n",
      "epoch no.12 train no.870  loss = 5.25337 avg_loss = 5.72835\n",
      "epoch no.12 train no.880  loss = 5.45385 avg_loss = 5.67400\n",
      "epoch no.12 train no.890  loss = 6.87887 avg_loss = 5.64875\n",
      "epoch no.12 train no.900  loss = 4.21809 avg_loss = 5.65238\n",
      "epoch no.12 train no.910  loss = 7.05360 avg_loss = 5.66336\n",
      "epoch no.12 train no.920  loss = 5.25553 avg_loss = 5.66557\n",
      "epoch no.12 train no.930  loss = 5.60292 avg_loss = 5.65805\n",
      "epoch no.12 train no.940  loss = 5.47913 avg_loss = 5.64729\n",
      "epoch no.12 train no.950  loss = 7.18037 avg_loss = 5.68830\n",
      "epoch no.12 train no.960  loss = 6.28689 avg_loss = 5.65936\n",
      "epoch no.12 train no.970  loss = 6.06641 avg_loss = 5.62844\n",
      "epoch no.12 train no.980  loss = 5.53215 avg_loss = 5.60623\n",
      "epoch no.12 train no.990  loss = 5.22779 avg_loss = 5.64377\n",
      "epoch no.12 train no.1000  loss = 5.30013 avg_loss = 5.66940\n",
      "to_tokens: ['</s>', '</s>', '</s>']\n",
      "뭐함</s>\n",
      "epoch no.12 train no.1010  loss = 6.04859 avg_loss = 5.71333\n",
      "epoch no.12 train no.1020  loss = 3.93732 avg_loss = 5.72501\n",
      "epoch no.12 train no.1030  loss = 4.62430 avg_loss = 5.70861\n",
      "epoch no.12 train no.1040  loss = 6.11552 avg_loss = 5.67992\n",
      "epoch no.12 train no.1050  loss = 4.79782 avg_loss = 5.65150\n",
      "epoch no.12 train no.1060  loss = 5.44124 avg_loss = 5.63106\n",
      "epoch no.12 train no.1070  loss = 8.26620 avg_loss = 5.65094\n",
      "epoch no.12 train no.1080  loss = 6.50128 avg_loss = 5.65042\n",
      "epoch no.12 train no.1090  loss = 4.66956 avg_loss = 5.70364\n",
      "epoch no.12 train no.1100  loss = 5.72826 avg_loss = 5.68457\n",
      "epoch no.12 train no.1110  loss = 7.10385 avg_loss = 5.73339\n",
      "epoch no.12 train no.1120  loss = 4.51004 avg_loss = 5.73768\n",
      "epoch no.12 train no.1130  loss = 6.69775 avg_loss = 5.75605\n",
      "epoch no.12 train no.1140  loss = 3.81773 avg_loss = 5.73050\n",
      "epoch no.12 train no.1150  loss = 6.31528 avg_loss = 5.73380\n",
      "epoch no.12 train no.1160  loss = 7.12604 avg_loss = 5.73155\n",
      "epoch no.12 train no.1170  loss = 4.24286 avg_loss = 5.70252\n",
      "epoch no.12 train no.1180  loss = 4.37932 avg_loss = 5.72485\n",
      "epoch no.12 train no.1190  loss = 4.98826 avg_loss = 5.72435\n",
      "epoch no.12 train no.1200  loss = 5.39162 avg_loss = 5.70482\n",
      "epoch no.12 train no.1210  loss = 6.24219 avg_loss = 5.69734\n",
      "epoch no.12 train no.1220  loss = 6.43530 avg_loss = 5.73894\n",
      "epoch no.12 train no.1230  loss = 6.97507 avg_loss = 5.75204\n",
      "epoch no.12 train no.1240  loss = 4.40876 avg_loss = 5.74291\n",
      "epoch no.12 train no.1250  loss = 3.21226 avg_loss = 5.69918\n",
      "epoch no.12 train no.1260  loss = 5.90311 avg_loss = 5.73651\n",
      "epoch no.12 train no.1270  loss = 4.30619 avg_loss = 5.71599\n",
      "epoch no.12 train no.1280  loss = 5.65812 avg_loss = 5.71021\n",
      "epoch no.12 train no.1290  loss = 5.01259 avg_loss = 5.68082\n",
      "epoch no.12 train no.1300  loss = 4.81528 avg_loss = 5.66735\n",
      "epoch no.12 train no.1310  loss = 5.58861 avg_loss = 5.67924\n",
      "epoch no.12 train no.1320  loss = 6.14968 avg_loss = 5.69439\n",
      "epoch no.12 train no.1330  loss = 4.17391 avg_loss = 5.70534\n",
      "epoch no.12 train no.1340  loss = 3.96102 avg_loss = 5.66550\n",
      "epoch no.12 train no.1350  loss = 5.45221 avg_loss = 5.68493\n",
      "epoch no.12 train no.1360  loss = 5.08222 avg_loss = 5.71443\n",
      "epoch no.12 train no.1370  loss = 6.10419 avg_loss = 5.69321\n",
      "epoch no.12 train no.1380  loss = 5.42675 avg_loss = 5.65616\n",
      "epoch no.12 train no.1390  loss = 5.98473 avg_loss = 5.65429\n",
      "epoch no.12 train no.1400  loss = 5.02571 avg_loss = 5.61809\n",
      "epoch no.12 train no.1410  loss = 6.05538 avg_loss = 5.67261\n",
      "epoch no.12 train no.1420  loss = 6.28166 avg_loss = 5.71055\n",
      "epoch no.12 train no.1430  loss = 6.26445 avg_loss = 5.72377\n",
      "epoch no.12 train no.1440  loss = 6.52499 avg_loss = 5.75711\n",
      "epoch no.12 train no.1450  loss = 5.27931 avg_loss = 5.73886\n",
      "epoch no.12 train no.1460  loss = 6.68615 avg_loss = 5.73392\n",
      "epoch no.12 train no.1470  loss = 5.64525 avg_loss = 5.72259\n",
      "epoch no.12 train no.1480  loss = 5.17652 avg_loss = 5.73916\n",
      "epoch no.12 train no.1490  loss = 5.82811 avg_loss = 5.76578\n",
      "epoch no.12 train no.1500  loss = 6.50169 avg_loss = 5.75723\n",
      "epoch no.12 train no.1510  loss = 5.94168 avg_loss = 5.73642\n",
      "epoch no.12 train no.1520  loss = 7.37585 avg_loss = 5.75800\n",
      "epoch no.12 train no.1530  loss = 7.05825 avg_loss = 5.78746\n",
      "epoch no.12 train no.1540  loss = 6.85493 avg_loss = 5.72507\n",
      "epoch no.12 train no.1550  loss = 6.01655 avg_loss = 5.76890\n",
      "epoch no.12 train no.1560  loss = 5.09247 avg_loss = 5.77054\n",
      "epoch no.12 train no.1570  loss = 5.67817 avg_loss = 5.77150\n",
      "epoch no.12 train no.1580  loss = 5.26984 avg_loss = 5.74220\n",
      "epoch no.12 train no.1590  loss = 4.03844 avg_loss = 5.71912\n",
      "epoch no.12 train no.1600  loss = 6.56430 avg_loss = 5.69988\n",
      "epoch no.12 train no.1610  loss = 5.70525 avg_loss = 5.73397\n",
      "epoch no.12 train no.1620  loss = 4.94822 avg_loss = 5.69623\n",
      "epoch no.12 train no.1630  loss = 3.95490 avg_loss = 5.69732\n",
      "epoch no.12 train no.1640  loss = 5.72710 avg_loss = 5.71940\n",
      "epoch no.12 train no.1650  loss = 3.57647 avg_loss = 5.69198\n",
      "epoch no.12 train no.1660  loss = 5.93855 avg_loss = 5.71382\n",
      "epoch no.12 train no.1670  loss = 6.19570 avg_loss = 5.74744\n",
      "epoch no.12 train no.1680  loss = 5.45607 avg_loss = 5.75136\n",
      "epoch no.12 train no.1690  loss = 6.45171 avg_loss = 5.69495\n",
      "epoch no.12 train no.1700  loss = 6.19279 avg_loss = 5.69371\n",
      "epoch no.12 train no.1710  loss = 5.11447 avg_loss = 5.65204\n",
      "epoch no.12 train no.1720  loss = 6.40852 avg_loss = 5.66265\n",
      "epoch no.12 train no.1730  loss = 4.57950 avg_loss = 5.66731\n",
      "epoch no.12 train no.1740  loss = 6.08874 avg_loss = 5.68471\n",
      "epoch no.12 train no.1750  loss = 6.08642 avg_loss = 5.69128\n",
      "epoch no.13 train no.0  loss = 4.53914 avg_loss = 5.68695\n",
      "epoch no.13 train no.10  loss = 6.00471 avg_loss = 5.68601\n",
      "epoch no.13 train no.20  loss = 6.73685 avg_loss = 5.71565\n",
      "epoch no.13 train no.30  loss = 4.68960 avg_loss = 5.70083\n",
      "epoch no.13 train no.40  loss = 4.92829 avg_loss = 5.69656\n",
      "epoch no.13 train no.50  loss = 7.01940 avg_loss = 5.67459\n",
      "epoch no.13 train no.60  loss = 6.13643 avg_loss = 5.67457\n",
      "epoch no.13 train no.70  loss = 6.10662 avg_loss = 5.68835\n",
      "epoch no.13 train no.80  loss = 5.73609 avg_loss = 5.71051\n",
      "epoch no.13 train no.90  loss = 6.00101 avg_loss = 5.73802\n",
      "epoch no.13 train no.100  loss = 6.07679 avg_loss = 5.74664\n",
      "epoch no.13 train no.110  loss = 4.64680 avg_loss = 5.70135\n",
      "epoch no.13 train no.120  loss = 6.16369 avg_loss = 5.70086\n",
      "epoch no.13 train no.130  loss = 5.01120 avg_loss = 5.69767\n",
      "epoch no.13 train no.140  loss = 4.95125 avg_loss = 5.70540\n",
      "epoch no.13 train no.150  loss = 5.95271 avg_loss = 5.68212\n",
      "epoch no.13 train no.160  loss = 4.99685 avg_loss = 5.66500\n",
      "epoch no.13 train no.170  loss = 6.10705 avg_loss = 5.70499\n",
      "epoch no.13 train no.180  loss = 6.08848 avg_loss = 5.67680\n",
      "epoch no.13 train no.190  loss = 6.30677 avg_loss = 5.68640\n",
      "epoch no.13 train no.200  loss = 5.32487 avg_loss = 5.63019\n",
      "epoch no.13 train no.210  loss = 6.86416 avg_loss = 5.63183\n",
      "epoch no.13 train no.220  loss = 5.31614 avg_loss = 5.66084\n",
      "epoch no.13 train no.230  loss = 6.71598 avg_loss = 5.63636\n",
      "epoch no.13 train no.240  loss = 5.37142 avg_loss = 5.63175\n",
      "epoch no.13 train no.250  loss = 4.27947 avg_loss = 5.56234\n",
      "epoch no.13 train no.260  loss = 6.00429 avg_loss = 5.57238\n",
      "epoch no.13 train no.270  loss = 6.81508 avg_loss = 5.59086\n",
      "epoch no.13 train no.280  loss = 5.14679 avg_loss = 5.57428\n",
      "epoch no.13 train no.290  loss = 5.51747 avg_loss = 5.54006\n",
      "epoch no.13 train no.300  loss = 5.72358 avg_loss = 5.55663\n",
      "epoch no.13 train no.310  loss = 6.06197 avg_loss = 5.61208\n",
      "epoch no.13 train no.320  loss = 5.20532 avg_loss = 5.60300\n",
      "epoch no.13 train no.330  loss = 4.34626 avg_loss = 5.60930\n",
      "epoch no.13 train no.340  loss = 5.84092 avg_loss = 5.65003\n",
      "epoch no.13 train no.350  loss = 7.24105 avg_loss = 5.66447\n",
      "epoch no.13 train no.360  loss = 5.53739 avg_loss = 5.65389\n",
      "epoch no.13 train no.370  loss = 4.80421 avg_loss = 5.61915\n",
      "epoch no.13 train no.380  loss = 4.57275 avg_loss = 5.57735\n",
      "epoch no.13 train no.390  loss = 5.18217 avg_loss = 5.56244\n",
      "epoch no.13 train no.400  loss = 5.13763 avg_loss = 5.55364\n",
      "epoch no.13 train no.410  loss = 4.97037 avg_loss = 5.55324\n",
      "epoch no.13 train no.420  loss = 4.08043 avg_loss = 5.51880\n",
      "epoch no.13 train no.430  loss = 5.26286 avg_loss = 5.50854\n",
      "epoch no.13 train no.440  loss = 5.59616 avg_loss = 5.52427\n",
      "epoch no.13 train no.450  loss = 6.06295 avg_loss = 5.51478\n",
      "epoch no.13 train no.460  loss = 5.46922 avg_loss = 5.55038\n",
      "epoch no.13 train no.470  loss = 4.97121 avg_loss = 5.54935\n",
      "epoch no.13 train no.480  loss = 4.97363 avg_loss = 5.52719\n",
      "epoch no.13 train no.490  loss = 4.97847 avg_loss = 5.53239\n",
      "epoch no.13 train no.500  loss = 6.56967 avg_loss = 5.58092\n",
      "epoch no.13 train no.510  loss = 6.67044 avg_loss = 5.61681\n",
      "epoch no.13 train no.520  loss = 6.03345 avg_loss = 5.62522\n",
      "epoch no.13 train no.530  loss = 4.62930 avg_loss = 5.59807\n",
      "epoch no.13 train no.540  loss = 6.43733 avg_loss = 5.62955\n",
      "epoch no.13 train no.550  loss = 6.27377 avg_loss = 5.64843\n",
      "epoch no.13 train no.560  loss = 6.52638 avg_loss = 5.64715\n",
      "epoch no.13 train no.570  loss = 6.36491 avg_loss = 5.67367\n",
      "epoch no.13 train no.580  loss = 7.43471 avg_loss = 5.68389\n",
      "epoch no.13 train no.590  loss = 4.53091 avg_loss = 5.70563\n",
      "epoch no.13 train no.600  loss = 5.16263 avg_loss = 5.66236\n",
      "epoch no.13 train no.610  loss = 6.17034 avg_loss = 5.67239\n",
      "epoch no.13 train no.620  loss = 6.12272 avg_loss = 5.66011\n",
      "epoch no.13 train no.630  loss = 5.49414 avg_loss = 5.65243\n",
      "epoch no.13 train no.640  loss = 6.36910 avg_loss = 5.63514\n",
      "epoch no.13 train no.650  loss = 6.41260 avg_loss = 5.65980\n",
      "epoch no.13 train no.660  loss = 5.28155 avg_loss = 5.63556\n",
      "epoch no.13 train no.670  loss = 4.74869 avg_loss = 5.62414\n",
      "epoch no.13 train no.680  loss = 6.01878 avg_loss = 5.62605\n",
      "epoch no.13 train no.690  loss = 4.33348 avg_loss = 5.61241\n",
      "epoch no.13 train no.700  loss = 5.94957 avg_loss = 5.60598\n",
      "epoch no.13 train no.710  loss = 6.15715 avg_loss = 5.59171\n",
      "epoch no.13 train no.720  loss = 5.70488 avg_loss = 5.61573\n",
      "epoch no.13 train no.730  loss = 5.58634 avg_loss = 5.57693\n",
      "epoch no.13 train no.740  loss = 6.13858 avg_loss = 5.56399\n",
      "epoch no.13 train no.750  loss = 5.17461 avg_loss = 5.55492\n",
      "epoch no.13 train no.760  loss = 5.00134 avg_loss = 5.58844\n",
      "epoch no.13 train no.770  loss = 4.72229 avg_loss = 5.60061\n",
      "epoch no.13 train no.780  loss = 5.89874 avg_loss = 5.60851\n",
      "epoch no.13 train no.790  loss = 5.22690 avg_loss = 5.63405\n",
      "epoch no.13 train no.800  loss = 6.73192 avg_loss = 5.67742\n",
      "epoch no.13 train no.810  loss = 4.08675 avg_loss = 5.63698\n",
      "epoch no.13 train no.820  loss = 4.72077 avg_loss = 5.65838\n",
      "epoch no.13 train no.830  loss = 4.98201 avg_loss = 5.67847\n",
      "epoch no.13 train no.840  loss = 4.20153 avg_loss = 5.72843\n",
      "epoch no.13 train no.850  loss = 6.17091 avg_loss = 5.73570\n",
      "epoch no.13 train no.860  loss = 6.14859 avg_loss = 5.71847\n",
      "epoch no.13 train no.870  loss = 7.12413 avg_loss = 5.72760\n",
      "epoch no.13 train no.880  loss = 5.29025 avg_loss = 5.74411\n",
      "epoch no.13 train no.890  loss = 5.20620 avg_loss = 5.75383\n",
      "epoch no.13 train no.900  loss = 6.16200 avg_loss = 5.78027\n",
      "epoch no.13 train no.910  loss = 6.30310 avg_loss = 5.78665\n",
      "epoch no.13 train no.920  loss = 4.82970 avg_loss = 5.81644\n",
      "epoch no.13 train no.930  loss = 4.30155 avg_loss = 5.76082\n",
      "epoch no.13 train no.940  loss = 4.95445 avg_loss = 5.74809\n",
      "epoch no.13 train no.950  loss = 4.19009 avg_loss = 5.71870\n",
      "epoch no.13 train no.960  loss = 4.35574 avg_loss = 5.70414\n",
      "epoch no.13 train no.970  loss = 5.69861 avg_loss = 5.66126\n",
      "epoch no.13 train no.980  loss = 5.47504 avg_loss = 5.70084\n",
      "epoch no.13 train no.990  loss = 5.45151 avg_loss = 5.72714\n",
      "epoch no.13 train no.1000  loss = 4.85380 avg_loss = 5.72907\n",
      "to_tokens: ['</s>', '</s>', '</s>']\n",
      "뭐함</s>\n",
      "epoch no.13 train no.1010  loss = 4.77380 avg_loss = 5.67575\n",
      "epoch no.13 train no.1020  loss = 3.31078 avg_loss = 5.61260\n",
      "epoch no.13 train no.1030  loss = 5.40737 avg_loss = 5.62031\n",
      "epoch no.13 train no.1040  loss = 6.83669 avg_loss = 5.61025\n",
      "epoch no.13 train no.1050  loss = 7.36318 avg_loss = 5.67764\n",
      "epoch no.13 train no.1060  loss = 7.22085 avg_loss = 5.65051\n",
      "epoch no.13 train no.1070  loss = 4.80839 avg_loss = 5.66435\n",
      "epoch no.13 train no.1080  loss = 5.71915 avg_loss = 5.71130\n",
      "epoch no.13 train no.1090  loss = 4.79777 avg_loss = 5.70412\n",
      "epoch no.13 train no.1100  loss = 4.92404 avg_loss = 5.67386\n",
      "epoch no.13 train no.1110  loss = 5.18557 avg_loss = 5.66327\n",
      "epoch no.13 train no.1120  loss = 7.46547 avg_loss = 5.68892\n",
      "epoch no.13 train no.1130  loss = 6.53018 avg_loss = 5.70518\n",
      "epoch no.13 train no.1140  loss = 6.50395 avg_loss = 5.67923\n",
      "epoch no.13 train no.1150  loss = 4.65522 avg_loss = 5.68380\n",
      "epoch no.13 train no.1160  loss = 6.55662 avg_loss = 5.70224\n",
      "epoch no.13 train no.1170  loss = 6.08455 avg_loss = 5.69946\n",
      "epoch no.13 train no.1180  loss = 6.96860 avg_loss = 5.69505\n",
      "epoch no.13 train no.1190  loss = 4.89500 avg_loss = 5.69553\n",
      "epoch no.13 train no.1200  loss = 6.60312 avg_loss = 5.66355\n",
      "epoch no.13 train no.1210  loss = 5.38236 avg_loss = 5.68060\n",
      "epoch no.13 train no.1220  loss = 4.38926 avg_loss = 5.70105\n",
      "epoch no.13 train no.1230  loss = 6.58188 avg_loss = 5.72159\n",
      "epoch no.13 train no.1240  loss = 4.65355 avg_loss = 5.75210\n",
      "epoch no.13 train no.1250  loss = 6.60538 avg_loss = 5.78254\n",
      "epoch no.13 train no.1260  loss = 5.09243 avg_loss = 5.75175\n",
      "epoch no.13 train no.1270  loss = 6.39857 avg_loss = 5.75646\n",
      "epoch no.13 train no.1280  loss = 4.30632 avg_loss = 5.72371\n",
      "epoch no.13 train no.1290  loss = 3.11441 avg_loss = 5.68215\n",
      "epoch no.13 train no.1300  loss = 5.50407 avg_loss = 5.68967\n",
      "epoch no.13 train no.1310  loss = 5.86481 avg_loss = 5.67999\n",
      "epoch no.13 train no.1320  loss = 5.64169 avg_loss = 5.70999\n",
      "epoch no.13 train no.1330  loss = 5.28877 avg_loss = 5.72026\n",
      "epoch no.13 train no.1340  loss = 7.17416 avg_loss = 5.70529\n",
      "epoch no.13 train no.1350  loss = 5.80865 avg_loss = 5.70031\n",
      "epoch no.13 train no.1360  loss = 5.67845 avg_loss = 5.68493\n",
      "epoch no.13 train no.1370  loss = 6.03708 avg_loss = 5.72264\n",
      "epoch no.13 train no.1380  loss = 6.38316 avg_loss = 5.75321\n",
      "epoch no.13 train no.1390  loss = 6.24989 avg_loss = 5.76779\n",
      "epoch no.13 train no.1400  loss = 4.56171 avg_loss = 5.74872\n",
      "epoch no.13 train no.1410  loss = 4.14257 avg_loss = 5.76299\n",
      "epoch no.13 train no.1420  loss = 5.83074 avg_loss = 5.74558\n",
      "epoch no.13 train no.1430  loss = 5.75329 avg_loss = 5.70315\n",
      "epoch no.13 train no.1440  loss = 5.53167 avg_loss = 5.72173\n",
      "epoch no.13 train no.1450  loss = 5.23593 avg_loss = 5.67120\n",
      "epoch no.13 train no.1460  loss = 7.40693 avg_loss = 5.73356\n",
      "epoch no.13 train no.1470  loss = 5.30534 avg_loss = 5.75485\n",
      "epoch no.13 train no.1480  loss = 6.42877 avg_loss = 5.72084\n",
      "epoch no.13 train no.1490  loss = 3.90199 avg_loss = 5.68510\n",
      "epoch no.13 train no.1500  loss = 5.68456 avg_loss = 5.71989\n",
      "epoch no.13 train no.1510  loss = 4.28289 avg_loss = 5.72050\n",
      "epoch no.13 train no.1520  loss = 5.27294 avg_loss = 5.69531\n",
      "epoch no.13 train no.1530  loss = 6.00759 avg_loss = 5.70435\n",
      "epoch no.13 train no.1540  loss = 5.92205 avg_loss = 5.72060\n",
      "epoch no.13 train no.1550  loss = 5.20557 avg_loss = 5.68852\n",
      "epoch no.13 train no.1560  loss = 6.28378 avg_loss = 5.71647\n",
      "epoch no.13 train no.1570  loss = 8.10700 avg_loss = 5.72149\n",
      "epoch no.13 train no.1580  loss = 6.39837 avg_loss = 5.71197\n",
      "epoch no.13 train no.1590  loss = 5.50315 avg_loss = 5.71140\n",
      "epoch no.13 train no.1600  loss = 5.10931 avg_loss = 5.68258\n",
      "epoch no.13 train no.1610  loss = 5.29677 avg_loss = 5.70163\n",
      "epoch no.13 train no.1620  loss = 4.78588 avg_loss = 5.72126\n",
      "epoch no.13 train no.1630  loss = 7.16250 avg_loss = 5.72779\n",
      "epoch no.13 train no.1640  loss = 5.66165 avg_loss = 5.73491\n",
      "epoch no.13 train no.1650  loss = 6.31947 avg_loss = 5.73304\n",
      "epoch no.13 train no.1660  loss = 5.75593 avg_loss = 5.75031\n",
      "epoch no.13 train no.1670  loss = 4.36833 avg_loss = 5.75268\n",
      "epoch no.13 train no.1680  loss = 4.14829 avg_loss = 5.71208\n",
      "epoch no.13 train no.1690  loss = 4.78695 avg_loss = 5.64376\n",
      "epoch no.13 train no.1700  loss = 4.31385 avg_loss = 5.63282\n",
      "epoch no.13 train no.1710  loss = 5.04587 avg_loss = 5.64471\n",
      "epoch no.13 train no.1720  loss = 4.90714 avg_loss = 5.64515\n",
      "epoch no.13 train no.1730  loss = 5.54473 avg_loss = 5.64997\n",
      "epoch no.13 train no.1740  loss = 6.49893 avg_loss = 5.64802\n",
      "epoch no.13 train no.1750  loss = 7.79528 avg_loss = 5.69193\n",
      "epoch no.14 train no.0  loss = 4.65532 avg_loss = 5.68747\n",
      "epoch no.14 train no.10  loss = 5.21591 avg_loss = 5.69666\n",
      "epoch no.14 train no.20  loss = 5.94440 avg_loss = 5.71292\n",
      "epoch no.14 train no.30  loss = 5.23171 avg_loss = 5.68624\n",
      "epoch no.14 train no.40  loss = 5.96354 avg_loss = 5.67833\n",
      "epoch no.14 train no.50  loss = 6.43112 avg_loss = 5.70604\n",
      "epoch no.14 train no.60  loss = 5.73365 avg_loss = 5.68467\n",
      "epoch no.14 train no.70  loss = 5.77594 avg_loss = 5.69436\n",
      "epoch no.14 train no.80  loss = 4.40403 avg_loss = 5.66549\n",
      "epoch no.14 train no.90  loss = 5.56409 avg_loss = 5.68004\n",
      "epoch no.14 train no.100  loss = 4.75340 avg_loss = 5.69274\n",
      "epoch no.14 train no.110  loss = 5.74514 avg_loss = 5.74963\n",
      "epoch no.14 train no.120  loss = 5.45300 avg_loss = 5.78415\n",
      "epoch no.14 train no.130  loss = 5.37889 avg_loss = 5.79040\n",
      "epoch no.14 train no.140  loss = 5.92746 avg_loss = 5.80889\n",
      "epoch no.14 train no.150  loss = 6.30229 avg_loss = 5.86148\n",
      "epoch no.14 train no.160  loss = 3.96116 avg_loss = 5.77066\n",
      "epoch no.14 train no.170  loss = 5.79688 avg_loss = 5.76304\n",
      "epoch no.14 train no.180  loss = 6.24130 avg_loss = 5.74125\n",
      "epoch no.14 train no.190  loss = 5.13381 avg_loss = 5.66922\n",
      "epoch no.14 train no.200  loss = 5.94510 avg_loss = 5.66594\n",
      "epoch no.14 train no.210  loss = 5.47801 avg_loss = 5.67730\n",
      "epoch no.14 train no.220  loss = 6.34398 avg_loss = 5.65077\n",
      "epoch no.14 train no.230  loss = 5.11310 avg_loss = 5.63261\n",
      "epoch no.14 train no.240  loss = 5.12735 avg_loss = 5.65589\n",
      "epoch no.14 train no.250  loss = 6.57421 avg_loss = 5.65704\n",
      "epoch no.14 train no.260  loss = 6.56867 avg_loss = 5.67555\n",
      "epoch no.14 train no.270  loss = 5.81831 avg_loss = 5.71002\n",
      "epoch no.14 train no.280  loss = 5.50306 avg_loss = 5.71009\n",
      "epoch no.14 train no.290  loss = 5.37770 avg_loss = 5.74204\n",
      "epoch no.14 train no.300  loss = 6.15351 avg_loss = 5.72957\n",
      "epoch no.14 train no.310  loss = 4.24624 avg_loss = 5.73176\n",
      "epoch no.14 train no.320  loss = 4.88735 avg_loss = 5.74294\n",
      "epoch no.14 train no.330  loss = 4.60093 avg_loss = 5.71811\n",
      "epoch no.14 train no.340  loss = 5.49498 avg_loss = 5.71498\n",
      "epoch no.14 train no.350  loss = 5.13664 avg_loss = 5.73378\n",
      "epoch no.14 train no.360  loss = 5.52574 avg_loss = 5.76426\n",
      "epoch no.14 train no.370  loss = 6.18300 avg_loss = 5.75395\n",
      "epoch no.14 train no.380  loss = 5.89031 avg_loss = 5.72912\n",
      "epoch no.14 train no.390  loss = 6.25470 avg_loss = 5.68952\n",
      "epoch no.14 train no.400  loss = 3.62580 avg_loss = 5.67613\n",
      "epoch no.14 train no.410  loss = 6.35238 avg_loss = 5.67179\n",
      "epoch no.14 train no.420  loss = 5.88895 avg_loss = 5.68537\n",
      "epoch no.14 train no.430  loss = 5.37778 avg_loss = 5.68702\n",
      "epoch no.14 train no.440  loss = 4.83099 avg_loss = 5.68490\n",
      "epoch no.14 train no.450  loss = 6.96499 avg_loss = 5.69398\n",
      "epoch no.14 train no.460  loss = 5.86948 avg_loss = 5.65311\n",
      "epoch no.14 train no.470  loss = 4.05508 avg_loss = 5.65387\n",
      "epoch no.14 train no.480  loss = 4.97283 avg_loss = 5.60739\n",
      "epoch no.14 train no.490  loss = 6.39450 avg_loss = 5.66050\n",
      "epoch no.14 train no.500  loss = 5.70361 avg_loss = 5.68359\n",
      "epoch no.14 train no.510  loss = 4.92663 avg_loss = 5.69841\n",
      "epoch no.14 train no.520  loss = 7.40232 avg_loss = 5.68440\n",
      "epoch no.14 train no.530  loss = 4.20786 avg_loss = 5.66600\n",
      "epoch no.14 train no.540  loss = 4.70335 avg_loss = 5.61939\n",
      "epoch no.14 train no.550  loss = 5.42012 avg_loss = 5.61812\n",
      "epoch no.14 train no.560  loss = 5.37595 avg_loss = 5.67133\n",
      "epoch no.14 train no.570  loss = 5.95134 avg_loss = 5.65250\n",
      "epoch no.14 train no.580  loss = 5.20039 avg_loss = 5.68244\n",
      "epoch no.14 train no.590  loss = 5.66445 avg_loss = 5.68745\n",
      "epoch no.14 train no.600  loss = 4.63318 avg_loss = 5.65047\n",
      "epoch no.14 train no.610  loss = 6.12522 avg_loss = 5.63917\n",
      "epoch no.14 train no.620  loss = 6.15048 avg_loss = 5.63318\n",
      "epoch no.14 train no.630  loss = 5.19124 avg_loss = 5.63337\n",
      "epoch no.14 train no.640  loss = 6.19959 avg_loss = 5.63135\n",
      "epoch no.14 train no.650  loss = 4.86803 avg_loss = 5.62819\n",
      "epoch no.14 train no.660  loss = 3.25988 avg_loss = 5.54216\n",
      "epoch no.14 train no.670  loss = 6.44305 avg_loss = 5.56146\n",
      "epoch no.14 train no.680  loss = 6.32167 avg_loss = 5.56958\n",
      "epoch no.14 train no.690  loss = 6.11036 avg_loss = 5.63562\n",
      "epoch no.14 train no.700  loss = 6.02484 avg_loss = 5.64762\n",
      "epoch no.14 train no.710  loss = 3.91424 avg_loss = 5.61318\n",
      "epoch no.14 train no.720  loss = 6.55889 avg_loss = 5.63198\n",
      "epoch no.14 train no.730  loss = 5.85190 avg_loss = 5.63130\n",
      "epoch no.14 train no.740  loss = 5.25294 avg_loss = 5.60925\n",
      "epoch no.14 train no.750  loss = 5.78039 avg_loss = 5.63834\n",
      "epoch no.14 train no.760  loss = 6.04769 avg_loss = 5.63494\n",
      "epoch no.14 train no.770  loss = 5.92002 avg_loss = 5.62470\n",
      "epoch no.14 train no.780  loss = 4.69823 avg_loss = 5.59526\n",
      "epoch no.14 train no.790  loss = 6.01255 avg_loss = 5.57445\n",
      "epoch no.14 train no.800  loss = 7.31250 avg_loss = 5.62342\n",
      "epoch no.14 train no.810  loss = 5.41351 avg_loss = 5.61589\n",
      "epoch no.14 train no.820  loss = 7.44815 avg_loss = 5.63010\n",
      "epoch no.14 train no.830  loss = 4.79109 avg_loss = 5.63605\n",
      "epoch no.14 train no.840  loss = 6.20416 avg_loss = 5.63785\n",
      "epoch no.14 train no.850  loss = 5.58808 avg_loss = 5.67157\n",
      "epoch no.14 train no.860  loss = 5.67759 avg_loss = 5.64951\n",
      "epoch no.14 train no.870  loss = 7.16891 avg_loss = 5.65795\n",
      "epoch no.14 train no.880  loss = 6.21769 avg_loss = 5.69269\n",
      "epoch no.14 train no.890  loss = 6.34920 avg_loss = 5.63274\n",
      "epoch no.14 train no.900  loss = 3.48975 avg_loss = 5.64551\n",
      "epoch no.14 train no.910  loss = 6.15071 avg_loss = 5.62480\n",
      "epoch no.14 train no.920  loss = 7.06287 avg_loss = 5.68691\n",
      "epoch no.14 train no.930  loss = 5.59764 avg_loss = 5.70866\n",
      "epoch no.14 train no.940  loss = 5.26438 avg_loss = 5.70233\n",
      "epoch no.14 train no.950  loss = 6.15307 avg_loss = 5.67512\n",
      "epoch no.14 train no.960  loss = 5.74932 avg_loss = 5.67474\n",
      "epoch no.14 train no.970  loss = 5.66643 avg_loss = 5.65864\n",
      "epoch no.14 train no.980  loss = 7.04109 avg_loss = 5.67348\n",
      "epoch no.14 train no.990  loss = 5.17382 avg_loss = 5.68749\n",
      "epoch no.14 train no.1000  loss = 5.93768 avg_loss = 5.69893\n",
      "to_tokens: ['</s>', '</s>', '</s>']\n",
      "뭐함</s>\n",
      "epoch no.14 train no.1010  loss = 4.26025 avg_loss = 5.68361\n",
      "epoch no.14 train no.1020  loss = 6.33651 avg_loss = 5.70869\n",
      "epoch no.14 train no.1030  loss = 5.51461 avg_loss = 5.71772\n",
      "epoch no.14 train no.1040  loss = 6.33231 avg_loss = 5.73105\n",
      "epoch no.14 train no.1050  loss = 5.13010 avg_loss = 5.71451\n",
      "epoch no.14 train no.1060  loss = 5.05079 avg_loss = 5.74334\n",
      "epoch no.14 train no.1070  loss = 5.71051 avg_loss = 5.78025\n",
      "epoch no.14 train no.1080  loss = 4.48920 avg_loss = 5.76989\n",
      "epoch no.14 train no.1090  loss = 4.26440 avg_loss = 5.78773\n",
      "epoch no.14 train no.1100  loss = 4.98058 avg_loss = 5.74765\n",
      "epoch no.14 train no.1110  loss = 4.67387 avg_loss = 5.66456\n",
      "epoch no.14 train no.1120  loss = 5.41616 avg_loss = 5.67545\n",
      "epoch no.14 train no.1130  loss = 7.04388 avg_loss = 5.70227\n",
      "epoch no.14 train no.1140  loss = 6.76892 avg_loss = 5.73488\n",
      "epoch no.14 train no.1150  loss = 4.38705 avg_loss = 5.73604\n",
      "epoch no.14 train no.1160  loss = 6.78102 avg_loss = 5.71271\n",
      "epoch no.14 train no.1170  loss = 4.30264 avg_loss = 5.71747\n",
      "epoch no.14 train no.1180  loss = 6.34051 avg_loss = 5.67153\n",
      "epoch no.14 train no.1190  loss = 7.13067 avg_loss = 5.68816\n",
      "epoch no.14 train no.1200  loss = 4.47820 avg_loss = 5.69659\n",
      "epoch no.14 train no.1210  loss = 6.99663 avg_loss = 5.71812\n",
      "epoch no.14 train no.1220  loss = 5.05564 avg_loss = 5.69883\n",
      "epoch no.14 train no.1230  loss = 5.06201 avg_loss = 5.71384\n",
      "epoch no.14 train no.1240  loss = 6.16883 avg_loss = 5.69583\n",
      "epoch no.14 train no.1250  loss = 6.67897 avg_loss = 5.71648\n",
      "epoch no.14 train no.1260  loss = 7.25963 avg_loss = 5.72437\n",
      "epoch no.14 train no.1270  loss = 6.45467 avg_loss = 5.72388\n",
      "epoch no.14 train no.1280  loss = 5.45215 avg_loss = 5.70129\n",
      "epoch no.14 train no.1290  loss = 6.27253 avg_loss = 5.68793\n",
      "epoch no.14 train no.1300  loss = 5.75831 avg_loss = 5.70443\n",
      "epoch no.14 train no.1310  loss = 6.96184 avg_loss = 5.71103\n",
      "epoch no.14 train no.1320  loss = 5.48615 avg_loss = 5.69694\n",
      "epoch no.14 train no.1330  loss = 6.05134 avg_loss = 5.69103\n",
      "epoch no.14 train no.1340  loss = 5.78129 avg_loss = 5.68138\n",
      "epoch no.14 train no.1350  loss = 6.49130 avg_loss = 5.68857\n",
      "epoch no.14 train no.1360  loss = 5.50900 avg_loss = 5.67079\n",
      "epoch no.14 train no.1370  loss = 5.59946 avg_loss = 5.70782\n",
      "epoch no.14 train no.1380  loss = 5.40192 avg_loss = 5.68677\n",
      "epoch no.14 train no.1390  loss = 4.64541 avg_loss = 5.69000\n",
      "epoch no.14 train no.1400  loss = 6.40485 avg_loss = 5.70396\n",
      "epoch no.14 train no.1410  loss = 5.53791 avg_loss = 5.74403\n",
      "epoch no.14 train no.1420  loss = 6.26675 avg_loss = 5.74690\n",
      "epoch no.14 train no.1430  loss = 4.20689 avg_loss = 5.70531\n",
      "epoch no.14 train no.1440  loss = 6.57411 avg_loss = 5.72113\n",
      "epoch no.14 train no.1450  loss = 6.44024 avg_loss = 5.72621\n",
      "epoch no.14 train no.1460  loss = 5.94751 avg_loss = 5.75871\n",
      "epoch no.14 train no.1470  loss = 7.40174 avg_loss = 5.75890\n",
      "epoch no.14 train no.1480  loss = 7.73843 avg_loss = 5.76309\n",
      "epoch no.14 train no.1490  loss = 6.86030 avg_loss = 5.77291\n",
      "epoch no.14 train no.1500  loss = 4.42612 avg_loss = 5.75631\n",
      "epoch no.14 train no.1510  loss = 3.84398 avg_loss = 5.77444\n",
      "epoch no.14 train no.1520  loss = 5.24566 avg_loss = 5.75196\n",
      "epoch no.14 train no.1530  loss = 5.55479 avg_loss = 5.68814\n",
      "epoch no.14 train no.1540  loss = 5.56942 avg_loss = 5.69798\n",
      "epoch no.14 train no.1550  loss = 5.15407 avg_loss = 5.69323\n",
      "epoch no.14 train no.1560  loss = 5.23502 avg_loss = 5.71894\n",
      "epoch no.14 train no.1570  loss = 5.18841 avg_loss = 5.71808\n",
      "epoch no.14 train no.1580  loss = 5.22268 avg_loss = 5.71557\n",
      "epoch no.14 train no.1590  loss = 3.66075 avg_loss = 5.68701\n",
      "epoch no.14 train no.1600  loss = 6.67070 avg_loss = 5.66272\n",
      "epoch no.14 train no.1610  loss = 5.52163 avg_loss = 5.65081\n",
      "epoch no.14 train no.1620  loss = 6.05049 avg_loss = 5.66048\n",
      "epoch no.14 train no.1630  loss = 6.28251 avg_loss = 5.64644\n",
      "epoch no.14 train no.1640  loss = 6.22826 avg_loss = 5.66177\n",
      "epoch no.14 train no.1650  loss = 4.94633 avg_loss = 5.64464\n",
      "epoch no.14 train no.1660  loss = 6.05757 avg_loss = 5.62170\n",
      "epoch no.14 train no.1670  loss = 6.07691 avg_loss = 5.68461\n",
      "epoch no.14 train no.1680  loss = 6.66437 avg_loss = 5.71000\n",
      "epoch no.14 train no.1690  loss = 5.84375 avg_loss = 5.68692\n",
      "epoch no.14 train no.1700  loss = 5.61291 avg_loss = 5.71931\n",
      "epoch no.14 train no.1710  loss = 5.65957 avg_loss = 5.69213\n",
      "epoch no.14 train no.1720  loss = 6.03684 avg_loss = 5.71927\n",
      "epoch no.14 train no.1730  loss = 4.82707 avg_loss = 5.69874\n",
      "epoch no.14 train no.1740  loss = 7.41169 avg_loss = 5.69428\n",
      "epoch no.14 train no.1750  loss = 7.42612 avg_loss = 5.70259\n",
      "epoch no.15 train no.0  loss = 4.11143 avg_loss = 5.69445\n",
      "epoch no.15 train no.10  loss = 6.11783 avg_loss = 5.70233\n",
      "epoch no.15 train no.20  loss = 5.78329 avg_loss = 5.64084\n",
      "epoch no.15 train no.30  loss = 4.15117 avg_loss = 5.63074\n",
      "epoch no.15 train no.40  loss = 4.57840 avg_loss = 5.60917\n",
      "epoch no.15 train no.50  loss = 6.11324 avg_loss = 5.64359\n",
      "epoch no.15 train no.60  loss = 6.20353 avg_loss = 5.64312\n",
      "epoch no.15 train no.70  loss = 6.68540 avg_loss = 5.67703\n",
      "epoch no.15 train no.80  loss = 6.92139 avg_loss = 5.71585\n",
      "epoch no.15 train no.90  loss = 4.93711 avg_loss = 5.70464\n",
      "epoch no.15 train no.100  loss = 5.91159 avg_loss = 5.69562\n",
      "epoch no.15 train no.110  loss = 5.46840 avg_loss = 5.67627\n",
      "epoch no.15 train no.120  loss = 6.75367 avg_loss = 5.65889\n",
      "epoch no.15 train no.130  loss = 5.55396 avg_loss = 5.69008\n",
      "epoch no.15 train no.140  loss = 4.72952 avg_loss = 5.66462\n",
      "epoch no.15 train no.150  loss = 5.17052 avg_loss = 5.65466\n",
      "epoch no.15 train no.160  loss = 5.89472 avg_loss = 5.62901\n",
      "epoch no.15 train no.170  loss = 5.53796 avg_loss = 5.63021\n",
      "epoch no.15 train no.180  loss = 5.30351 avg_loss = 5.59014\n",
      "epoch no.15 train no.190  loss = 4.84737 avg_loss = 5.57439\n",
      "epoch no.15 train no.200  loss = 5.98270 avg_loss = 5.59291\n",
      "epoch no.15 train no.210  loss = 5.20544 avg_loss = 5.62040\n",
      "epoch no.15 train no.220  loss = 6.55621 avg_loss = 5.64478\n",
      "epoch no.15 train no.230  loss = 7.10774 avg_loss = 5.61241\n",
      "epoch no.15 train no.240  loss = 5.05191 avg_loss = 5.64550\n",
      "epoch no.15 train no.250  loss = 5.29420 avg_loss = 5.58613\n",
      "epoch no.15 train no.260  loss = 6.21037 avg_loss = 5.59729\n",
      "epoch no.15 train no.270  loss = 6.90641 avg_loss = 5.62650\n",
      "epoch no.15 train no.280  loss = 5.90171 avg_loss = 5.66207\n",
      "epoch no.15 train no.290  loss = 5.53489 avg_loss = 5.62508\n",
      "epoch no.15 train no.300  loss = 5.84942 avg_loss = 5.59023\n",
      "epoch no.15 train no.310  loss = 6.31867 avg_loss = 5.56668\n",
      "epoch no.15 train no.320  loss = 4.28503 avg_loss = 5.59353\n",
      "epoch no.15 train no.330  loss = 6.88267 avg_loss = 5.61228\n",
      "epoch no.15 train no.340  loss = 4.74434 avg_loss = 5.58885\n",
      "epoch no.15 train no.350  loss = 5.98811 avg_loss = 5.60069\n",
      "epoch no.15 train no.360  loss = 5.82143 avg_loss = 5.63573\n",
      "epoch no.15 train no.370  loss = 6.10779 avg_loss = 5.66868\n",
      "epoch no.15 train no.380  loss = 4.69349 avg_loss = 5.64937\n",
      "epoch no.15 train no.390  loss = 6.16606 avg_loss = 5.63203\n",
      "epoch no.15 train no.400  loss = 5.54942 avg_loss = 5.63145\n",
      "epoch no.15 train no.410  loss = 5.41893 avg_loss = 5.57837\n",
      "epoch no.15 train no.420  loss = 5.01836 avg_loss = 5.58575\n",
      "epoch no.15 train no.430  loss = 7.04908 avg_loss = 5.62696\n",
      "epoch no.15 train no.440  loss = 6.67845 avg_loss = 5.66969\n",
      "epoch no.15 train no.450  loss = 5.93315 avg_loss = 5.64190\n",
      "epoch no.15 train no.460  loss = 4.97039 avg_loss = 5.57416\n",
      "epoch no.15 train no.470  loss = 4.98162 avg_loss = 5.55051\n",
      "epoch no.15 train no.480  loss = 5.31343 avg_loss = 5.54245\n",
      "epoch no.15 train no.490  loss = 6.72742 avg_loss = 5.55106\n",
      "epoch no.15 train no.500  loss = 4.97820 avg_loss = 5.54410\n",
      "epoch no.15 train no.510  loss = 5.96314 avg_loss = 5.55388\n",
      "epoch no.15 train no.520  loss = 5.77657 avg_loss = 5.55499\n",
      "epoch no.15 train no.530  loss = 5.18359 avg_loss = 5.54962\n",
      "epoch no.15 train no.540  loss = 5.64317 avg_loss = 5.54538\n",
      "epoch no.15 train no.550  loss = 5.95020 avg_loss = 5.57165\n",
      "epoch no.15 train no.560  loss = 4.93050 avg_loss = 5.58827\n",
      "epoch no.15 train no.570  loss = 4.68131 avg_loss = 5.59120\n",
      "epoch no.15 train no.580  loss = 6.05423 avg_loss = 5.57632\n",
      "epoch no.15 train no.590  loss = 5.76165 avg_loss = 5.56734\n",
      "epoch no.15 train no.600  loss = 6.47386 avg_loss = 5.59170\n",
      "epoch no.15 train no.610  loss = 5.67230 avg_loss = 5.59304\n",
      "epoch no.15 train no.620  loss = 6.83248 avg_loss = 5.58172\n",
      "epoch no.15 train no.630  loss = 5.80591 avg_loss = 5.61329\n",
      "epoch no.15 train no.640  loss = 4.89334 avg_loss = 5.62040\n",
      "epoch no.15 train no.650  loss = 6.14447 avg_loss = 5.64771\n",
      "epoch no.15 train no.660  loss = 4.99835 avg_loss = 5.64259\n",
      "epoch no.15 train no.670  loss = 5.29449 avg_loss = 5.67874\n",
      "epoch no.15 train no.680  loss = 4.24171 avg_loss = 5.68043\n",
      "epoch no.15 train no.690  loss = 5.40291 avg_loss = 5.66088\n",
      "epoch no.15 train no.700  loss = 5.00968 avg_loss = 5.67078\n",
      "epoch no.15 train no.710  loss = 5.28642 avg_loss = 5.62257\n",
      "epoch no.15 train no.720  loss = 4.95696 avg_loss = 5.57172\n",
      "epoch no.15 train no.730  loss = 4.19168 avg_loss = 5.58141\n",
      "epoch no.15 train no.740  loss = 4.36034 avg_loss = 5.58069\n",
      "epoch no.15 train no.750  loss = 4.65223 avg_loss = 5.60144\n",
      "epoch no.15 train no.760  loss = 5.41328 avg_loss = 5.59697\n",
      "epoch no.15 train no.770  loss = 4.13671 avg_loss = 5.57137\n",
      "epoch no.15 train no.780  loss = 6.38651 avg_loss = 5.60611\n",
      "epoch no.15 train no.790  loss = 3.21325 avg_loss = 5.59576\n",
      "epoch no.15 train no.800  loss = 6.53243 avg_loss = 5.61261\n",
      "epoch no.15 train no.810  loss = 4.87427 avg_loss = 5.60619\n",
      "epoch no.15 train no.820  loss = 5.07254 avg_loss = 5.62192\n",
      "epoch no.15 train no.830  loss = 4.42584 avg_loss = 5.62712\n",
      "epoch no.15 train no.840  loss = 5.50499 avg_loss = 5.63460\n",
      "epoch no.15 train no.850  loss = 5.44690 avg_loss = 5.58644\n",
      "epoch no.15 train no.860  loss = 4.41296 avg_loss = 5.55087\n",
      "epoch no.15 train no.870  loss = 5.10237 avg_loss = 5.53855\n",
      "epoch no.15 train no.880  loss = 5.23034 avg_loss = 5.53754\n",
      "epoch no.15 train no.890  loss = 3.87593 avg_loss = 5.58595\n",
      "epoch no.15 train no.900  loss = 4.55235 avg_loss = 5.58503\n",
      "epoch no.15 train no.910  loss = 5.64220 avg_loss = 5.56381\n",
      "epoch no.15 train no.920  loss = 6.25734 avg_loss = 5.60165\n",
      "epoch no.15 train no.930  loss = 5.06137 avg_loss = 5.59903\n",
      "epoch no.15 train no.940  loss = 5.33183 avg_loss = 5.58095\n",
      "epoch no.15 train no.950  loss = 6.36367 avg_loss = 5.62334\n",
      "epoch no.15 train no.960  loss = 6.25254 avg_loss = 5.63288\n",
      "epoch no.15 train no.970  loss = 7.01514 avg_loss = 5.66296\n",
      "epoch no.15 train no.980  loss = 5.95193 avg_loss = 5.65095\n",
      "epoch no.15 train no.990  loss = 6.10958 avg_loss = 5.65559\n",
      "epoch no.15 train no.1000  loss = 5.91811 avg_loss = 5.71652\n",
      "to_tokens: ['</s>', '</s>', '</s>']\n",
      "뭐함</s>\n",
      "epoch no.15 train no.1010  loss = 5.83513 avg_loss = 5.72641\n",
      "epoch no.15 train no.1020  loss = 5.09936 avg_loss = 5.72518\n",
      "epoch no.15 train no.1030  loss = 4.80864 avg_loss = 5.69941\n",
      "epoch no.15 train no.1040  loss = 4.16569 avg_loss = 5.71500\n",
      "epoch no.15 train no.1050  loss = 4.32620 avg_loss = 5.68105\n",
      "epoch no.15 train no.1060  loss = 5.75647 avg_loss = 5.67758\n",
      "epoch no.15 train no.1070  loss = 5.68146 avg_loss = 5.65656\n",
      "epoch no.15 train no.1080  loss = 4.38342 avg_loss = 5.67973\n",
      "epoch no.15 train no.1090  loss = 6.29025 avg_loss = 5.65891\n",
      "epoch no.15 train no.1100  loss = 6.74610 avg_loss = 5.64784\n",
      "epoch no.15 train no.1110  loss = 5.45797 avg_loss = 5.64048\n",
      "epoch no.15 train no.1120  loss = 5.73298 avg_loss = 5.66000\n",
      "epoch no.15 train no.1130  loss = 4.62383 avg_loss = 5.67577\n",
      "epoch no.15 train no.1140  loss = 6.10138 avg_loss = 5.69613\n",
      "epoch no.15 train no.1150  loss = 5.07019 avg_loss = 5.68149\n",
      "epoch no.15 train no.1160  loss = 5.10167 avg_loss = 5.71157\n",
      "epoch no.15 train no.1170  loss = 6.14799 avg_loss = 5.74679\n",
      "epoch no.15 train no.1180  loss = 4.95829 avg_loss = 5.74052\n",
      "epoch no.15 train no.1190  loss = 5.57189 avg_loss = 5.72944\n",
      "epoch no.15 train no.1200  loss = 5.45326 avg_loss = 5.76116\n",
      "epoch no.15 train no.1210  loss = 4.91961 avg_loss = 5.73546\n",
      "epoch no.15 train no.1220  loss = 6.16621 avg_loss = 5.76631\n",
      "epoch no.15 train no.1230  loss = 5.32715 avg_loss = 5.75624\n",
      "epoch no.15 train no.1240  loss = 5.22009 avg_loss = 5.73233\n",
      "epoch no.15 train no.1250  loss = 6.50765 avg_loss = 5.71057\n",
      "epoch no.15 train no.1260  loss = 5.70422 avg_loss = 5.69341\n",
      "epoch no.15 train no.1270  loss = 4.86249 avg_loss = 5.69869\n",
      "epoch no.15 train no.1280  loss = 5.00249 avg_loss = 5.69606\n",
      "epoch no.15 train no.1290  loss = 6.48283 avg_loss = 5.67868\n",
      "epoch no.15 train no.1300  loss = 6.79341 avg_loss = 5.69450\n",
      "epoch no.15 train no.1310  loss = 6.22985 avg_loss = 5.69405\n",
      "epoch no.15 train no.1320  loss = 5.27597 avg_loss = 5.68117\n",
      "epoch no.15 train no.1330  loss = 6.51776 avg_loss = 5.63528\n",
      "epoch no.15 train no.1340  loss = 3.84946 avg_loss = 5.60786\n",
      "epoch no.15 train no.1350  loss = 4.05136 avg_loss = 5.60995\n",
      "epoch no.15 train no.1360  loss = 5.99105 avg_loss = 5.63246\n",
      "epoch no.15 train no.1370  loss = 2.94364 avg_loss = 5.61429\n",
      "epoch no.15 train no.1380  loss = 5.92470 avg_loss = 5.62902\n",
      "epoch no.15 train no.1390  loss = 6.51755 avg_loss = 5.63682\n",
      "epoch no.15 train no.1400  loss = 6.43044 avg_loss = 5.64368\n",
      "epoch no.15 train no.1410  loss = 5.72597 avg_loss = 5.67007\n",
      "epoch no.15 train no.1420  loss = 6.53270 avg_loss = 5.69658\n",
      "epoch no.15 train no.1430  loss = 3.46495 avg_loss = 5.68981\n",
      "epoch no.15 train no.1440  loss = 5.60704 avg_loss = 5.67743\n",
      "epoch no.15 train no.1450  loss = 6.87132 avg_loss = 5.67963\n",
      "epoch no.15 train no.1460  loss = 6.33059 avg_loss = 5.71049\n",
      "epoch no.15 train no.1470  loss = 5.73865 avg_loss = 5.69295\n",
      "epoch no.15 train no.1480  loss = 5.34034 avg_loss = 5.69205\n",
      "epoch no.15 train no.1490  loss = 5.68259 avg_loss = 5.72479\n",
      "epoch no.15 train no.1500  loss = 3.85853 avg_loss = 5.71174\n",
      "epoch no.15 train no.1510  loss = 5.76804 avg_loss = 5.72579\n",
      "epoch no.15 train no.1520  loss = 5.50677 avg_loss = 5.72440\n",
      "epoch no.15 train no.1530  loss = 4.81951 avg_loss = 5.72962\n",
      "epoch no.15 train no.1540  loss = 7.22043 avg_loss = 5.74151\n",
      "epoch no.15 train no.1550  loss = 5.62804 avg_loss = 5.77440\n",
      "epoch no.15 train no.1560  loss = 6.93012 avg_loss = 5.77469\n",
      "epoch no.15 train no.1570  loss = 5.19313 avg_loss = 5.77583\n",
      "epoch no.15 train no.1580  loss = 6.27077 avg_loss = 5.76564\n",
      "epoch no.15 train no.1590  loss = 4.98372 avg_loss = 5.76221\n",
      "epoch no.15 train no.1600  loss = 6.51858 avg_loss = 5.79115\n",
      "epoch no.15 train no.1610  loss = 6.94897 avg_loss = 5.78014\n",
      "epoch no.15 train no.1620  loss = 6.72993 avg_loss = 5.75028\n",
      "epoch no.15 train no.1630  loss = 6.30988 avg_loss = 5.76510\n",
      "epoch no.15 train no.1640  loss = 5.90799 avg_loss = 5.74180\n",
      "epoch no.15 train no.1650  loss = 5.35572 avg_loss = 5.73442\n",
      "epoch no.15 train no.1660  loss = 5.68629 avg_loss = 5.70801\n",
      "epoch no.15 train no.1670  loss = 5.82613 avg_loss = 5.72947\n",
      "epoch no.15 train no.1680  loss = 5.48881 avg_loss = 5.74393\n",
      "epoch no.15 train no.1690  loss = 5.91371 avg_loss = 5.73882\n",
      "epoch no.15 train no.1700  loss = 6.13863 avg_loss = 5.75471\n",
      "epoch no.15 train no.1710  loss = 6.26819 avg_loss = 5.71655\n",
      "epoch no.15 train no.1720  loss = 5.79826 avg_loss = 5.70759\n",
      "epoch no.15 train no.1730  loss = 7.13578 avg_loss = 5.71247\n",
      "epoch no.15 train no.1740  loss = 6.33381 avg_loss = 5.72229\n",
      "epoch no.15 train no.1750  loss = 4.90240 avg_loss = 5.70787\n",
      "epoch no.16 train no.0  loss = 6.30540 avg_loss = 5.70644\n",
      "epoch no.16 train no.10  loss = 6.42721 avg_loss = 5.72437\n",
      "epoch no.16 train no.20  loss = 6.36343 avg_loss = 5.75079\n",
      "epoch no.16 train no.30  loss = 4.17239 avg_loss = 5.74683\n",
      "epoch no.16 train no.40  loss = 5.68614 avg_loss = 5.73817\n",
      "epoch no.16 train no.50  loss = 4.58537 avg_loss = 5.73379\n",
      "epoch no.16 train no.60  loss = 6.67008 avg_loss = 5.73264\n",
      "epoch no.16 train no.70  loss = 6.11313 avg_loss = 5.69929\n",
      "epoch no.16 train no.80  loss = 5.05460 avg_loss = 5.68695\n",
      "epoch no.16 train no.90  loss = 4.49904 avg_loss = 5.68958\n",
      "epoch no.16 train no.100  loss = 4.52625 avg_loss = 5.69757\n",
      "epoch no.16 train no.110  loss = 5.98482 avg_loss = 5.71058\n",
      "epoch no.16 train no.120  loss = 6.46112 avg_loss = 5.71428\n",
      "epoch no.16 train no.130  loss = 5.77293 avg_loss = 5.72076\n",
      "epoch no.16 train no.140  loss = 5.42222 avg_loss = 5.70858\n",
      "epoch no.16 train no.150  loss = 6.14919 avg_loss = 5.72644\n",
      "epoch no.16 train no.160  loss = 4.28567 avg_loss = 5.71451\n",
      "epoch no.16 train no.170  loss = 5.18003 avg_loss = 5.70689\n",
      "epoch no.16 train no.180  loss = 6.23923 avg_loss = 5.68033\n",
      "epoch no.16 train no.190  loss = 6.07700 avg_loss = 5.71571\n",
      "epoch no.16 train no.200  loss = 6.69704 avg_loss = 5.72759\n",
      "epoch no.16 train no.210  loss = 6.41053 avg_loss = 5.75156\n",
      "epoch no.16 train no.220  loss = 5.85413 avg_loss = 5.70939\n",
      "epoch no.16 train no.230  loss = 6.35075 avg_loss = 5.71242\n",
      "epoch no.16 train no.240  loss = 5.59129 avg_loss = 5.73672\n",
      "epoch no.16 train no.250  loss = 6.87960 avg_loss = 5.73989\n",
      "epoch no.16 train no.260  loss = 6.87991 avg_loss = 5.72062\n",
      "epoch no.16 train no.270  loss = 5.77773 avg_loss = 5.74397\n",
      "epoch no.16 train no.280  loss = 5.79771 avg_loss = 5.73652\n",
      "epoch no.16 train no.290  loss = 6.63857 avg_loss = 5.74891\n",
      "epoch no.16 train no.300  loss = 4.26825 avg_loss = 5.66650\n",
      "epoch no.16 train no.310  loss = 5.56619 avg_loss = 5.62697\n",
      "epoch no.16 train no.320  loss = 5.03147 avg_loss = 5.61834\n",
      "epoch no.16 train no.330  loss = 3.59020 avg_loss = 5.60723\n",
      "epoch no.16 train no.340  loss = 4.60213 avg_loss = 5.64534\n",
      "epoch no.16 train no.350  loss = 5.78670 avg_loss = 5.66571\n",
      "epoch no.16 train no.360  loss = 6.79317 avg_loss = 5.65423\n",
      "epoch no.16 train no.370  loss = 5.08208 avg_loss = 5.65961\n",
      "epoch no.16 train no.380  loss = 5.83321 avg_loss = 5.70894\n",
      "epoch no.16 train no.390  loss = 4.60272 avg_loss = 5.69966\n",
      "epoch no.16 train no.400  loss = 6.14600 avg_loss = 5.71058\n",
      "epoch no.16 train no.410  loss = 6.68459 avg_loss = 5.71449\n",
      "epoch no.16 train no.420  loss = 6.15276 avg_loss = 5.71983\n",
      "epoch no.16 train no.430  loss = 4.77647 avg_loss = 5.71870\n",
      "epoch no.16 train no.440  loss = 5.18077 avg_loss = 5.73353\n",
      "epoch no.16 train no.450  loss = 7.01607 avg_loss = 5.75033\n",
      "epoch no.16 train no.460  loss = 5.50497 avg_loss = 5.73702\n",
      "epoch no.16 train no.470  loss = 3.51435 avg_loss = 5.71331\n",
      "epoch no.16 train no.480  loss = 5.93921 avg_loss = 5.74594\n",
      "epoch no.16 train no.490  loss = 4.88549 avg_loss = 5.70249\n",
      "epoch no.16 train no.500  loss = 5.49260 avg_loss = 5.68474\n",
      "epoch no.16 train no.510  loss = 5.17688 avg_loss = 5.65123\n",
      "epoch no.16 train no.520  loss = 5.39040 avg_loss = 5.63288\n",
      "epoch no.16 train no.530  loss = 6.16296 avg_loss = 5.63997\n",
      "epoch no.16 train no.540  loss = 5.43593 avg_loss = 5.63924\n",
      "epoch no.16 train no.550  loss = 6.55592 avg_loss = 5.66479\n",
      "epoch no.16 train no.560  loss = 6.52374 avg_loss = 5.64377\n",
      "epoch no.16 train no.570  loss = 5.26771 avg_loss = 5.63764\n",
      "epoch no.16 train no.580  loss = 5.76526 avg_loss = 5.64421\n",
      "epoch no.16 train no.590  loss = 6.38229 avg_loss = 5.67726\n",
      "epoch no.16 train no.600  loss = 6.08829 avg_loss = 5.67308\n",
      "epoch no.16 train no.610  loss = 4.04550 avg_loss = 5.66583\n",
      "epoch no.16 train no.620  loss = 5.64316 avg_loss = 5.68756\n",
      "epoch no.16 train no.630  loss = 5.86850 avg_loss = 5.69289\n",
      "epoch no.16 train no.640  loss = 4.03247 avg_loss = 5.66189\n",
      "epoch no.16 train no.650  loss = 4.80166 avg_loss = 5.66586\n",
      "epoch no.16 train no.660  loss = 5.17502 avg_loss = 5.65845\n",
      "epoch no.16 train no.670  loss = 4.54266 avg_loss = 5.66272\n",
      "epoch no.16 train no.680  loss = 5.51259 avg_loss = 5.62190\n",
      "epoch no.16 train no.690  loss = 4.63251 avg_loss = 5.58559\n",
      "epoch no.16 train no.700  loss = 5.65773 avg_loss = 5.61031\n",
      "epoch no.16 train no.710  loss = 4.54937 avg_loss = 5.61691\n",
      "epoch no.16 train no.720  loss = 5.18395 avg_loss = 5.63980\n",
      "epoch no.16 train no.730  loss = 4.92363 avg_loss = 5.66356\n",
      "epoch no.16 train no.740  loss = 6.54142 avg_loss = 5.66395\n",
      "epoch no.16 train no.750  loss = 5.17337 avg_loss = 5.63508\n",
      "epoch no.16 train no.760  loss = 5.71418 avg_loss = 5.60748\n",
      "epoch no.16 train no.770  loss = 5.09180 avg_loss = 5.61575\n",
      "epoch no.16 train no.780  loss = 5.32476 avg_loss = 5.61620\n",
      "epoch no.16 train no.790  loss = 4.49203 avg_loss = 5.61770\n",
      "epoch no.16 train no.800  loss = 5.35269 avg_loss = 5.64286\n",
      "epoch no.16 train no.810  loss = 4.53542 avg_loss = 5.62819\n",
      "epoch no.16 train no.820  loss = 6.39166 avg_loss = 5.58621\n",
      "epoch no.16 train no.830  loss = 7.27988 avg_loss = 5.63508\n",
      "epoch no.16 train no.840  loss = 6.94605 avg_loss = 5.68549\n",
      "epoch no.16 train no.850  loss = 5.11159 avg_loss = 5.64533\n",
      "epoch no.16 train no.860  loss = 5.78023 avg_loss = 5.66875\n",
      "epoch no.16 train no.870  loss = 6.03363 avg_loss = 5.69289\n",
      "epoch no.16 train no.880  loss = 5.64304 avg_loss = 5.74203\n",
      "epoch no.16 train no.890  loss = 5.46219 avg_loss = 5.70452\n",
      "epoch no.16 train no.900  loss = 5.21251 avg_loss = 5.68369\n",
      "epoch no.16 train no.910  loss = 5.42392 avg_loss = 5.69673\n",
      "epoch no.16 train no.920  loss = 4.76928 avg_loss = 5.67695\n",
      "epoch no.16 train no.930  loss = 6.19623 avg_loss = 5.68058\n",
      "epoch no.16 train no.940  loss = 5.51191 avg_loss = 5.69955\n",
      "epoch no.16 train no.950  loss = 5.94444 avg_loss = 5.69510\n",
      "epoch no.16 train no.960  loss = 6.10997 avg_loss = 5.71000\n",
      "epoch no.16 train no.970  loss = 4.70417 avg_loss = 5.65988\n",
      "epoch no.16 train no.980  loss = 6.87484 avg_loss = 5.68282\n",
      "epoch no.16 train no.990  loss = 6.27021 avg_loss = 5.67089\n",
      "epoch no.16 train no.1000  loss = 6.51355 avg_loss = 5.67245\n",
      "to_tokens: ['</s>', '</s>', '</s>']\n",
      "뭐함</s>\n",
      "epoch no.16 train no.1010  loss = 6.40312 avg_loss = 5.69078\n",
      "epoch no.16 train no.1020  loss = 6.05705 avg_loss = 5.68622\n",
      "epoch no.16 train no.1030  loss = 7.22829 avg_loss = 5.72307\n",
      "epoch no.16 train no.1040  loss = 6.01031 avg_loss = 5.68044\n",
      "epoch no.16 train no.1050  loss = 6.55581 avg_loss = 5.70655\n",
      "epoch no.16 train no.1060  loss = 6.14830 avg_loss = 5.71656\n",
      "epoch no.16 train no.1070  loss = 4.74519 avg_loss = 5.68946\n",
      "epoch no.16 train no.1080  loss = 5.85429 avg_loss = 5.64049\n",
      "epoch no.16 train no.1090  loss = 5.49841 avg_loss = 5.65479\n",
      "epoch no.16 train no.1100  loss = 5.37909 avg_loss = 5.67571\n",
      "epoch no.16 train no.1110  loss = 5.18497 avg_loss = 5.68593\n",
      "epoch no.16 train no.1120  loss = 5.73055 avg_loss = 5.64994\n",
      "epoch no.16 train no.1130  loss = 3.10220 avg_loss = 5.61539\n",
      "epoch no.16 train no.1140  loss = 5.09934 avg_loss = 5.61688\n",
      "epoch no.16 train no.1150  loss = 4.96863 avg_loss = 5.64407\n",
      "epoch no.16 train no.1160  loss = 6.45863 avg_loss = 5.63856\n",
      "epoch no.16 train no.1170  loss = 5.41554 avg_loss = 5.65205\n",
      "epoch no.16 train no.1180  loss = 5.71321 avg_loss = 5.67445\n",
      "epoch no.16 train no.1190  loss = 5.46534 avg_loss = 5.69337\n",
      "epoch no.16 train no.1200  loss = 4.08418 avg_loss = 5.65096\n",
      "epoch no.16 train no.1210  loss = 4.83779 avg_loss = 5.60172\n",
      "epoch no.16 train no.1220  loss = 5.40021 avg_loss = 5.60246\n",
      "epoch no.16 train no.1230  loss = 5.33523 avg_loss = 5.63401\n",
      "epoch no.16 train no.1240  loss = 5.25389 avg_loss = 5.63111\n",
      "epoch no.16 train no.1250  loss = 6.00491 avg_loss = 5.61510\n",
      "epoch no.16 train no.1260  loss = 5.97409 avg_loss = 5.63715\n",
      "epoch no.16 train no.1270  loss = 6.57072 avg_loss = 5.65520\n",
      "epoch no.16 train no.1280  loss = 6.10216 avg_loss = 5.63535\n",
      "epoch no.16 train no.1290  loss = 6.69972 avg_loss = 5.67590\n",
      "epoch no.16 train no.1300  loss = 5.91184 avg_loss = 5.70842\n",
      "epoch no.16 train no.1310  loss = 4.82885 avg_loss = 5.66286\n",
      "epoch no.16 train no.1320  loss = 4.42519 avg_loss = 5.68744\n",
      "epoch no.16 train no.1330  loss = 4.33683 avg_loss = 5.64073\n",
      "epoch no.16 train no.1340  loss = 6.37816 avg_loss = 5.65383\n",
      "epoch no.16 train no.1350  loss = 6.58592 avg_loss = 5.63812\n",
      "epoch no.16 train no.1360  loss = 5.56525 avg_loss = 5.63487\n",
      "epoch no.16 train no.1370  loss = 4.99413 avg_loss = 5.65358\n",
      "epoch no.16 train no.1380  loss = 5.23662 avg_loss = 5.63729\n",
      "epoch no.16 train no.1390  loss = 5.00814 avg_loss = 5.64182\n",
      "epoch no.16 train no.1400  loss = 6.18867 avg_loss = 5.64211\n",
      "epoch no.16 train no.1410  loss = 5.88562 avg_loss = 5.67614\n",
      "epoch no.16 train no.1420  loss = 5.32979 avg_loss = 5.67218\n",
      "epoch no.16 train no.1430  loss = 4.74269 avg_loss = 5.65911\n",
      "epoch no.16 train no.1440  loss = 5.04135 avg_loss = 5.62684\n",
      "epoch no.16 train no.1450  loss = 4.43740 avg_loss = 5.63483\n",
      "epoch no.16 train no.1460  loss = 4.05840 avg_loss = 5.64450\n",
      "epoch no.16 train no.1470  loss = 5.73520 avg_loss = 5.60425\n",
      "epoch no.16 train no.1480  loss = 5.42310 avg_loss = 5.60227\n",
      "epoch no.16 train no.1490  loss = 5.91320 avg_loss = 5.60477\n",
      "epoch no.16 train no.1500  loss = 4.63258 avg_loss = 5.63247\n",
      "epoch no.16 train no.1510  loss = 5.71792 avg_loss = 5.60376\n",
      "epoch no.16 train no.1520  loss = 7.58775 avg_loss = 5.63802\n",
      "epoch no.16 train no.1530  loss = 5.20828 avg_loss = 5.67320\n",
      "epoch no.16 train no.1540  loss = 6.27167 avg_loss = 5.68454\n",
      "epoch no.16 train no.1550  loss = 7.45043 avg_loss = 5.74900\n",
      "epoch no.16 train no.1560  loss = 5.06083 avg_loss = 5.75764\n",
      "epoch no.16 train no.1570  loss = 3.66951 avg_loss = 5.75422\n",
      "epoch no.16 train no.1580  loss = 5.19977 avg_loss = 5.75507\n",
      "epoch no.16 train no.1590  loss = 5.95332 avg_loss = 5.70772\n",
      "epoch no.16 train no.1600  loss = 6.41827 avg_loss = 5.67576\n",
      "epoch no.16 train no.1610  loss = 7.23591 avg_loss = 5.67543\n",
      "epoch no.16 train no.1620  loss = 5.19813 avg_loss = 5.62827\n",
      "epoch no.16 train no.1630  loss = 6.38278 avg_loss = 5.62947\n",
      "epoch no.16 train no.1640  loss = 4.44214 avg_loss = 5.62302\n",
      "epoch no.16 train no.1650  loss = 6.27189 avg_loss = 5.71532\n",
      "epoch no.16 train no.1660  loss = 5.18184 avg_loss = 5.70229\n",
      "epoch no.16 train no.1670  loss = 5.03636 avg_loss = 5.68376\n",
      "epoch no.16 train no.1680  loss = 6.28175 avg_loss = 5.70860\n",
      "epoch no.16 train no.1690  loss = 5.32464 avg_loss = 5.67685\n",
      "epoch no.16 train no.1700  loss = 7.11310 avg_loss = 5.71989\n",
      "epoch no.16 train no.1710  loss = 4.85157 avg_loss = 5.73574\n",
      "epoch no.16 train no.1720  loss = 5.79338 avg_loss = 5.72811\n",
      "epoch no.16 train no.1730  loss = 4.29563 avg_loss = 5.73867\n",
      "epoch no.16 train no.1740  loss = 4.51180 avg_loss = 5.71502\n",
      "epoch no.16 train no.1750  loss = 5.69342 avg_loss = 5.71331\n",
      "epoch no.17 train no.0  loss = 5.08247 avg_loss = 5.65691\n",
      "epoch no.17 train no.10  loss = 6.45979 avg_loss = 5.66409\n",
      "epoch no.17 train no.20  loss = 5.19381 avg_loss = 5.68430\n",
      "epoch no.17 train no.30  loss = 3.67110 avg_loss = 5.64897\n",
      "epoch no.17 train no.40  loss = 5.40602 avg_loss = 5.60777\n",
      "epoch no.17 train no.50  loss = 6.10410 avg_loss = 5.60168\n",
      "epoch no.17 train no.60  loss = 5.91399 avg_loss = 5.64520\n",
      "epoch no.17 train no.70  loss = 5.59918 avg_loss = 5.66326\n",
      "epoch no.17 train no.80  loss = 5.18051 avg_loss = 5.71159\n",
      "epoch no.17 train no.90  loss = 6.55775 avg_loss = 5.71586\n",
      "epoch no.17 train no.100  loss = 6.04714 avg_loss = 5.73614\n",
      "epoch no.17 train no.110  loss = 5.65307 avg_loss = 5.69826\n",
      "epoch no.17 train no.120  loss = 5.07307 avg_loss = 5.68946\n",
      "epoch no.17 train no.130  loss = 4.63000 avg_loss = 5.66676\n",
      "epoch no.17 train no.140  loss = 6.19699 avg_loss = 5.68582\n",
      "epoch no.17 train no.150  loss = 4.90506 avg_loss = 5.68861\n",
      "epoch no.17 train no.160  loss = 6.43728 avg_loss = 5.71390\n",
      "epoch no.17 train no.170  loss = 7.20494 avg_loss = 5.68603\n",
      "epoch no.17 train no.180  loss = 4.61768 avg_loss = 5.68150\n",
      "epoch no.17 train no.190  loss = 6.62206 avg_loss = 5.66746\n",
      "epoch no.17 train no.200  loss = 5.04911 avg_loss = 5.63814\n",
      "epoch no.17 train no.210  loss = 6.90711 avg_loss = 5.65603\n",
      "epoch no.17 train no.220  loss = 4.85205 avg_loss = 5.64243\n",
      "epoch no.17 train no.230  loss = 4.15177 avg_loss = 5.63372\n",
      "epoch no.17 train no.240  loss = 6.08343 avg_loss = 5.67853\n",
      "epoch no.17 train no.250  loss = 6.82046 avg_loss = 5.68011\n",
      "epoch no.17 train no.260  loss = 4.65234 avg_loss = 5.67780\n",
      "epoch no.17 train no.270  loss = 5.38522 avg_loss = 5.67199\n",
      "epoch no.17 train no.280  loss = 5.71906 avg_loss = 5.63804\n",
      "epoch no.17 train no.290  loss = 5.27761 avg_loss = 5.64004\n",
      "epoch no.17 train no.300  loss = 5.53903 avg_loss = 5.61846\n",
      "epoch no.17 train no.310  loss = 6.97555 avg_loss = 5.61308\n",
      "epoch no.17 train no.320  loss = 5.06936 avg_loss = 5.57920\n",
      "epoch no.17 train no.330  loss = 5.51670 avg_loss = 5.60353\n",
      "epoch no.17 train no.340  loss = 5.27508 avg_loss = 5.64947\n",
      "epoch no.17 train no.350  loss = 6.25383 avg_loss = 5.59673\n",
      "epoch no.17 train no.360  loss = 5.96263 avg_loss = 5.58870\n",
      "epoch no.17 train no.370  loss = 6.08969 avg_loss = 5.57787\n",
      "epoch no.17 train no.380  loss = 5.39325 avg_loss = 5.57578\n",
      "epoch no.17 train no.390  loss = 4.32760 avg_loss = 5.59459\n",
      "epoch no.17 train no.400  loss = 5.59810 avg_loss = 5.59030\n",
      "epoch no.17 train no.410  loss = 5.90364 avg_loss = 5.61689\n",
      "epoch no.17 train no.420  loss = 6.14997 avg_loss = 5.60911\n",
      "epoch no.17 train no.430  loss = 5.99060 avg_loss = 5.60955\n",
      "epoch no.17 train no.440  loss = 6.55217 avg_loss = 5.64131\n",
      "epoch no.17 train no.450  loss = 6.10337 avg_loss = 5.69194\n",
      "epoch no.17 train no.460  loss = 5.44889 avg_loss = 5.69192\n",
      "epoch no.17 train no.470  loss = 5.43063 avg_loss = 5.67402\n",
      "epoch no.17 train no.480  loss = 4.76172 avg_loss = 5.67134\n",
      "epoch no.17 train no.490  loss = 5.70852 avg_loss = 5.66165\n",
      "epoch no.17 train no.500  loss = 4.82030 avg_loss = 5.62800\n",
      "epoch no.17 train no.510  loss = 6.34542 avg_loss = 5.66425\n",
      "epoch no.17 train no.520  loss = 4.11333 avg_loss = 5.64484\n",
      "epoch no.17 train no.530  loss = 4.62266 avg_loss = 5.64216\n",
      "epoch no.17 train no.540  loss = 4.61898 avg_loss = 5.66647\n",
      "epoch no.17 train no.550  loss = 6.49464 avg_loss = 5.62255\n",
      "epoch no.17 train no.560  loss = 6.47189 avg_loss = 5.66586\n",
      "epoch no.17 train no.570  loss = 6.33317 avg_loss = 5.66043\n",
      "epoch no.17 train no.580  loss = 5.99355 avg_loss = 5.66416\n",
      "epoch no.17 train no.590  loss = 7.43644 avg_loss = 5.67784\n",
      "epoch no.17 train no.600  loss = 6.98784 avg_loss = 5.63898\n",
      "epoch no.17 train no.610  loss = 6.77794 avg_loss = 5.64919\n",
      "epoch no.17 train no.620  loss = 5.75771 avg_loss = 5.67933\n",
      "epoch no.17 train no.630  loss = 5.56607 avg_loss = 5.70747\n",
      "epoch no.17 train no.640  loss = 7.03598 avg_loss = 5.69799\n",
      "epoch no.17 train no.650  loss = 4.72424 avg_loss = 5.66083\n",
      "epoch no.17 train no.660  loss = 3.99603 avg_loss = 5.70372\n",
      "epoch no.17 train no.670  loss = 3.49623 avg_loss = 5.68363\n",
      "epoch no.17 train no.680  loss = 4.79657 avg_loss = 5.63529\n",
      "epoch no.17 train no.690  loss = 4.99067 avg_loss = 5.57702\n",
      "epoch no.17 train no.700  loss = 6.05857 avg_loss = 5.58781\n",
      "epoch no.17 train no.710  loss = 5.77515 avg_loss = 5.61600\n",
      "epoch no.17 train no.720  loss = 5.92884 avg_loss = 5.63411\n",
      "epoch no.17 train no.730  loss = 6.55485 avg_loss = 5.62356\n",
      "epoch no.17 train no.740  loss = 5.02463 avg_loss = 5.63579\n",
      "epoch no.17 train no.750  loss = 6.38697 avg_loss = 5.65158\n",
      "epoch no.17 train no.760  loss = 6.13403 avg_loss = 5.66596\n",
      "epoch no.17 train no.770  loss = 3.27983 avg_loss = 5.62612\n",
      "epoch no.17 train no.780  loss = 6.03270 avg_loss = 5.61536\n",
      "epoch no.17 train no.790  loss = 5.77651 avg_loss = 5.60598\n",
      "epoch no.17 train no.800  loss = 5.32556 avg_loss = 5.61640\n",
      "epoch no.17 train no.810  loss = 6.66878 avg_loss = 5.59876\n",
      "epoch no.17 train no.820  loss = 5.02097 avg_loss = 5.55895\n",
      "epoch no.17 train no.830  loss = 6.47737 avg_loss = 5.58279\n",
      "epoch no.17 train no.840  loss = 6.66631 avg_loss = 5.61753\n",
      "epoch no.17 train no.850  loss = 4.71522 avg_loss = 5.61203\n",
      "epoch no.17 train no.860  loss = 6.66927 avg_loss = 5.61310\n",
      "epoch no.17 train no.870  loss = 5.90672 avg_loss = 5.60721\n",
      "epoch no.17 train no.880  loss = 5.56946 avg_loss = 5.55170\n",
      "epoch no.17 train no.890  loss = 6.05423 avg_loss = 5.56643\n",
      "epoch no.17 train no.900  loss = 6.59041 avg_loss = 5.58414\n",
      "epoch no.17 train no.910  loss = 4.55430 avg_loss = 5.58327\n",
      "epoch no.17 train no.920  loss = 5.26451 avg_loss = 5.59987\n",
      "epoch no.17 train no.930  loss = 6.75551 avg_loss = 5.61062\n",
      "epoch no.17 train no.940  loss = 3.75551 avg_loss = 5.58485\n",
      "epoch no.17 train no.950  loss = 5.19785 avg_loss = 5.57862\n",
      "epoch no.17 train no.960  loss = 4.78520 avg_loss = 5.65802\n",
      "epoch no.17 train no.970  loss = 4.40752 avg_loss = 5.63343\n",
      "epoch no.17 train no.980  loss = 7.09071 avg_loss = 5.64698\n",
      "epoch no.17 train no.990  loss = 6.21424 avg_loss = 5.66285\n",
      "epoch no.17 train no.1000  loss = 4.87572 avg_loss = 5.61945\n",
      "to_tokens: ['</s>', '</s>', '</s>']\n",
      "뭐함</s>\n",
      "epoch no.17 train no.1010  loss = 5.72697 avg_loss = 5.64109\n",
      "epoch no.17 train no.1020  loss = 6.21822 avg_loss = 5.67961\n",
      "epoch no.17 train no.1030  loss = 6.94254 avg_loss = 5.67924\n",
      "epoch no.17 train no.1040  loss = 6.74737 avg_loss = 5.70993\n",
      "epoch no.17 train no.1050  loss = 6.22252 avg_loss = 5.70547\n",
      "epoch no.17 train no.1060  loss = 6.42812 avg_loss = 5.70315\n",
      "epoch no.17 train no.1070  loss = 4.17137 avg_loss = 5.66122\n",
      "epoch no.17 train no.1080  loss = 6.17136 avg_loss = 5.62880\n",
      "epoch no.17 train no.1090  loss = 5.97547 avg_loss = 5.63230\n",
      "epoch no.17 train no.1100  loss = 5.79129 avg_loss = 5.64494\n",
      "epoch no.17 train no.1110  loss = 4.92509 avg_loss = 5.62081\n",
      "epoch no.17 train no.1120  loss = 4.60928 avg_loss = 5.62284\n",
      "epoch no.17 train no.1130  loss = 5.74487 avg_loss = 5.59678\n",
      "epoch no.17 train no.1140  loss = 5.87577 avg_loss = 5.60444\n",
      "epoch no.17 train no.1150  loss = 5.42487 avg_loss = 5.62529\n",
      "epoch no.17 train no.1160  loss = 6.24192 avg_loss = 5.64657\n",
      "epoch no.17 train no.1170  loss = 5.16217 avg_loss = 5.65679\n",
      "epoch no.17 train no.1180  loss = 5.18787 avg_loss = 5.67610\n",
      "epoch no.17 train no.1190  loss = 6.14422 avg_loss = 5.70710\n",
      "epoch no.17 train no.1200  loss = 5.28307 avg_loss = 5.67801\n",
      "epoch no.17 train no.1210  loss = 4.31101 avg_loss = 5.67388\n",
      "epoch no.17 train no.1220  loss = 4.10619 avg_loss = 5.62217\n",
      "epoch no.17 train no.1230  loss = 6.48846 avg_loss = 5.66220\n",
      "epoch no.17 train no.1240  loss = 5.54908 avg_loss = 5.64358\n",
      "epoch no.17 train no.1250  loss = 5.72923 avg_loss = 5.59196\n",
      "epoch no.17 train no.1260  loss = 5.70958 avg_loss = 5.57740\n",
      "epoch no.17 train no.1270  loss = 4.56609 avg_loss = 5.54534\n",
      "epoch no.17 train no.1280  loss = 6.31249 avg_loss = 5.55852\n",
      "epoch no.17 train no.1290  loss = 4.95227 avg_loss = 5.59397\n",
      "epoch no.17 train no.1300  loss = 5.62816 avg_loss = 5.59752\n",
      "epoch no.17 train no.1310  loss = 5.33678 avg_loss = 5.57534\n",
      "epoch no.17 train no.1320  loss = 4.97388 avg_loss = 5.60414\n",
      "epoch no.17 train no.1330  loss = 5.81866 avg_loss = 5.63380\n",
      "epoch no.17 train no.1340  loss = 4.93803 avg_loss = 5.67669\n",
      "epoch no.17 train no.1350  loss = 3.27575 avg_loss = 5.63859\n",
      "epoch no.17 train no.1360  loss = 5.47698 avg_loss = 5.65761\n",
      "epoch no.17 train no.1370  loss = 6.20508 avg_loss = 5.62791\n",
      "epoch no.17 train no.1380  loss = 3.59382 avg_loss = 5.60051\n",
      "epoch no.17 train no.1390  loss = 6.53804 avg_loss = 5.61141\n",
      "epoch no.17 train no.1400  loss = 5.84802 avg_loss = 5.61560\n",
      "epoch no.17 train no.1410  loss = 3.88091 avg_loss = 5.63617\n",
      "epoch no.17 train no.1420  loss = 5.78993 avg_loss = 5.60740\n",
      "epoch no.17 train no.1430  loss = 6.57722 avg_loss = 5.62822\n",
      "epoch no.17 train no.1440  loss = 6.73512 avg_loss = 5.65687\n",
      "epoch no.17 train no.1450  loss = 6.01654 avg_loss = 5.61142\n",
      "epoch no.17 train no.1460  loss = 4.62116 avg_loss = 5.64036\n",
      "epoch no.17 train no.1470  loss = 6.74211 avg_loss = 5.64848\n",
      "epoch no.17 train no.1480  loss = 6.56304 avg_loss = 5.70901\n",
      "epoch no.17 train no.1490  loss = 4.49402 avg_loss = 5.68063\n",
      "epoch no.17 train no.1500  loss = 5.42903 avg_loss = 5.67403\n",
      "epoch no.17 train no.1510  loss = 5.96402 avg_loss = 5.66238\n",
      "epoch no.17 train no.1520  loss = 6.59812 avg_loss = 5.68469\n",
      "epoch no.17 train no.1530  loss = 5.67259 avg_loss = 5.73266\n",
      "epoch no.17 train no.1540  loss = 4.52998 avg_loss = 5.72576\n",
      "epoch no.17 train no.1550  loss = 6.31571 avg_loss = 5.71901\n",
      "epoch no.17 train no.1560  loss = 5.69562 avg_loss = 5.68812\n",
      "epoch no.17 train no.1570  loss = 5.20376 avg_loss = 5.65877\n",
      "epoch no.17 train no.1580  loss = 5.84159 avg_loss = 5.64818\n",
      "epoch no.17 train no.1590  loss = 7.06826 avg_loss = 5.64751\n",
      "epoch no.17 train no.1600  loss = 6.43148 avg_loss = 5.73199\n",
      "epoch no.17 train no.1610  loss = 6.93532 avg_loss = 5.76394\n",
      "epoch no.17 train no.1620  loss = 6.09342 avg_loss = 5.72938\n",
      "epoch no.17 train no.1630  loss = 4.83825 avg_loss = 5.74078\n",
      "epoch no.17 train no.1640  loss = 4.77190 avg_loss = 5.72538\n",
      "epoch no.17 train no.1650  loss = 6.70103 avg_loss = 5.78271\n",
      "epoch no.17 train no.1660  loss = 5.00840 avg_loss = 5.74373\n",
      "epoch no.17 train no.1670  loss = 6.43711 avg_loss = 5.74528\n",
      "epoch no.17 train no.1680  loss = 6.22636 avg_loss = 5.73035\n",
      "epoch no.17 train no.1690  loss = 4.14048 avg_loss = 5.71710\n",
      "epoch no.17 train no.1700  loss = 7.15350 avg_loss = 5.72756\n",
      "epoch no.17 train no.1710  loss = 4.84636 avg_loss = 5.68035\n",
      "epoch no.17 train no.1720  loss = 6.27458 avg_loss = 5.62997\n",
      "epoch no.17 train no.1730  loss = 5.13330 avg_loss = 5.63857\n",
      "epoch no.17 train no.1740  loss = 5.99059 avg_loss = 5.59882\n",
      "epoch no.17 train no.1750  loss = 5.90843 avg_loss = 5.62971\n",
      "epoch no.18 train no.0  loss = 4.41998 avg_loss = 5.64136\n",
      "epoch no.18 train no.10  loss = 5.37754 avg_loss = 5.65250\n",
      "epoch no.18 train no.20  loss = 4.39295 avg_loss = 5.63704\n",
      "epoch no.18 train no.30  loss = 5.92546 avg_loss = 5.63149\n",
      "epoch no.18 train no.40  loss = 4.74923 avg_loss = 5.61809\n",
      "epoch no.18 train no.50  loss = 5.05631 avg_loss = 5.63582\n",
      "epoch no.18 train no.60  loss = 6.83213 avg_loss = 5.59408\n",
      "epoch no.18 train no.70  loss = 5.99476 avg_loss = 5.59191\n",
      "epoch no.18 train no.80  loss = 6.74334 avg_loss = 5.61091\n",
      "epoch no.18 train no.90  loss = 3.71793 avg_loss = 5.59219\n",
      "epoch no.18 train no.100  loss = 5.93430 avg_loss = 5.59394\n",
      "epoch no.18 train no.110  loss = 5.80939 avg_loss = 5.59877\n",
      "epoch no.18 train no.120  loss = 5.18096 avg_loss = 5.60223\n",
      "epoch no.18 train no.130  loss = 6.47528 avg_loss = 5.57794\n",
      "epoch no.18 train no.140  loss = 3.97276 avg_loss = 5.55803\n",
      "epoch no.18 train no.150  loss = 6.79746 avg_loss = 5.58518\n",
      "epoch no.18 train no.160  loss = 6.30409 avg_loss = 5.56714\n",
      "epoch no.18 train no.170  loss = 5.35098 avg_loss = 5.54464\n",
      "epoch no.18 train no.180  loss = 5.81543 avg_loss = 5.55720\n",
      "epoch no.18 train no.190  loss = 7.00946 avg_loss = 5.58697\n",
      "epoch no.18 train no.200  loss = 5.14644 avg_loss = 5.57095\n",
      "epoch no.18 train no.210  loss = 4.53226 avg_loss = 5.54771\n",
      "epoch no.18 train no.220  loss = 6.56446 avg_loss = 5.56637\n",
      "epoch no.18 train no.230  loss = 4.82563 avg_loss = 5.58040\n",
      "epoch no.18 train no.240  loss = 4.78540 avg_loss = 5.58606\n",
      "epoch no.18 train no.250  loss = 5.10742 avg_loss = 5.55536\n",
      "epoch no.18 train no.260  loss = 5.63927 avg_loss = 5.60555\n",
      "epoch no.18 train no.270  loss = 6.32911 avg_loss = 5.63502\n",
      "epoch no.18 train no.280  loss = 5.40791 avg_loss = 5.63210\n",
      "epoch no.18 train no.290  loss = 6.04947 avg_loss = 5.64619\n",
      "epoch no.18 train no.300  loss = 4.80452 avg_loss = 5.59018\n",
      "epoch no.18 train no.310  loss = 6.23459 avg_loss = 5.60274\n",
      "epoch no.18 train no.320  loss = 5.15820 avg_loss = 5.62184\n",
      "epoch no.18 train no.330  loss = 5.05501 avg_loss = 5.63567\n",
      "epoch no.18 train no.340  loss = 3.09379 avg_loss = 5.59681\n",
      "epoch no.18 train no.350  loss = 6.01551 avg_loss = 5.57401\n",
      "epoch no.18 train no.360  loss = 6.39802 avg_loss = 5.61927\n",
      "epoch no.18 train no.370  loss = 5.91931 avg_loss = 5.61788\n",
      "epoch no.18 train no.380  loss = 5.41043 avg_loss = 5.62950\n",
      "epoch no.18 train no.390  loss = 5.60536 avg_loss = 5.63431\n",
      "epoch no.18 train no.400  loss = 4.50225 avg_loss = 5.63605\n",
      "epoch no.18 train no.410  loss = 7.02722 avg_loss = 5.67039\n",
      "epoch no.18 train no.420  loss = 5.25499 avg_loss = 5.68253\n",
      "epoch no.18 train no.430  loss = 6.10211 avg_loss = 5.70820\n",
      "epoch no.18 train no.440  loss = 5.25774 avg_loss = 5.73703\n",
      "epoch no.18 train no.450  loss = 5.01777 avg_loss = 5.70494\n",
      "epoch no.18 train no.460  loss = 6.90221 avg_loss = 5.72521\n",
      "epoch no.18 train no.470  loss = 5.28079 avg_loss = 5.70944\n",
      "epoch no.18 train no.480  loss = 4.47161 avg_loss = 5.68857\n",
      "epoch no.18 train no.490  loss = 4.75187 avg_loss = 5.64646\n",
      "epoch no.18 train no.500  loss = 5.22760 avg_loss = 5.62181\n",
      "epoch no.18 train no.510  loss = 5.36393 avg_loss = 5.66266\n",
      "epoch no.18 train no.520  loss = 4.30260 avg_loss = 5.66816\n",
      "epoch no.18 train no.530  loss = 7.04197 avg_loss = 5.63575\n",
      "epoch no.18 train no.540  loss = 5.29822 avg_loss = 5.63323\n",
      "epoch no.18 train no.550  loss = 5.56840 avg_loss = 5.61644\n",
      "epoch no.18 train no.560  loss = 7.10059 avg_loss = 5.65356\n",
      "epoch no.18 train no.570  loss = 6.06228 avg_loss = 5.61965\n",
      "epoch no.18 train no.580  loss = 4.22531 avg_loss = 5.62792\n",
      "epoch no.18 train no.590  loss = 4.16419 avg_loss = 5.58600\n",
      "epoch no.18 train no.600  loss = 4.54474 avg_loss = 5.55847\n",
      "epoch no.18 train no.610  loss = 6.82209 avg_loss = 5.58819\n",
      "epoch no.18 train no.620  loss = 5.86831 avg_loss = 5.63089\n",
      "epoch no.18 train no.630  loss = 4.70960 avg_loss = 5.63443\n",
      "epoch no.18 train no.640  loss = 4.88959 avg_loss = 5.63463\n",
      "epoch no.18 train no.650  loss = 5.96697 avg_loss = 5.60009\n",
      "epoch no.18 train no.660  loss = 5.05250 avg_loss = 5.59832\n",
      "epoch no.18 train no.670  loss = 4.06791 avg_loss = 5.60508\n",
      "epoch no.18 train no.680  loss = 6.69547 avg_loss = 5.60706\n",
      "epoch no.18 train no.690  loss = 4.80159 avg_loss = 5.57059\n",
      "epoch no.18 train no.700  loss = 3.98024 avg_loss = 5.53505\n",
      "epoch no.18 train no.710  loss = 6.33928 avg_loss = 5.57432\n",
      "epoch no.18 train no.720  loss = 5.71391 avg_loss = 5.61534\n",
      "epoch no.18 train no.730  loss = 5.18263 avg_loss = 5.60356\n",
      "epoch no.18 train no.740  loss = 4.49060 avg_loss = 5.66119\n",
      "epoch no.18 train no.750  loss = 6.52890 avg_loss = 5.66950\n",
      "epoch no.18 train no.760  loss = 5.92830 avg_loss = 5.65933\n",
      "epoch no.18 train no.770  loss = 6.18254 avg_loss = 5.67547\n",
      "epoch no.18 train no.780  loss = 5.82350 avg_loss = 5.68429\n",
      "epoch no.18 train no.790  loss = 6.53035 avg_loss = 5.65641\n",
      "epoch no.18 train no.800  loss = 5.82818 avg_loss = 5.65585\n",
      "epoch no.18 train no.810  loss = 5.95997 avg_loss = 5.62268\n",
      "epoch no.18 train no.820  loss = 5.57702 avg_loss = 5.57450\n",
      "epoch no.18 train no.830  loss = 6.54544 avg_loss = 5.60087\n",
      "epoch no.18 train no.840  loss = 4.77104 avg_loss = 5.56769\n",
      "epoch no.18 train no.850  loss = 4.77390 avg_loss = 5.60528\n",
      "epoch no.18 train no.860  loss = 5.83206 avg_loss = 5.61925\n",
      "epoch no.18 train no.870  loss = 4.65227 avg_loss = 5.59413\n",
      "epoch no.18 train no.880  loss = 6.19462 avg_loss = 5.62357\n",
      "epoch no.18 train no.890  loss = 4.68116 avg_loss = 5.62973\n",
      "epoch no.18 train no.900  loss = 4.51756 avg_loss = 5.63143\n",
      "epoch no.18 train no.910  loss = 5.10438 avg_loss = 5.64485\n",
      "epoch no.18 train no.920  loss = 5.87168 avg_loss = 5.65861\n",
      "epoch no.18 train no.930  loss = 4.48244 avg_loss = 5.63573\n",
      "epoch no.18 train no.940  loss = 7.61245 avg_loss = 5.65446\n",
      "epoch no.18 train no.950  loss = 7.40601 avg_loss = 5.66778\n",
      "epoch no.18 train no.960  loss = 4.50436 avg_loss = 5.68648\n",
      "epoch no.18 train no.970  loss = 5.80082 avg_loss = 5.70769\n",
      "epoch no.18 train no.980  loss = 5.94048 avg_loss = 5.69163\n",
      "epoch no.18 train no.990  loss = 5.71827 avg_loss = 5.69553\n",
      "epoch no.18 train no.1000  loss = 6.32478 avg_loss = 5.70270\n",
      "to_tokens: ['</s>', '</s>', '</s>']\n",
      "뭐함</s>\n",
      "epoch no.18 train no.1010  loss = 3.82570 avg_loss = 5.65715\n",
      "epoch no.18 train no.1020  loss = 7.07153 avg_loss = 5.69589\n",
      "epoch no.18 train no.1030  loss = 6.81118 avg_loss = 5.71368\n",
      "epoch no.18 train no.1040  loss = 5.34671 avg_loss = 5.66931\n",
      "epoch no.18 train no.1050  loss = 6.36098 avg_loss = 5.67068\n",
      "epoch no.18 train no.1060  loss = 4.97801 avg_loss = 5.67307\n",
      "epoch no.18 train no.1070  loss = 5.01465 avg_loss = 5.67983\n",
      "epoch no.18 train no.1080  loss = 3.90253 avg_loss = 5.66334\n",
      "epoch no.18 train no.1090  loss = 6.33658 avg_loss = 5.65119\n",
      "epoch no.18 train no.1100  loss = 5.09199 avg_loss = 5.68262\n",
      "epoch no.18 train no.1110  loss = 5.86543 avg_loss = 5.72417\n",
      "epoch no.18 train no.1120  loss = 5.22988 avg_loss = 5.71839\n",
      "epoch no.18 train no.1130  loss = 5.36383 avg_loss = 5.72004\n",
      "epoch no.18 train no.1140  loss = 4.21904 avg_loss = 5.68288\n",
      "epoch no.18 train no.1150  loss = 5.19265 avg_loss = 5.70366\n",
      "epoch no.18 train no.1160  loss = 7.12435 avg_loss = 5.70774\n",
      "epoch no.18 train no.1170  loss = 6.74935 avg_loss = 5.74390\n",
      "epoch no.18 train no.1180  loss = 7.26392 avg_loss = 5.71855\n",
      "epoch no.18 train no.1190  loss = 4.90364 avg_loss = 5.74600\n",
      "epoch no.18 train no.1200  loss = 6.66227 avg_loss = 5.76110\n",
      "epoch no.18 train no.1210  loss = 6.15774 avg_loss = 5.75822\n",
      "epoch no.18 train no.1220  loss = 6.43276 avg_loss = 5.78434\n",
      "epoch no.18 train no.1230  loss = 5.90502 avg_loss = 5.75148\n",
      "epoch no.18 train no.1240  loss = 6.62362 avg_loss = 5.78792\n",
      "epoch no.18 train no.1250  loss = 5.41718 avg_loss = 5.74634\n",
      "epoch no.18 train no.1260  loss = 3.01494 avg_loss = 5.69219\n",
      "epoch no.18 train no.1270  loss = 7.17143 avg_loss = 5.67594\n",
      "epoch no.18 train no.1280  loss = 5.87504 avg_loss = 5.66713\n",
      "epoch no.18 train no.1290  loss = 5.83669 avg_loss = 5.66749\n",
      "epoch no.18 train no.1300  loss = 5.83546 avg_loss = 5.65104\n",
      "epoch no.18 train no.1310  loss = 6.52810 avg_loss = 5.70461\n",
      "epoch no.18 train no.1320  loss = 6.79867 avg_loss = 5.74920\n",
      "epoch no.18 train no.1330  loss = 5.94594 avg_loss = 5.74998\n",
      "epoch no.18 train no.1340  loss = 6.80954 avg_loss = 5.73103\n",
      "epoch no.18 train no.1350  loss = 5.11295 avg_loss = 5.71332\n",
      "epoch no.18 train no.1360  loss = 7.23305 avg_loss = 5.70730\n",
      "epoch no.18 train no.1370  loss = 5.82040 avg_loss = 5.74210\n",
      "epoch no.18 train no.1380  loss = 5.03364 avg_loss = 5.68898\n",
      "epoch no.18 train no.1390  loss = 5.10762 avg_loss = 5.70740\n",
      "epoch no.18 train no.1400  loss = 6.20616 avg_loss = 5.70278\n",
      "epoch no.18 train no.1410  loss = 5.43003 avg_loss = 5.69153\n",
      "epoch no.18 train no.1420  loss = 5.38523 avg_loss = 5.68851\n",
      "epoch no.18 train no.1430  loss = 6.72899 avg_loss = 5.70045\n",
      "epoch no.18 train no.1440  loss = 6.31670 avg_loss = 5.66728\n",
      "epoch no.18 train no.1450  loss = 5.74171 avg_loss = 5.69994\n",
      "epoch no.18 train no.1460  loss = 5.41515 avg_loss = 5.69702\n",
      "epoch no.18 train no.1470  loss = 4.92158 avg_loss = 5.70367\n",
      "epoch no.18 train no.1480  loss = 5.58660 avg_loss = 5.70391\n",
      "epoch no.18 train no.1490  loss = 5.58669 avg_loss = 5.65090\n",
      "epoch no.18 train no.1500  loss = 5.42527 avg_loss = 5.62971\n",
      "epoch no.18 train no.1510  loss = 6.06731 avg_loss = 5.66129\n",
      "epoch no.18 train no.1520  loss = 6.65055 avg_loss = 5.64999\n",
      "epoch no.18 train no.1530  loss = 5.10722 avg_loss = 5.64591\n",
      "epoch no.18 train no.1540  loss = 5.36626 avg_loss = 5.61164\n",
      "epoch no.18 train no.1550  loss = 4.95097 avg_loss = 5.58368\n",
      "epoch no.18 train no.1560  loss = 5.90538 avg_loss = 5.55230\n",
      "epoch no.18 train no.1570  loss = 5.55396 avg_loss = 5.54972\n",
      "epoch no.18 train no.1580  loss = 6.30895 avg_loss = 5.57199\n",
      "epoch no.18 train no.1590  loss = 5.05894 avg_loss = 5.55691\n",
      "epoch no.18 train no.1600  loss = 5.97294 avg_loss = 5.55304\n",
      "epoch no.18 train no.1610  loss = 6.25281 avg_loss = 5.55374\n",
      "epoch no.18 train no.1620  loss = 5.32293 avg_loss = 5.54127\n",
      "epoch no.18 train no.1630  loss = 5.64597 avg_loss = 5.56578\n",
      "epoch no.18 train no.1640  loss = 6.08489 avg_loss = 5.56670\n",
      "epoch no.18 train no.1650  loss = 5.49717 avg_loss = 5.60877\n",
      "epoch no.18 train no.1660  loss = 6.16803 avg_loss = 5.61691\n",
      "epoch no.18 train no.1670  loss = 5.23843 avg_loss = 5.59350\n",
      "epoch no.18 train no.1680  loss = 5.52801 avg_loss = 5.55590\n",
      "epoch no.18 train no.1690  loss = 6.02713 avg_loss = 5.56370\n",
      "epoch no.18 train no.1700  loss = 6.33211 avg_loss = 5.61829\n",
      "epoch no.18 train no.1710  loss = 7.13881 avg_loss = 5.62588\n",
      "epoch no.18 train no.1720  loss = 3.74671 avg_loss = 5.56223\n",
      "epoch no.18 train no.1730  loss = 6.67394 avg_loss = 5.59820\n",
      "epoch no.18 train no.1740  loss = 6.12333 avg_loss = 5.58704\n",
      "epoch no.18 train no.1750  loss = 4.90714 avg_loss = 5.58951\n",
      "epoch no.19 train no.0  loss = 4.94550 avg_loss = 5.57548\n",
      "epoch no.19 train no.10  loss = 6.04147 avg_loss = 5.54375\n",
      "epoch no.19 train no.20  loss = 7.18209 avg_loss = 5.53147\n",
      "epoch no.19 train no.30  loss = 4.76147 avg_loss = 5.55767\n",
      "epoch no.19 train no.40  loss = 6.11845 avg_loss = 5.58205\n",
      "epoch no.19 train no.50  loss = 5.60954 avg_loss = 5.57697\n",
      "epoch no.19 train no.60  loss = 5.02890 avg_loss = 5.58820\n",
      "epoch no.19 train no.70  loss = 6.22625 avg_loss = 5.57019\n",
      "epoch no.19 train no.80  loss = 4.55513 avg_loss = 5.56336\n",
      "epoch no.19 train no.90  loss = 4.62367 avg_loss = 5.57666\n",
      "epoch no.19 train no.100  loss = 5.74103 avg_loss = 5.56021\n",
      "epoch no.19 train no.110  loss = 6.53740 avg_loss = 5.55183\n",
      "epoch no.19 train no.120  loss = 6.92358 avg_loss = 5.51749\n",
      "epoch no.19 train no.130  loss = 4.01480 avg_loss = 5.53220\n",
      "epoch no.19 train no.140  loss = 5.78875 avg_loss = 5.54521\n",
      "epoch no.19 train no.150  loss = 7.69326 avg_loss = 5.56721\n",
      "epoch no.19 train no.160  loss = 5.66179 avg_loss = 5.57586\n",
      "epoch no.19 train no.170  loss = 3.56723 avg_loss = 5.55097\n",
      "epoch no.19 train no.180  loss = 3.61590 avg_loss = 5.53816\n",
      "epoch no.19 train no.190  loss = 5.59813 avg_loss = 5.55583\n",
      "epoch no.19 train no.200  loss = 5.59302 avg_loss = 5.55346\n",
      "epoch no.19 train no.210  loss = 5.87268 avg_loss = 5.58953\n",
      "epoch no.19 train no.220  loss = 6.08652 avg_loss = 5.62430\n",
      "epoch no.19 train no.230  loss = 5.19944 avg_loss = 5.61777\n",
      "epoch no.19 train no.240  loss = 5.38568 avg_loss = 5.61832\n",
      "epoch no.19 train no.250  loss = 5.14549 avg_loss = 5.60231\n",
      "epoch no.19 train no.260  loss = 7.72827 avg_loss = 5.63279\n",
      "epoch no.19 train no.270  loss = 5.94545 avg_loss = 5.62314\n",
      "epoch no.19 train no.280  loss = 7.23170 avg_loss = 5.60778\n",
      "epoch no.19 train no.290  loss = 5.24538 avg_loss = 5.65837\n",
      "epoch no.19 train no.300  loss = 6.36170 avg_loss = 5.62935\n",
      "epoch no.19 train no.310  loss = 7.28448 avg_loss = 5.68955\n",
      "epoch no.19 train no.320  loss = 6.02880 avg_loss = 5.65237\n",
      "epoch no.19 train no.330  loss = 5.22044 avg_loss = 5.65548\n",
      "epoch no.19 train no.340  loss = 7.59226 avg_loss = 5.71830\n",
      "epoch no.19 train no.350  loss = 6.24263 avg_loss = 5.74029\n",
      "epoch no.19 train no.360  loss = 4.13416 avg_loss = 5.71917\n",
      "epoch no.19 train no.370  loss = 6.00800 avg_loss = 5.71869\n",
      "epoch no.19 train no.380  loss = 6.33329 avg_loss = 5.68138\n",
      "epoch no.19 train no.390  loss = 5.30140 avg_loss = 5.68253\n",
      "epoch no.19 train no.400  loss = 5.84306 avg_loss = 5.64126\n",
      "epoch no.19 train no.410  loss = 5.26493 avg_loss = 5.64603\n",
      "epoch no.19 train no.420  loss = 7.07848 avg_loss = 5.68090\n",
      "epoch no.19 train no.430  loss = 6.05953 avg_loss = 5.63076\n",
      "epoch no.19 train no.440  loss = 5.96440 avg_loss = 5.63968\n",
      "epoch no.19 train no.450  loss = 5.84475 avg_loss = 5.64652\n",
      "epoch no.19 train no.460  loss = 5.43525 avg_loss = 5.64048\n",
      "epoch no.19 train no.470  loss = 5.64277 avg_loss = 5.60837\n",
      "epoch no.19 train no.480  loss = 4.86791 avg_loss = 5.60367\n",
      "epoch no.19 train no.490  loss = 4.23197 avg_loss = 5.60378\n",
      "epoch no.19 train no.500  loss = 6.57922 avg_loss = 5.62487\n",
      "epoch no.19 train no.510  loss = 5.03281 avg_loss = 5.63046\n",
      "epoch no.19 train no.520  loss = 6.05142 avg_loss = 5.62146\n",
      "epoch no.19 train no.530  loss = 6.48768 avg_loss = 5.65861\n",
      "epoch no.19 train no.540  loss = 5.92382 avg_loss = 5.63934\n",
      "epoch no.19 train no.550  loss = 5.75500 avg_loss = 5.59723\n",
      "epoch no.19 train no.560  loss = 5.89817 avg_loss = 5.60196\n",
      "epoch no.19 train no.570  loss = 6.74427 avg_loss = 5.64588\n",
      "epoch no.19 train no.580  loss = 6.00350 avg_loss = 5.69326\n",
      "epoch no.19 train no.590  loss = 6.23319 avg_loss = 5.66411\n",
      "epoch no.19 train no.600  loss = 6.05853 avg_loss = 5.65899\n",
      "epoch no.19 train no.610  loss = 4.72632 avg_loss = 5.68349\n",
      "epoch no.19 train no.620  loss = 6.56906 avg_loss = 5.69409\n",
      "epoch no.19 train no.630  loss = 6.18997 avg_loss = 5.66751\n",
      "epoch no.19 train no.640  loss = 5.70212 avg_loss = 5.67639\n",
      "epoch no.19 train no.650  loss = 6.34501 avg_loss = 5.67659\n",
      "epoch no.19 train no.660  loss = 6.70278 avg_loss = 5.68828\n",
      "epoch no.19 train no.670  loss = 6.57464 avg_loss = 5.68621\n",
      "epoch no.19 train no.680  loss = 6.02869 avg_loss = 5.73110\n",
      "epoch no.19 train no.690  loss = 6.24276 avg_loss = 5.76291\n",
      "epoch no.19 train no.700  loss = 6.99038 avg_loss = 5.76766\n",
      "epoch no.19 train no.710  loss = 5.76141 avg_loss = 5.78115\n",
      "epoch no.19 train no.720  loss = 4.64296 avg_loss = 5.73652\n",
      "epoch no.19 train no.730  loss = 5.81224 avg_loss = 5.73119\n",
      "epoch no.19 train no.740  loss = 4.64136 avg_loss = 5.72898\n",
      "epoch no.19 train no.750  loss = 6.47276 avg_loss = 5.70987\n",
      "epoch no.19 train no.760  loss = 4.66160 avg_loss = 5.69546\n",
      "epoch no.19 train no.770  loss = 6.99501 avg_loss = 5.71958\n",
      "epoch no.19 train no.780  loss = 5.72135 avg_loss = 5.70883\n",
      "epoch no.19 train no.790  loss = 3.52060 avg_loss = 5.62512\n",
      "epoch no.19 train no.800  loss = 6.08439 avg_loss = 5.59249\n",
      "epoch no.19 train no.810  loss = 7.49666 avg_loss = 5.60455\n",
      "epoch no.19 train no.820  loss = 7.37642 avg_loss = 5.58403\n",
      "epoch no.19 train no.830  loss = 6.36065 avg_loss = 5.60482\n",
      "epoch no.19 train no.840  loss = 5.42228 avg_loss = 5.59691\n",
      "epoch no.19 train no.850  loss = 4.92114 avg_loss = 5.60829\n",
      "epoch no.19 train no.860  loss = 6.16614 avg_loss = 5.60280\n",
      "epoch no.19 train no.870  loss = 6.20455 avg_loss = 5.57660\n",
      "epoch no.19 train no.880  loss = 4.44658 avg_loss = 5.57477\n",
      "epoch no.19 train no.890  loss = 5.35579 avg_loss = 5.60022\n",
      "epoch no.19 train no.900  loss = 5.98366 avg_loss = 5.59732\n",
      "epoch no.19 train no.910  loss = 6.73929 avg_loss = 5.62904\n",
      "epoch no.19 train no.920  loss = 5.91968 avg_loss = 5.62303\n",
      "epoch no.19 train no.930  loss = 4.46465 avg_loss = 5.59977\n",
      "epoch no.19 train no.940  loss = 4.89250 avg_loss = 5.61658\n",
      "epoch no.19 train no.950  loss = 5.82756 avg_loss = 5.63516\n",
      "epoch no.19 train no.960  loss = 5.47091 avg_loss = 5.62707\n",
      "epoch no.19 train no.970  loss = 7.39888 avg_loss = 5.64110\n",
      "epoch no.19 train no.980  loss = 5.71460 avg_loss = 5.64561\n",
      "epoch no.19 train no.990  loss = 5.70840 avg_loss = 5.59134\n",
      "epoch no.19 train no.1000  loss = 3.84178 avg_loss = 5.56391\n",
      "to_tokens: ['</s>', '</s>', '</s>']\n",
      "뭐함</s>\n",
      "epoch no.19 train no.1010  loss = 5.59028 avg_loss = 5.51481\n",
      "epoch no.19 train no.1020  loss = 5.64917 avg_loss = 5.56309\n",
      "epoch no.19 train no.1030  loss = 4.69419 avg_loss = 5.51030\n",
      "epoch no.19 train no.1040  loss = 5.29424 avg_loss = 5.53037\n",
      "epoch no.19 train no.1050  loss = 6.69188 avg_loss = 5.56724\n",
      "epoch no.19 train no.1060  loss = 5.78480 avg_loss = 5.55182\n",
      "epoch no.19 train no.1070  loss = 5.72211 avg_loss = 5.55538\n",
      "epoch no.19 train no.1080  loss = 4.95672 avg_loss = 5.55269\n",
      "epoch no.19 train no.1090  loss = 6.69362 avg_loss = 5.59971\n",
      "epoch no.19 train no.1100  loss = 6.42795 avg_loss = 5.63428\n",
      "epoch no.19 train no.1110  loss = 4.19897 avg_loss = 5.62450\n",
      "epoch no.19 train no.1120  loss = 5.31103 avg_loss = 5.60509\n",
      "epoch no.19 train no.1130  loss = 5.39799 avg_loss = 5.60201\n",
      "epoch no.19 train no.1140  loss = 4.24262 avg_loss = 5.59968\n",
      "epoch no.19 train no.1150  loss = 5.45518 avg_loss = 5.61842\n",
      "epoch no.19 train no.1160  loss = 6.69626 avg_loss = 5.62294\n",
      "epoch no.19 train no.1170  loss = 6.13893 avg_loss = 5.63297\n",
      "epoch no.19 train no.1180  loss = 5.70049 avg_loss = 5.62486\n",
      "epoch no.19 train no.1190  loss = 6.79993 avg_loss = 5.62879\n",
      "epoch no.19 train no.1200  loss = 6.18680 avg_loss = 5.65217\n",
      "epoch no.19 train no.1210  loss = 5.21363 avg_loss = 5.69213\n",
      "epoch no.19 train no.1220  loss = 5.55801 avg_loss = 5.65389\n",
      "epoch no.19 train no.1230  loss = 4.74724 avg_loss = 5.69192\n",
      "epoch no.19 train no.1240  loss = 5.79397 avg_loss = 5.69799\n",
      "epoch no.19 train no.1250  loss = 5.32398 avg_loss = 5.63649\n",
      "epoch no.19 train no.1260  loss = 7.02322 avg_loss = 5.68781\n",
      "epoch no.19 train no.1270  loss = 5.69708 avg_loss = 5.66553\n",
      "epoch no.19 train no.1280  loss = 5.54454 avg_loss = 5.65902\n",
      "epoch no.19 train no.1290  loss = 4.70054 avg_loss = 5.65358\n",
      "epoch no.19 train no.1300  loss = 6.86869 avg_loss = 5.67628\n",
      "epoch no.19 train no.1310  loss = 5.45684 avg_loss = 5.67069\n",
      "epoch no.19 train no.1320  loss = 5.70164 avg_loss = 5.67273\n",
      "epoch no.19 train no.1330  loss = 3.46704 avg_loss = 5.63465\n",
      "epoch no.19 train no.1340  loss = 4.42043 avg_loss = 5.63841\n",
      "epoch no.19 train no.1350  loss = 4.17095 avg_loss = 5.62492\n",
      "epoch no.19 train no.1360  loss = 6.50858 avg_loss = 5.63159\n",
      "epoch no.19 train no.1370  loss = 6.59062 avg_loss = 5.62069\n",
      "epoch no.19 train no.1380  loss = 5.01737 avg_loss = 5.61324\n",
      "epoch no.19 train no.1390  loss = 4.57072 avg_loss = 5.60865\n",
      "epoch no.19 train no.1400  loss = 6.16117 avg_loss = 5.64888\n",
      "epoch no.19 train no.1410  loss = 5.55926 avg_loss = 5.65636\n",
      "epoch no.19 train no.1420  loss = 7.63709 avg_loss = 5.68762\n",
      "epoch no.19 train no.1430  loss = 5.93591 avg_loss = 5.71916\n",
      "epoch no.19 train no.1440  loss = 7.04511 avg_loss = 5.72558\n",
      "epoch no.19 train no.1450  loss = 5.44877 avg_loss = 5.68077\n",
      "epoch no.19 train no.1460  loss = 4.55526 avg_loss = 5.68133\n",
      "epoch no.19 train no.1470  loss = 5.77079 avg_loss = 5.70373\n",
      "epoch no.19 train no.1480  loss = 4.51233 avg_loss = 5.66355\n",
      "epoch no.19 train no.1490  loss = 5.80183 avg_loss = 5.67577\n",
      "epoch no.19 train no.1500  loss = 3.57489 avg_loss = 5.64382\n",
      "epoch no.19 train no.1510  loss = 3.48921 avg_loss = 5.61961\n",
      "epoch no.19 train no.1520  loss = 6.64475 avg_loss = 5.59430\n",
      "epoch no.19 train no.1530  loss = 5.92028 avg_loss = 5.62143\n",
      "epoch no.19 train no.1540  loss = 5.07597 avg_loss = 5.62243\n",
      "epoch no.19 train no.1550  loss = 5.67684 avg_loss = 5.62955\n",
      "epoch no.19 train no.1560  loss = 5.72774 avg_loss = 5.65456\n",
      "epoch no.19 train no.1570  loss = 6.27462 avg_loss = 5.66067\n",
      "epoch no.19 train no.1580  loss = 6.44790 avg_loss = 5.67147\n",
      "epoch no.19 train no.1590  loss = 7.30936 avg_loss = 5.71476\n",
      "epoch no.19 train no.1600  loss = 5.51009 avg_loss = 5.72093\n",
      "epoch no.19 train no.1610  loss = 5.99427 avg_loss = 5.72526\n",
      "epoch no.19 train no.1620  loss = 5.57374 avg_loss = 5.73040\n",
      "epoch no.19 train no.1630  loss = 5.41870 avg_loss = 5.70978\n",
      "epoch no.19 train no.1640  loss = 4.09356 avg_loss = 5.68509\n",
      "epoch no.19 train no.1650  loss = 6.39752 avg_loss = 5.68519\n",
      "epoch no.19 train no.1660  loss = 4.71791 avg_loss = 5.68955\n",
      "epoch no.19 train no.1670  loss = 6.28590 avg_loss = 5.67987\n",
      "epoch no.19 train no.1680  loss = 5.47630 avg_loss = 5.73300\n",
      "epoch no.19 train no.1690  loss = 6.05377 avg_loss = 5.71769\n",
      "epoch no.19 train no.1700  loss = 5.14100 avg_loss = 5.68833\n",
      "epoch no.19 train no.1710  loss = 5.49635 avg_loss = 5.67007\n",
      "epoch no.19 train no.1720  loss = 6.05023 avg_loss = 5.72719\n",
      "epoch no.19 train no.1730  loss = 4.41496 avg_loss = 5.70730\n",
      "epoch no.19 train no.1740  loss = 4.62343 avg_loss = 5.72603\n",
      "epoch no.19 train no.1750  loss = 5.29919 avg_loss = 5.69172\n",
      "epoch no.20 train no.0  loss = 5.08030 avg_loss = 5.68578\n",
      "epoch no.20 train no.10  loss = 6.56045 avg_loss = 5.65511\n",
      "epoch no.20 train no.20  loss = 5.48432 avg_loss = 5.65510\n",
      "epoch no.20 train no.30  loss = 5.02671 avg_loss = 5.70043\n",
      "epoch no.20 train no.40  loss = 4.62406 avg_loss = 5.67323\n",
      "epoch no.20 train no.50  loss = 4.75722 avg_loss = 5.64193\n",
      "epoch no.20 train no.60  loss = 6.85834 avg_loss = 5.68551\n",
      "epoch no.20 train no.70  loss = 3.81204 avg_loss = 5.71318\n",
      "epoch no.20 train no.80  loss = 5.59783 avg_loss = 5.75943\n",
      "epoch no.20 train no.90  loss = 6.26066 avg_loss = 5.71289\n",
      "epoch no.20 train no.100  loss = 3.80232 avg_loss = 5.74016\n",
      "epoch no.20 train no.110  loss = 6.00462 avg_loss = 5.71222\n",
      "epoch no.20 train no.120  loss = 5.67991 avg_loss = 5.70596\n",
      "epoch no.20 train no.130  loss = 5.57020 avg_loss = 5.71400\n",
      "epoch no.20 train no.140  loss = 6.35330 avg_loss = 5.74131\n",
      "epoch no.20 train no.150  loss = 5.46916 avg_loss = 5.72843\n",
      "epoch no.20 train no.160  loss = 5.94054 avg_loss = 5.69595\n",
      "epoch no.20 train no.170  loss = 3.55209 avg_loss = 5.67274\n",
      "epoch no.20 train no.180  loss = 4.86783 avg_loss = 5.67117\n",
      "epoch no.20 train no.190  loss = 3.44821 avg_loss = 5.62960\n",
      "epoch no.20 train no.200  loss = 6.95028 avg_loss = 5.65426\n",
      "epoch no.20 train no.210  loss = 5.38768 avg_loss = 5.61078\n",
      "epoch no.20 train no.220  loss = 6.13403 avg_loss = 5.55618\n",
      "epoch no.20 train no.230  loss = 6.61062 avg_loss = 5.54380\n",
      "epoch no.20 train no.240  loss = 4.62281 avg_loss = 5.59729\n",
      "epoch no.20 train no.250  loss = 6.12363 avg_loss = 5.61694\n",
      "epoch no.20 train no.260  loss = 4.47479 avg_loss = 5.66735\n",
      "epoch no.20 train no.270  loss = 6.17012 avg_loss = 5.69325\n",
      "epoch no.20 train no.280  loss = 6.02263 avg_loss = 5.69292\n",
      "epoch no.20 train no.290  loss = 5.90916 avg_loss = 5.69591\n",
      "epoch no.20 train no.300  loss = 7.04014 avg_loss = 5.67350\n",
      "epoch no.20 train no.310  loss = 5.78418 avg_loss = 5.67632\n",
      "epoch no.20 train no.320  loss = 5.97824 avg_loss = 5.62844\n",
      "epoch no.20 train no.330  loss = 6.72646 avg_loss = 5.64843\n",
      "epoch no.20 train no.340  loss = 5.19402 avg_loss = 5.63226\n",
      "epoch no.20 train no.350  loss = 4.59777 avg_loss = 5.60644\n",
      "epoch no.20 train no.360  loss = 3.89652 avg_loss = 5.54525\n",
      "epoch no.20 train no.370  loss = 5.38286 avg_loss = 5.52292\n",
      "epoch no.20 train no.380  loss = 6.67115 avg_loss = 5.53245\n",
      "epoch no.20 train no.390  loss = 3.75894 avg_loss = 5.51994\n",
      "epoch no.20 train no.400  loss = 5.75192 avg_loss = 5.55954\n",
      "epoch no.20 train no.410  loss = 6.45586 avg_loss = 5.59996\n",
      "epoch no.20 train no.420  loss = 6.05851 avg_loss = 5.59440\n",
      "epoch no.20 train no.430  loss = 4.89990 avg_loss = 5.61153\n",
      "epoch no.20 train no.440  loss = 5.76424 avg_loss = 5.59900\n",
      "epoch no.20 train no.450  loss = 6.02775 avg_loss = 5.62755\n",
      "epoch no.20 train no.460  loss = 6.83733 avg_loss = 5.65243\n",
      "epoch no.20 train no.470  loss = 4.25783 avg_loss = 5.62876\n",
      "epoch no.20 train no.480  loss = 6.15491 avg_loss = 5.65995\n",
      "epoch no.20 train no.490  loss = 4.90803 avg_loss = 5.64270\n",
      "epoch no.20 train no.500  loss = 5.88195 avg_loss = 5.65317\n",
      "epoch no.20 train no.510  loss = 6.09228 avg_loss = 5.67862\n",
      "epoch no.20 train no.520  loss = 6.34552 avg_loss = 5.65864\n",
      "epoch no.20 train no.530  loss = 5.92027 avg_loss = 5.65929\n",
      "epoch no.20 train no.540  loss = 3.36779 avg_loss = 5.64092\n",
      "epoch no.20 train no.550  loss = 4.96120 avg_loss = 5.67999\n",
      "epoch no.20 train no.560  loss = 5.14304 avg_loss = 5.67123\n",
      "epoch no.20 train no.570  loss = 5.38538 avg_loss = 5.62438\n",
      "epoch no.20 train no.580  loss = 5.39647 avg_loss = 5.58583\n",
      "epoch no.20 train no.590  loss = 5.16254 avg_loss = 5.54817\n",
      "epoch no.20 train no.600  loss = 6.93905 avg_loss = 5.61038\n",
      "epoch no.20 train no.610  loss = 6.35761 avg_loss = 5.59226\n",
      "epoch no.20 train no.620  loss = 3.70875 avg_loss = 5.62213\n",
      "epoch no.20 train no.630  loss = 5.05296 avg_loss = 5.63540\n",
      "epoch no.20 train no.640  loss = 5.53604 avg_loss = 5.67873\n",
      "epoch no.20 train no.650  loss = 3.95431 avg_loss = 5.69925\n",
      "epoch no.20 train no.660  loss = 5.32837 avg_loss = 5.68228\n",
      "epoch no.20 train no.670  loss = 6.44371 avg_loss = 5.70588\n",
      "epoch no.20 train no.680  loss = 4.96758 avg_loss = 5.67829\n",
      "epoch no.20 train no.690  loss = 6.66577 avg_loss = 5.65855\n",
      "epoch no.20 train no.700  loss = 5.38005 avg_loss = 5.66559\n",
      "epoch no.20 train no.710  loss = 6.81754 avg_loss = 5.66994\n",
      "epoch no.20 train no.720  loss = 5.93122 avg_loss = 5.67119\n",
      "epoch no.20 train no.730  loss = 5.42908 avg_loss = 5.69507\n",
      "epoch no.20 train no.740  loss = 6.29926 avg_loss = 5.72789\n",
      "epoch no.20 train no.750  loss = 4.42821 avg_loss = 5.70585\n",
      "epoch no.20 train no.760  loss = 5.33120 avg_loss = 5.70600\n",
      "epoch no.20 train no.770  loss = 4.80594 avg_loss = 5.72111\n",
      "epoch no.20 train no.780  loss = 5.15994 avg_loss = 5.70807\n",
      "epoch no.20 train no.790  loss = 5.53433 avg_loss = 5.69079\n",
      "epoch no.20 train no.800  loss = 4.44064 avg_loss = 5.66582\n",
      "epoch no.20 train no.810  loss = 5.33346 avg_loss = 5.66630\n",
      "epoch no.20 train no.820  loss = 4.26444 avg_loss = 5.63714\n",
      "epoch no.20 train no.830  loss = 4.74469 avg_loss = 5.65322\n",
      "epoch no.20 train no.840  loss = 5.55231 avg_loss = 5.65913\n",
      "epoch no.20 train no.850  loss = 6.30179 avg_loss = 5.67036\n",
      "epoch no.20 train no.860  loss = 5.36512 avg_loss = 5.63026\n",
      "epoch no.20 train no.870  loss = 5.41929 avg_loss = 5.62681\n",
      "epoch no.20 train no.880  loss = 6.47526 avg_loss = 5.58854\n",
      "epoch no.20 train no.890  loss = 6.36935 avg_loss = 5.60699\n",
      "epoch no.20 train no.900  loss = 5.44688 avg_loss = 5.60855\n",
      "epoch no.20 train no.910  loss = 5.18038 avg_loss = 5.58492\n",
      "epoch no.20 train no.920  loss = 7.34786 avg_loss = 5.62269\n",
      "epoch no.20 train no.930  loss = 7.50362 avg_loss = 5.64963\n",
      "epoch no.20 train no.940  loss = 4.57191 avg_loss = 5.64878\n",
      "epoch no.20 train no.950  loss = 6.45648 avg_loss = 5.65651\n",
      "epoch no.20 train no.960  loss = 6.62218 avg_loss = 5.70290\n",
      "epoch no.20 train no.970  loss = 6.03979 avg_loss = 5.73189\n",
      "epoch no.20 train no.980  loss = 5.40927 avg_loss = 5.72950\n",
      "epoch no.20 train no.990  loss = 6.25199 avg_loss = 5.73205\n",
      "epoch no.20 train no.1000  loss = 5.92887 avg_loss = 5.70131\n",
      "to_tokens: ['</s>', '</s>', '</s>']\n",
      "뭐함</s>\n",
      "epoch no.20 train no.1010  loss = 5.40873 avg_loss = 5.70470\n",
      "epoch no.20 train no.1020  loss = 6.03604 avg_loss = 5.69679\n",
      "epoch no.20 train no.1030  loss = 6.67809 avg_loss = 5.70267\n",
      "epoch no.20 train no.1040  loss = 4.74904 avg_loss = 5.66159\n",
      "epoch no.20 train no.1050  loss = 7.12637 avg_loss = 5.69330\n",
      "epoch no.20 train no.1060  loss = 4.18380 avg_loss = 5.66989\n",
      "epoch no.20 train no.1070  loss = 6.79297 avg_loss = 5.69720\n",
      "epoch no.20 train no.1080  loss = 5.44239 avg_loss = 5.70306\n",
      "epoch no.20 train no.1090  loss = 5.31989 avg_loss = 5.66117\n",
      "epoch no.20 train no.1100  loss = 5.57231 avg_loss = 5.63151\n",
      "epoch no.20 train no.1110  loss = 6.62599 avg_loss = 5.68682\n",
      "epoch no.20 train no.1120  loss = 4.91373 avg_loss = 5.63187\n",
      "epoch no.20 train no.1130  loss = 4.55186 avg_loss = 5.62176\n",
      "epoch no.20 train no.1140  loss = 6.52272 avg_loss = 5.63450\n",
      "epoch no.20 train no.1150  loss = 5.28248 avg_loss = 5.60989\n",
      "epoch no.20 train no.1160  loss = 4.58712 avg_loss = 5.61466\n",
      "epoch no.20 train no.1170  loss = 5.85636 avg_loss = 5.63695\n",
      "epoch no.20 train no.1180  loss = 4.01486 avg_loss = 5.60712\n",
      "epoch no.20 train no.1190  loss = 6.25093 avg_loss = 5.60908\n",
      "epoch no.20 train no.1200  loss = 5.19179 avg_loss = 5.58539\n",
      "epoch no.20 train no.1210  loss = 6.02483 avg_loss = 5.59802\n",
      "epoch no.20 train no.1220  loss = 5.16276 avg_loss = 5.57257\n",
      "epoch no.20 train no.1230  loss = 4.56616 avg_loss = 5.58224\n",
      "epoch no.20 train no.1240  loss = 6.99571 avg_loss = 5.58119\n",
      "epoch no.20 train no.1250  loss = 5.71324 avg_loss = 5.60426\n",
      "epoch no.20 train no.1260  loss = 6.14361 avg_loss = 5.60706\n",
      "epoch no.20 train no.1270  loss = 4.94665 avg_loss = 5.60812\n",
      "epoch no.20 train no.1280  loss = 5.03022 avg_loss = 5.66650\n",
      "epoch no.20 train no.1290  loss = 6.41237 avg_loss = 5.67902\n",
      "epoch no.20 train no.1300  loss = 4.76295 avg_loss = 5.65841\n",
      "epoch no.20 train no.1310  loss = 3.49731 avg_loss = 5.65153\n",
      "epoch no.20 train no.1320  loss = 5.67427 avg_loss = 5.63981\n",
      "epoch no.20 train no.1330  loss = 5.53887 avg_loss = 5.62311\n",
      "epoch no.20 train no.1340  loss = 4.38960 avg_loss = 5.57136\n",
      "epoch no.20 train no.1350  loss = 6.92126 avg_loss = 5.59545\n",
      "epoch no.20 train no.1360  loss = 4.38944 avg_loss = 5.58545\n",
      "epoch no.20 train no.1370  loss = 4.79146 avg_loss = 5.59643\n",
      "epoch no.20 train no.1380  loss = 6.15002 avg_loss = 5.53889\n",
      "epoch no.20 train no.1390  loss = 5.01020 avg_loss = 5.53686\n",
      "epoch no.20 train no.1400  loss = 4.34085 avg_loss = 5.52340\n",
      "epoch no.20 train no.1410  loss = 5.08608 avg_loss = 5.58939\n",
      "epoch no.20 train no.1420  loss = 6.28703 avg_loss = 5.60689\n",
      "epoch no.20 train no.1430  loss = 5.22974 avg_loss = 5.60796\n",
      "epoch no.20 train no.1440  loss = 6.09133 avg_loss = 5.62038\n",
      "epoch no.20 train no.1450  loss = 5.40074 avg_loss = 5.64073\n",
      "epoch no.20 train no.1460  loss = 5.32865 avg_loss = 5.59602\n",
      "epoch no.20 train no.1470  loss = 5.74938 avg_loss = 5.63315\n",
      "epoch no.20 train no.1480  loss = 5.75981 avg_loss = 5.63465\n",
      "epoch no.20 train no.1490  loss = 6.32847 avg_loss = 5.60583\n",
      "epoch no.20 train no.1500  loss = 4.53365 avg_loss = 5.58340\n",
      "epoch no.20 train no.1510  loss = 5.36008 avg_loss = 5.58933\n",
      "epoch no.20 train no.1520  loss = 4.65878 avg_loss = 5.54934\n",
      "epoch no.20 train no.1530  loss = 6.59566 avg_loss = 5.61319\n",
      "epoch no.20 train no.1540  loss = 5.84998 avg_loss = 5.58018\n",
      "epoch no.20 train no.1550  loss = 6.87760 avg_loss = 5.56944\n",
      "epoch no.20 train no.1560  loss = 5.12002 avg_loss = 5.59866\n",
      "epoch no.20 train no.1570  loss = 5.41080 avg_loss = 5.57848\n",
      "epoch no.20 train no.1580  loss = 5.59896 avg_loss = 5.58916\n",
      "epoch no.20 train no.1590  loss = 4.53001 avg_loss = 5.55817\n",
      "epoch no.20 train no.1600  loss = 6.12174 avg_loss = 5.55338\n",
      "epoch no.20 train no.1610  loss = 6.88719 avg_loss = 5.55542\n",
      "epoch no.20 train no.1620  loss = 5.28161 avg_loss = 5.55902\n",
      "epoch no.20 train no.1630  loss = 5.74206 avg_loss = 5.59142\n",
      "epoch no.20 train no.1640  loss = 5.68257 avg_loss = 5.61136\n",
      "epoch no.20 train no.1650  loss = 7.27642 avg_loss = 5.61971\n",
      "epoch no.20 train no.1660  loss = 4.84531 avg_loss = 5.59626\n",
      "epoch no.20 train no.1670  loss = 6.85387 avg_loss = 5.59120\n",
      "epoch no.20 train no.1680  loss = 4.84754 avg_loss = 5.60040\n",
      "epoch no.20 train no.1690  loss = 5.86735 avg_loss = 5.67146\n",
      "epoch no.20 train no.1700  loss = 5.68461 avg_loss = 5.67710\n",
      "epoch no.20 train no.1710  loss = 6.06420 avg_loss = 5.67244\n",
      "epoch no.20 train no.1720  loss = 5.77007 avg_loss = 5.67392\n",
      "epoch no.20 train no.1730  loss = 5.12941 avg_loss = 5.65224\n",
      "epoch no.20 train no.1740  loss = 5.18798 avg_loss = 5.64119\n",
      "epoch no.20 train no.1750  loss = 7.24600 avg_loss = 5.71778\n",
      "epoch no.21 train no.0  loss = 6.56620 avg_loss = 5.73767\n",
      "epoch no.21 train no.10  loss = 5.37807 avg_loss = 5.65090\n",
      "epoch no.21 train no.20  loss = 4.59520 avg_loss = 5.61234\n",
      "epoch no.21 train no.30  loss = 6.76386 avg_loss = 5.63235\n",
      "epoch no.21 train no.40  loss = 4.30034 avg_loss = 5.61754\n",
      "epoch no.21 train no.50  loss = 5.46231 avg_loss = 5.61493\n",
      "epoch no.21 train no.60  loss = 6.80064 avg_loss = 5.62574\n",
      "epoch no.21 train no.70  loss = 4.77377 avg_loss = 5.59486\n",
      "epoch no.21 train no.80  loss = 5.69309 avg_loss = 5.62912\n",
      "epoch no.21 train no.90  loss = 7.46626 avg_loss = 5.63237\n",
      "epoch no.21 train no.100  loss = 5.58403 avg_loss = 5.64405\n",
      "epoch no.21 train no.110  loss = 4.76313 avg_loss = 5.63388\n",
      "epoch no.21 train no.120  loss = 5.29790 avg_loss = 5.63988\n",
      "epoch no.21 train no.130  loss = 6.54579 avg_loss = 5.62995\n",
      "epoch no.21 train no.140  loss = 5.10614 avg_loss = 5.67645\n",
      "epoch no.21 train no.150  loss = 5.17820 avg_loss = 5.70071\n",
      "epoch no.21 train no.160  loss = 6.42389 avg_loss = 5.71011\n",
      "epoch no.21 train no.170  loss = 7.61652 avg_loss = 5.75265\n",
      "epoch no.21 train no.180  loss = 4.89820 avg_loss = 5.78060\n",
      "epoch no.21 train no.190  loss = 4.28370 avg_loss = 5.78507\n",
      "epoch no.21 train no.200  loss = 6.08096 avg_loss = 5.76920\n",
      "epoch no.21 train no.210  loss = 5.29251 avg_loss = 5.72464\n",
      "epoch no.21 train no.220  loss = 6.72201 avg_loss = 5.69373\n",
      "epoch no.21 train no.230  loss = 5.01220 avg_loss = 5.70749\n",
      "epoch no.21 train no.240  loss = 5.47024 avg_loss = 5.67111\n",
      "epoch no.21 train no.250  loss = 6.02684 avg_loss = 5.64955\n",
      "epoch no.21 train no.260  loss = 6.32477 avg_loss = 5.63878\n",
      "epoch no.21 train no.270  loss = 7.24360 avg_loss = 5.68496\n",
      "epoch no.21 train no.280  loss = 6.04412 avg_loss = 5.70538\n",
      "epoch no.21 train no.290  loss = 6.32054 avg_loss = 5.71205\n",
      "epoch no.21 train no.300  loss = 4.95895 avg_loss = 5.71149\n",
      "epoch no.21 train no.310  loss = 6.64119 avg_loss = 5.68047\n",
      "epoch no.21 train no.320  loss = 4.81712 avg_loss = 5.68085\n",
      "epoch no.21 train no.330  loss = 5.47562 avg_loss = 5.65884\n",
      "epoch no.21 train no.340  loss = 5.82050 avg_loss = 5.64089\n",
      "epoch no.21 train no.350  loss = 4.68099 avg_loss = 5.66352\n",
      "epoch no.21 train no.360  loss = 6.41968 avg_loss = 5.69720\n",
      "epoch no.21 train no.370  loss = 6.79942 avg_loss = 5.67309\n",
      "epoch no.21 train no.380  loss = 7.01478 avg_loss = 5.69646\n",
      "epoch no.21 train no.390  loss = 6.28608 avg_loss = 5.69353\n",
      "epoch no.21 train no.400  loss = 5.91335 avg_loss = 5.68048\n",
      "epoch no.21 train no.410  loss = 5.05520 avg_loss = 5.66842\n",
      "epoch no.21 train no.420  loss = 5.28108 avg_loss = 5.72278\n",
      "epoch no.21 train no.430  loss = 5.17107 avg_loss = 5.75701\n",
      "epoch no.21 train no.440  loss = 6.13415 avg_loss = 5.72430\n",
      "epoch no.21 train no.450  loss = 5.81484 avg_loss = 5.68867\n",
      "epoch no.21 train no.460  loss = 5.27806 avg_loss = 5.66274\n",
      "epoch no.21 train no.470  loss = 4.18862 avg_loss = 5.65803\n",
      "epoch no.21 train no.480  loss = 5.68640 avg_loss = 5.67266\n",
      "epoch no.21 train no.490  loss = 6.28998 avg_loss = 5.68663\n",
      "epoch no.21 train no.500  loss = 5.25916 avg_loss = 5.64151\n",
      "epoch no.21 train no.510  loss = 5.99723 avg_loss = 5.65388\n",
      "epoch no.21 train no.520  loss = 4.58488 avg_loss = 5.62808\n",
      "epoch no.21 train no.530  loss = 4.66407 avg_loss = 5.63326\n",
      "epoch no.21 train no.540  loss = 5.55625 avg_loss = 5.65434\n",
      "epoch no.21 train no.550  loss = 5.86069 avg_loss = 5.61510\n",
      "epoch no.21 train no.560  loss = 5.83692 avg_loss = 5.65083\n",
      "epoch no.21 train no.570  loss = 5.00491 avg_loss = 5.63514\n",
      "epoch no.21 train no.580  loss = 5.23635 avg_loss = 5.65395\n",
      "epoch no.21 train no.590  loss = 4.68772 avg_loss = 5.68213\n",
      "epoch no.21 train no.600  loss = 6.30985 avg_loss = 5.69029\n",
      "epoch no.21 train no.610  loss = 5.94371 avg_loss = 5.68600\n",
      "epoch no.21 train no.620  loss = 5.19618 avg_loss = 5.69196\n",
      "epoch no.21 train no.630  loss = 6.35837 avg_loss = 5.71086\n",
      "epoch no.21 train no.640  loss = 5.36645 avg_loss = 5.68917\n",
      "epoch no.21 train no.650  loss = 6.61676 avg_loss = 5.68629\n",
      "epoch no.21 train no.660  loss = 6.05518 avg_loss = 5.70076\n",
      "epoch no.21 train no.670  loss = 7.19780 avg_loss = 5.67769\n",
      "epoch no.21 train no.680  loss = 6.62920 avg_loss = 5.65879\n",
      "epoch no.21 train no.690  loss = 4.41367 avg_loss = 5.66925\n",
      "epoch no.21 train no.700  loss = 4.86170 avg_loss = 5.68367\n",
      "epoch no.21 train no.710  loss = 5.99730 avg_loss = 5.70518\n",
      "epoch no.21 train no.720  loss = 4.34878 avg_loss = 5.70852\n",
      "epoch no.21 train no.730  loss = 5.21828 avg_loss = 5.68373\n",
      "epoch no.21 train no.740  loss = 5.04015 avg_loss = 5.64937\n",
      "epoch no.21 train no.750  loss = 4.43314 avg_loss = 5.65114\n",
      "epoch no.21 train no.760  loss = 5.47047 avg_loss = 5.57985\n",
      "epoch no.21 train no.770  loss = 5.10969 avg_loss = 5.59597\n",
      "epoch no.21 train no.780  loss = 4.50337 avg_loss = 5.60030\n",
      "epoch no.21 train no.790  loss = 5.00083 avg_loss = 5.64271\n",
      "epoch no.21 train no.800  loss = 6.42089 avg_loss = 5.65605\n",
      "epoch no.21 train no.810  loss = 4.26498 avg_loss = 5.65661\n",
      "epoch no.21 train no.820  loss = 4.62671 avg_loss = 5.65379\n",
      "epoch no.21 train no.830  loss = 5.79685 avg_loss = 5.61405\n",
      "epoch no.21 train no.840  loss = 5.78937 avg_loss = 5.62565\n",
      "epoch no.21 train no.850  loss = 6.27474 avg_loss = 5.64251\n",
      "epoch no.21 train no.860  loss = 4.71425 avg_loss = 5.62411\n",
      "epoch no.21 train no.870  loss = 6.78498 avg_loss = 5.61963\n",
      "epoch no.21 train no.880  loss = 5.92436 avg_loss = 5.65707\n",
      "epoch no.21 train no.890  loss = 4.43239 avg_loss = 5.63025\n",
      "epoch no.21 train no.900  loss = 5.72950 avg_loss = 5.65894\n",
      "epoch no.21 train no.910  loss = 5.74173 avg_loss = 5.66240\n",
      "epoch no.21 train no.920  loss = 6.84230 avg_loss = 5.67814\n",
      "epoch no.21 train no.930  loss = 6.16738 avg_loss = 5.72327\n",
      "epoch no.21 train no.940  loss = 5.57646 avg_loss = 5.71030\n",
      "epoch no.21 train no.950  loss = 5.79917 avg_loss = 5.70128\n",
      "epoch no.21 train no.960  loss = 6.08435 avg_loss = 5.70897\n",
      "epoch no.21 train no.970  loss = 6.10976 avg_loss = 5.67079\n",
      "epoch no.21 train no.980  loss = 3.59781 avg_loss = 5.66053\n",
      "epoch no.21 train no.990  loss = 5.07356 avg_loss = 5.66369\n",
      "epoch no.21 train no.1000  loss = 6.25785 avg_loss = 5.66380\n",
      "to_tokens: ['</s>', '</s>', '</s>']\n",
      "뭐함</s>\n",
      "epoch no.21 train no.1010  loss = 6.73749 avg_loss = 5.67906\n",
      "epoch no.21 train no.1020  loss = 5.19268 avg_loss = 5.69963\n",
      "epoch no.21 train no.1030  loss = 5.91108 avg_loss = 5.68849\n",
      "epoch no.21 train no.1040  loss = 4.09163 avg_loss = 5.68936\n",
      "epoch no.21 train no.1050  loss = 6.88713 avg_loss = 5.69393\n",
      "epoch no.21 train no.1060  loss = 4.43919 avg_loss = 5.67065\n",
      "epoch no.21 train no.1070  loss = 6.66667 avg_loss = 5.68685\n",
      "epoch no.21 train no.1080  loss = 6.74452 avg_loss = 5.64319\n",
      "epoch no.21 train no.1090  loss = 4.72387 avg_loss = 5.66625\n",
      "epoch no.21 train no.1100  loss = 6.04596 avg_loss = 5.66914\n",
      "epoch no.21 train no.1110  loss = 3.87774 avg_loss = 5.64611\n",
      "epoch no.21 train no.1120  loss = 5.06521 avg_loss = 5.62636\n",
      "epoch no.21 train no.1130  loss = 4.22281 avg_loss = 5.65244\n",
      "epoch no.21 train no.1140  loss = 6.56205 avg_loss = 5.69535\n",
      "epoch no.21 train no.1150  loss = 5.75177 avg_loss = 5.66052\n",
      "epoch no.21 train no.1160  loss = 5.40205 avg_loss = 5.65083\n",
      "epoch no.21 train no.1170  loss = 5.62030 avg_loss = 5.61248\n",
      "epoch no.21 train no.1180  loss = 6.57734 avg_loss = 5.66633\n",
      "epoch no.21 train no.1190  loss = 5.92444 avg_loss = 5.63875\n",
      "epoch no.21 train no.1200  loss = 6.90591 avg_loss = 5.69746\n",
      "epoch no.21 train no.1210  loss = 6.42069 avg_loss = 5.73547\n",
      "epoch no.21 train no.1220  loss = 5.35294 avg_loss = 5.72687\n",
      "epoch no.21 train no.1230  loss = 5.76701 avg_loss = 5.72694\n",
      "epoch no.21 train no.1240  loss = 4.93927 avg_loss = 5.70103\n",
      "epoch no.21 train no.1250  loss = 7.20590 avg_loss = 5.75740\n",
      "epoch no.21 train no.1260  loss = 4.85147 avg_loss = 5.77190\n",
      "epoch no.21 train no.1270  loss = 4.31644 avg_loss = 5.75174\n",
      "epoch no.21 train no.1280  loss = 5.29468 avg_loss = 5.68998\n",
      "epoch no.21 train no.1290  loss = 6.77089 avg_loss = 5.68715\n",
      "epoch no.21 train no.1300  loss = 7.58583 avg_loss = 5.72175\n",
      "epoch no.21 train no.1310  loss = 5.37858 avg_loss = 5.70371\n",
      "epoch no.21 train no.1320  loss = 3.93037 avg_loss = 5.67912\n",
      "epoch no.21 train no.1330  loss = 5.65983 avg_loss = 5.64647\n",
      "epoch no.21 train no.1340  loss = 4.42163 avg_loss = 5.64336\n",
      "epoch no.21 train no.1350  loss = 5.05188 avg_loss = 5.67479\n",
      "epoch no.21 train no.1360  loss = 6.82716 avg_loss = 5.66974\n",
      "epoch no.21 train no.1370  loss = 5.02515 avg_loss = 5.69238\n",
      "epoch no.21 train no.1380  loss = 5.67831 avg_loss = 5.67677\n",
      "epoch no.21 train no.1390  loss = 4.51456 avg_loss = 5.65640\n",
      "epoch no.21 train no.1400  loss = 4.01735 avg_loss = 5.62950\n",
      "epoch no.21 train no.1410  loss = 6.09956 avg_loss = 5.63047\n",
      "epoch no.21 train no.1420  loss = 7.31578 avg_loss = 5.64771\n",
      "epoch no.21 train no.1430  loss = 5.84472 avg_loss = 5.70676\n",
      "epoch no.21 train no.1440  loss = 6.10908 avg_loss = 5.68887\n",
      "epoch no.21 train no.1450  loss = 6.68248 avg_loss = 5.71939\n",
      "epoch no.21 train no.1460  loss = 5.70490 avg_loss = 5.70876\n",
      "epoch no.21 train no.1470  loss = 4.37728 avg_loss = 5.67657\n",
      "epoch no.21 train no.1480  loss = 4.72205 avg_loss = 5.66612\n",
      "epoch no.21 train no.1490  loss = 5.26377 avg_loss = 5.64415\n",
      "epoch no.21 train no.1500  loss = 6.37217 avg_loss = 5.67391\n",
      "epoch no.21 train no.1510  loss = 3.57213 avg_loss = 5.68318\n",
      "epoch no.21 train no.1520  loss = 6.35576 avg_loss = 5.66993\n",
      "epoch no.21 train no.1530  loss = 4.93688 avg_loss = 5.69934\n",
      "epoch no.21 train no.1540  loss = 4.21976 avg_loss = 5.65892\n",
      "epoch no.21 train no.1550  loss = 4.80734 avg_loss = 5.63547\n",
      "epoch no.21 train no.1560  loss = 5.00516 avg_loss = 5.63869\n",
      "epoch no.21 train no.1570  loss = 5.10450 avg_loss = 5.62332\n",
      "epoch no.21 train no.1580  loss = 5.90564 avg_loss = 5.61093\n",
      "epoch no.21 train no.1590  loss = 6.30851 avg_loss = 5.57365\n",
      "epoch no.21 train no.1600  loss = 5.05050 avg_loss = 5.55221\n",
      "epoch no.21 train no.1610  loss = 4.28031 avg_loss = 5.56967\n",
      "epoch no.21 train no.1620  loss = 4.81314 avg_loss = 5.55763\n",
      "epoch no.21 train no.1630  loss = 4.62072 avg_loss = 5.60301\n",
      "epoch no.21 train no.1640  loss = 4.79171 avg_loss = 5.64177\n",
      "epoch no.21 train no.1650  loss = 5.53950 avg_loss = 5.59410\n",
      "epoch no.21 train no.1660  loss = 4.88548 avg_loss = 5.59360\n",
      "epoch no.21 train no.1670  loss = 3.30057 avg_loss = 5.54343\n",
      "epoch no.21 train no.1680  loss = 5.30464 avg_loss = 5.54408\n",
      "epoch no.21 train no.1690  loss = 5.48493 avg_loss = 5.55805\n",
      "epoch no.21 train no.1700  loss = 5.57349 avg_loss = 5.58855\n",
      "epoch no.21 train no.1710  loss = 8.00266 avg_loss = 5.57543\n",
      "epoch no.21 train no.1720  loss = 6.75169 avg_loss = 5.59543\n",
      "epoch no.21 train no.1730  loss = 4.89780 avg_loss = 5.58735\n",
      "epoch no.21 train no.1740  loss = 6.69344 avg_loss = 5.60481\n",
      "epoch no.21 train no.1750  loss = 4.03428 avg_loss = 5.60980\n",
      "epoch no.22 train no.0  loss = 6.38932 avg_loss = 5.57946\n",
      "epoch no.22 train no.10  loss = 4.94631 avg_loss = 5.56946\n",
      "epoch no.22 train no.20  loss = 5.07095 avg_loss = 5.57690\n",
      "epoch no.22 train no.30  loss = 6.04225 avg_loss = 5.57740\n",
      "epoch no.22 train no.40  loss = 4.04201 avg_loss = 5.63353\n",
      "epoch no.22 train no.50  loss = 5.15415 avg_loss = 5.61155\n",
      "epoch no.22 train no.60  loss = 5.17147 avg_loss = 5.55368\n",
      "epoch no.22 train no.70  loss = 4.99461 avg_loss = 5.53771\n",
      "epoch no.22 train no.80  loss = 4.70260 avg_loss = 5.52937\n",
      "epoch no.22 train no.90  loss = 5.60867 avg_loss = 5.52113\n",
      "epoch no.22 train no.100  loss = 5.34172 avg_loss = 5.53383\n",
      "epoch no.22 train no.110  loss = 3.16752 avg_loss = 5.54051\n",
      "epoch no.22 train no.120  loss = 5.91530 avg_loss = 5.56618\n",
      "epoch no.22 train no.130  loss = 6.39110 avg_loss = 5.57778\n",
      "epoch no.22 train no.140  loss = 4.38572 avg_loss = 5.58312\n",
      "epoch no.22 train no.150  loss = 6.31708 avg_loss = 5.62727\n",
      "epoch no.22 train no.160  loss = 5.57202 avg_loss = 5.60535\n",
      "epoch no.22 train no.170  loss = 5.92199 avg_loss = 5.62755\n",
      "epoch no.22 train no.180  loss = 5.65106 avg_loss = 5.63051\n",
      "epoch no.22 train no.190  loss = 5.46478 avg_loss = 5.59887\n",
      "epoch no.22 train no.200  loss = 5.48807 avg_loss = 5.56764\n",
      "epoch no.22 train no.210  loss = 4.85939 avg_loss = 5.57099\n",
      "epoch no.22 train no.220  loss = 5.75235 avg_loss = 5.54803\n",
      "epoch no.22 train no.230  loss = 6.64340 avg_loss = 5.56827\n",
      "epoch no.22 train no.240  loss = 7.03816 avg_loss = 5.55465\n",
      "epoch no.22 train no.250  loss = 5.14296 avg_loss = 5.58868\n",
      "epoch no.22 train no.260  loss = 4.48676 avg_loss = 5.60235\n",
      "epoch no.22 train no.270  loss = 5.99119 avg_loss = 5.58295\n",
      "epoch no.22 train no.280  loss = 6.28770 avg_loss = 5.60058\n",
      "epoch no.22 train no.290  loss = 6.63016 avg_loss = 5.59955\n",
      "epoch no.22 train no.300  loss = 6.09010 avg_loss = 5.61985\n",
      "epoch no.22 train no.310  loss = 6.33423 avg_loss = 5.59451\n",
      "epoch no.22 train no.320  loss = 5.73362 avg_loss = 5.58381\n",
      "epoch no.22 train no.330  loss = 5.20881 avg_loss = 5.59564\n",
      "epoch no.22 train no.340  loss = 4.37938 avg_loss = 5.57206\n",
      "epoch no.22 train no.350  loss = 6.23672 avg_loss = 5.58277\n",
      "epoch no.22 train no.360  loss = 5.94041 avg_loss = 5.57955\n",
      "epoch no.22 train no.370  loss = 5.52585 avg_loss = 5.59609\n",
      "epoch no.22 train no.380  loss = 6.01611 avg_loss = 5.55654\n",
      "epoch no.22 train no.390  loss = 5.46797 avg_loss = 5.54370\n",
      "epoch no.22 train no.400  loss = 4.74751 avg_loss = 5.53550\n",
      "epoch no.22 train no.410  loss = 4.89612 avg_loss = 5.52560\n",
      "epoch no.22 train no.420  loss = 4.91772 avg_loss = 5.56479\n",
      "epoch no.22 train no.430  loss = 5.49182 avg_loss = 5.55661\n",
      "epoch no.22 train no.440  loss = 3.48124 avg_loss = 5.52417\n",
      "epoch no.22 train no.450  loss = 5.39045 avg_loss = 5.47903\n",
      "epoch no.22 train no.460  loss = 6.51022 avg_loss = 5.52319\n",
      "epoch no.22 train no.470  loss = 3.56987 avg_loss = 5.41842\n",
      "epoch no.22 train no.480  loss = 4.79123 avg_loss = 5.42654\n",
      "epoch no.22 train no.490  loss = 6.50140 avg_loss = 5.42936\n",
      "epoch no.22 train no.500  loss = 5.83207 avg_loss = 5.43585\n",
      "epoch no.22 train no.510  loss = 7.25316 avg_loss = 5.45535\n",
      "epoch no.22 train no.520  loss = 6.58530 avg_loss = 5.50105\n",
      "epoch no.22 train no.530  loss = 7.43241 avg_loss = 5.55314\n",
      "epoch no.22 train no.540  loss = 6.21546 avg_loss = 5.52874\n",
      "epoch no.22 train no.550  loss = 3.45335 avg_loss = 5.47473\n",
      "epoch no.22 train no.560  loss = 4.95188 avg_loss = 5.46140\n",
      "epoch no.22 train no.570  loss = 4.58487 avg_loss = 5.50940\n",
      "epoch no.22 train no.580  loss = 5.48969 avg_loss = 5.52645\n",
      "epoch no.22 train no.590  loss = 6.85560 avg_loss = 5.53118\n",
      "epoch no.22 train no.600  loss = 7.26872 avg_loss = 5.55316\n",
      "epoch no.22 train no.610  loss = 6.15623 avg_loss = 5.57153\n",
      "epoch no.22 train no.620  loss = 4.99339 avg_loss = 5.57427\n",
      "epoch no.22 train no.630  loss = 4.09540 avg_loss = 5.57950\n",
      "epoch no.22 train no.640  loss = 5.21754 avg_loss = 5.55378\n",
      "epoch no.22 train no.650  loss = 3.93387 avg_loss = 5.49466\n",
      "epoch no.22 train no.660  loss = 3.10864 avg_loss = 5.47811\n",
      "epoch no.22 train no.670  loss = 5.13576 avg_loss = 5.47898\n",
      "epoch no.22 train no.680  loss = 4.48052 avg_loss = 5.48891\n",
      "epoch no.22 train no.690  loss = 5.09332 avg_loss = 5.52878\n",
      "epoch no.22 train no.700  loss = 7.00965 avg_loss = 5.54528\n",
      "epoch no.22 train no.710  loss = 5.26260 avg_loss = 5.57620\n",
      "epoch no.22 train no.720  loss = 5.98849 avg_loss = 5.58686\n",
      "epoch no.22 train no.730  loss = 5.84145 avg_loss = 5.61514\n",
      "epoch no.22 train no.740  loss = 7.14565 avg_loss = 5.64545\n",
      "epoch no.22 train no.750  loss = 6.01095 avg_loss = 5.64129\n",
      "epoch no.22 train no.760  loss = 5.49580 avg_loss = 5.60728\n",
      "epoch no.22 train no.770  loss = 5.16363 avg_loss = 5.60192\n",
      "epoch no.22 train no.780  loss = 4.27895 avg_loss = 5.64233\n",
      "epoch no.22 train no.790  loss = 5.93135 avg_loss = 5.64874\n",
      "epoch no.22 train no.800  loss = 5.51291 avg_loss = 5.66085\n",
      "epoch no.22 train no.810  loss = 3.72205 avg_loss = 5.66238\n",
      "epoch no.22 train no.820  loss = 5.16250 avg_loss = 5.63002\n",
      "epoch no.22 train no.830  loss = 5.60026 avg_loss = 5.64245\n",
      "epoch no.22 train no.840  loss = 5.78468 avg_loss = 5.61755\n",
      "epoch no.22 train no.850  loss = 4.24580 avg_loss = 5.61047\n",
      "epoch no.22 train no.860  loss = 6.51551 avg_loss = 5.65049\n",
      "epoch no.22 train no.870  loss = 5.56680 avg_loss = 5.61007\n",
      "epoch no.22 train no.880  loss = 4.52385 avg_loss = 5.57419\n",
      "epoch no.22 train no.890  loss = 4.71022 avg_loss = 5.54104\n",
      "epoch no.22 train no.900  loss = 6.91930 avg_loss = 5.54403\n",
      "epoch no.22 train no.910  loss = 4.26179 avg_loss = 5.53254\n",
      "epoch no.22 train no.920  loss = 5.48427 avg_loss = 5.53848\n",
      "epoch no.22 train no.930  loss = 6.47215 avg_loss = 5.53880\n",
      "epoch no.22 train no.940  loss = 6.21689 avg_loss = 5.61201\n",
      "epoch no.22 train no.950  loss = 6.52840 avg_loss = 5.62193\n",
      "epoch no.22 train no.960  loss = 5.59815 avg_loss = 5.63699\n",
      "epoch no.22 train no.970  loss = 5.44981 avg_loss = 5.67450\n",
      "epoch no.22 train no.980  loss = 4.78991 avg_loss = 5.69200\n",
      "epoch no.22 train no.990  loss = 6.02690 avg_loss = 5.65610\n",
      "epoch no.22 train no.1000  loss = 6.02532 avg_loss = 5.68149\n",
      "to_tokens: ['</s>', '</s>', '</s>']\n",
      "뭐함</s>\n",
      "epoch no.22 train no.1010  loss = 4.59762 avg_loss = 5.66813\n",
      "epoch no.22 train no.1020  loss = 6.16253 avg_loss = 5.68042\n",
      "epoch no.22 train no.1030  loss = 5.90905 avg_loss = 5.67458\n",
      "epoch no.22 train no.1040  loss = 5.79333 avg_loss = 5.63667\n",
      "epoch no.22 train no.1050  loss = 4.61991 avg_loss = 5.62218\n",
      "epoch no.22 train no.1060  loss = 7.07023 avg_loss = 5.67875\n",
      "epoch no.22 train no.1070  loss = 6.69609 avg_loss = 5.67329\n",
      "epoch no.22 train no.1080  loss = 5.08582 avg_loss = 5.62210\n",
      "epoch no.22 train no.1090  loss = 5.10086 avg_loss = 5.62807\n",
      "epoch no.22 train no.1100  loss = 5.06339 avg_loss = 5.58776\n",
      "epoch no.22 train no.1110  loss = 6.24218 avg_loss = 5.56613\n",
      "epoch no.22 train no.1120  loss = 5.65465 avg_loss = 5.60204\n",
      "epoch no.22 train no.1130  loss = 6.58920 avg_loss = 5.60494\n",
      "epoch no.22 train no.1140  loss = 6.03150 avg_loss = 5.64975\n",
      "epoch no.22 train no.1150  loss = 6.55449 avg_loss = 5.63577\n",
      "epoch no.22 train no.1160  loss = 5.08568 avg_loss = 5.63990\n",
      "epoch no.22 train no.1170  loss = 4.81420 avg_loss = 5.64558\n",
      "epoch no.22 train no.1180  loss = 4.78697 avg_loss = 5.61005\n",
      "epoch no.22 train no.1190  loss = 5.51347 avg_loss = 5.58900\n",
      "epoch no.22 train no.1200  loss = 5.21925 avg_loss = 5.58383\n",
      "epoch no.22 train no.1210  loss = 6.41430 avg_loss = 5.55595\n",
      "epoch no.22 train no.1220  loss = 6.82470 avg_loss = 5.60105\n",
      "epoch no.22 train no.1230  loss = 5.12488 avg_loss = 5.62130\n",
      "epoch no.22 train no.1240  loss = 5.66626 avg_loss = 5.59819\n",
      "epoch no.22 train no.1250  loss = 7.17095 avg_loss = 5.67388\n",
      "epoch no.22 train no.1260  loss = 6.83451 avg_loss = 5.70235\n",
      "epoch no.22 train no.1270  loss = 3.22971 avg_loss = 5.65406\n",
      "epoch no.22 train no.1280  loss = 6.23871 avg_loss = 5.66282\n",
      "epoch no.22 train no.1290  loss = 6.19174 avg_loss = 5.67273\n",
      "epoch no.22 train no.1300  loss = 5.71103 avg_loss = 5.67725\n",
      "epoch no.22 train no.1310  loss = 5.24644 avg_loss = 5.64735\n",
      "epoch no.22 train no.1320  loss = 4.29509 avg_loss = 5.61576\n",
      "epoch no.22 train no.1330  loss = 4.84226 avg_loss = 5.62791\n",
      "epoch no.22 train no.1340  loss = 3.66585 avg_loss = 5.61289\n",
      "epoch no.22 train no.1350  loss = 6.81608 avg_loss = 5.59811\n",
      "epoch no.22 train no.1360  loss = 4.94149 avg_loss = 5.61382\n",
      "epoch no.22 train no.1370  loss = 5.66947 avg_loss = 5.58870\n",
      "epoch no.22 train no.1380  loss = 5.72679 avg_loss = 5.61129\n",
      "epoch no.22 train no.1390  loss = 6.26998 avg_loss = 5.64726\n",
      "epoch no.22 train no.1400  loss = 5.93987 avg_loss = 5.65274\n",
      "epoch no.22 train no.1410  loss = 5.56089 avg_loss = 5.67459\n",
      "epoch no.22 train no.1420  loss = 5.06022 avg_loss = 5.64184\n",
      "epoch no.22 train no.1430  loss = 5.41682 avg_loss = 5.64318\n",
      "epoch no.22 train no.1440  loss = 4.76730 avg_loss = 5.63720\n",
      "epoch no.22 train no.1450  loss = 6.20181 avg_loss = 5.64622\n",
      "epoch no.22 train no.1460  loss = 4.27020 avg_loss = 5.62335\n",
      "epoch no.22 train no.1470  loss = 5.57230 avg_loss = 5.62893\n",
      "epoch no.22 train no.1480  loss = 5.52404 avg_loss = 5.62308\n",
      "epoch no.22 train no.1490  loss = 5.45665 avg_loss = 5.64637\n",
      "epoch no.22 train no.1500  loss = 6.67329 avg_loss = 5.67998\n",
      "epoch no.22 train no.1510  loss = 5.45723 avg_loss = 5.66641\n",
      "epoch no.22 train no.1520  loss = 4.35848 avg_loss = 5.64256\n",
      "epoch no.22 train no.1530  loss = 7.67127 avg_loss = 5.63217\n",
      "epoch no.22 train no.1540  loss = 6.15650 avg_loss = 5.65975\n",
      "epoch no.22 train no.1550  loss = 6.02003 avg_loss = 5.68970\n",
      "epoch no.22 train no.1560  loss = 6.24387 avg_loss = 5.69208\n",
      "epoch no.22 train no.1570  loss = 5.16417 avg_loss = 5.70065\n",
      "epoch no.22 train no.1580  loss = 5.97101 avg_loss = 5.69811\n",
      "epoch no.22 train no.1590  loss = 5.11724 avg_loss = 5.70005\n",
      "epoch no.22 train no.1600  loss = 5.27031 avg_loss = 5.66764\n",
      "epoch no.22 train no.1610  loss = 5.97038 avg_loss = 5.67176\n",
      "epoch no.22 train no.1620  loss = 6.72132 avg_loss = 5.69442\n",
      "epoch no.22 train no.1630  loss = 5.17413 avg_loss = 5.68969\n",
      "epoch no.22 train no.1640  loss = 5.57202 avg_loss = 5.70941\n",
      "epoch no.22 train no.1650  loss = 4.73869 avg_loss = 5.73883\n",
      "epoch no.22 train no.1660  loss = 6.87604 avg_loss = 5.76260\n",
      "epoch no.22 train no.1670  loss = 5.51786 avg_loss = 5.75113\n",
      "epoch no.22 train no.1680  loss = 5.64538 avg_loss = 5.74731\n",
      "epoch no.22 train no.1690  loss = 5.21276 avg_loss = 5.72710\n",
      "epoch no.22 train no.1700  loss = 6.68620 avg_loss = 5.75364\n",
      "epoch no.22 train no.1710  loss = 7.11628 avg_loss = 5.74127\n",
      "epoch no.22 train no.1720  loss = 6.40978 avg_loss = 5.77175\n",
      "epoch no.22 train no.1730  loss = 5.94093 avg_loss = 5.79322\n",
      "epoch no.22 train no.1740  loss = 4.70192 avg_loss = 5.80270\n",
      "epoch no.22 train no.1750  loss = 5.97731 avg_loss = 5.79842\n",
      "epoch no.23 train no.0  loss = 5.43280 avg_loss = 5.79739\n",
      "epoch no.23 train no.10  loss = 5.57287 avg_loss = 5.77958\n",
      "epoch no.23 train no.20  loss = 7.30791 avg_loss = 5.76340\n",
      "epoch no.23 train no.30  loss = 7.42236 avg_loss = 5.73848\n",
      "epoch no.23 train no.40  loss = 7.35730 avg_loss = 5.73998\n",
      "epoch no.23 train no.50  loss = 6.47221 avg_loss = 5.74181\n",
      "epoch no.23 train no.60  loss = 5.45806 avg_loss = 5.72682\n",
      "epoch no.23 train no.70  loss = 6.39747 avg_loss = 5.72148\n",
      "epoch no.23 train no.80  loss = 4.96914 avg_loss = 5.74217\n",
      "epoch no.23 train no.90  loss = 5.24383 avg_loss = 5.71698\n",
      "epoch no.23 train no.100  loss = 6.44135 avg_loss = 5.70582\n",
      "epoch no.23 train no.110  loss = 4.98300 avg_loss = 5.71286\n",
      "epoch no.23 train no.120  loss = 5.12788 avg_loss = 5.70634\n",
      "epoch no.23 train no.130  loss = 6.75695 avg_loss = 5.70351\n",
      "epoch no.23 train no.140  loss = 5.90508 avg_loss = 5.70214\n",
      "epoch no.23 train no.150  loss = 5.99404 avg_loss = 5.70417\n",
      "epoch no.23 train no.160  loss = 6.27234 avg_loss = 5.70505\n",
      "epoch no.23 train no.170  loss = 7.83644 avg_loss = 5.72474\n",
      "epoch no.23 train no.180  loss = 5.81842 avg_loss = 5.75082\n",
      "epoch no.23 train no.190  loss = 6.40150 avg_loss = 5.77879\n",
      "epoch no.23 train no.200  loss = 7.71530 avg_loss = 5.79863\n",
      "epoch no.23 train no.210  loss = 6.28629 avg_loss = 5.81012\n",
      "epoch no.23 train no.220  loss = 5.38916 avg_loss = 5.81425\n",
      "epoch no.23 train no.230  loss = 4.58199 avg_loss = 5.78516\n",
      "epoch no.23 train no.240  loss = 5.94633 avg_loss = 5.75341\n",
      "epoch no.23 train no.250  loss = 4.35345 avg_loss = 5.75492\n",
      "epoch no.23 train no.260  loss = 6.02875 avg_loss = 5.75269\n",
      "epoch no.23 train no.270  loss = 6.29333 avg_loss = 5.74659\n",
      "epoch no.23 train no.280  loss = 6.70688 avg_loss = 5.76179\n",
      "epoch no.23 train no.290  loss = 4.36168 avg_loss = 5.74106\n",
      "epoch no.23 train no.300  loss = 5.45352 avg_loss = 5.73735\n",
      "epoch no.23 train no.310  loss = 5.55271 avg_loss = 5.71382\n",
      "epoch no.23 train no.320  loss = 5.20201 avg_loss = 5.71044\n",
      "epoch no.23 train no.330  loss = 5.39894 avg_loss = 5.71222\n",
      "epoch no.23 train no.340  loss = 5.31597 avg_loss = 5.71429\n",
      "epoch no.23 train no.350  loss = 5.52643 avg_loss = 5.67857\n",
      "epoch no.23 train no.360  loss = 5.06192 avg_loss = 5.65911\n",
      "epoch no.23 train no.370  loss = 4.65981 avg_loss = 5.59232\n",
      "epoch no.23 train no.380  loss = 4.60898 avg_loss = 5.58238\n",
      "epoch no.23 train no.390  loss = 5.67525 avg_loss = 5.59456\n",
      "epoch no.23 train no.400  loss = 7.02776 avg_loss = 5.63936\n",
      "epoch no.23 train no.410  loss = 5.12003 avg_loss = 5.62388\n",
      "epoch no.23 train no.420  loss = 5.48649 avg_loss = 5.58935\n",
      "epoch no.23 train no.430  loss = 5.89292 avg_loss = 5.61323\n",
      "epoch no.23 train no.440  loss = 5.35895 avg_loss = 5.66206\n",
      "epoch no.23 train no.450  loss = 5.78238 avg_loss = 5.67144\n",
      "epoch no.23 train no.460  loss = 4.64995 avg_loss = 5.60151\n",
      "epoch no.23 train no.470  loss = 3.61748 avg_loss = 5.55384\n",
      "epoch no.23 train no.480  loss = 5.94005 avg_loss = 5.58097\n",
      "epoch no.23 train no.490  loss = 5.93009 avg_loss = 5.56419\n",
      "epoch no.23 train no.500  loss = 6.62807 avg_loss = 5.59719\n",
      "epoch no.23 train no.510  loss = 4.19176 avg_loss = 5.60940\n",
      "epoch no.23 train no.520  loss = 5.25970 avg_loss = 5.59646\n",
      "epoch no.23 train no.530  loss = 5.28847 avg_loss = 5.57279\n",
      "epoch no.23 train no.540  loss = 5.21874 avg_loss = 5.54266\n",
      "epoch no.23 train no.550  loss = 4.13878 avg_loss = 5.50221\n",
      "epoch no.23 train no.560  loss = 5.72767 avg_loss = 5.49736\n",
      "epoch no.23 train no.570  loss = 6.19155 avg_loss = 5.51092\n",
      "epoch no.23 train no.580  loss = 4.55634 avg_loss = 5.52223\n",
      "epoch no.23 train no.590  loss = 5.04899 avg_loss = 5.53170\n",
      "epoch no.23 train no.600  loss = 6.64690 avg_loss = 5.55883\n",
      "epoch no.23 train no.610  loss = 5.67334 avg_loss = 5.56946\n",
      "epoch no.23 train no.620  loss = 6.23701 avg_loss = 5.57908\n",
      "epoch no.23 train no.630  loss = 5.54478 avg_loss = 5.55207\n",
      "epoch no.23 train no.640  loss = 6.13176 avg_loss = 5.57504\n",
      "epoch no.23 train no.650  loss = 6.66706 avg_loss = 5.57979\n",
      "epoch no.23 train no.660  loss = 4.42953 avg_loss = 5.56875\n",
      "epoch no.23 train no.670  loss = 5.01979 avg_loss = 5.55713\n",
      "epoch no.23 train no.680  loss = 3.58992 avg_loss = 5.58302\n",
      "epoch no.23 train no.690  loss = 7.21150 avg_loss = 5.62535\n",
      "epoch no.23 train no.700  loss = 5.67330 avg_loss = 5.61216\n",
      "epoch no.23 train no.710  loss = 5.69412 avg_loss = 5.66324\n",
      "epoch no.23 train no.720  loss = 5.60575 avg_loss = 5.62223\n",
      "epoch no.23 train no.730  loss = 4.31650 avg_loss = 5.63135\n",
      "epoch no.23 train no.740  loss = 5.30611 avg_loss = 5.63015\n",
      "epoch no.23 train no.750  loss = 6.42209 avg_loss = 5.65139\n",
      "epoch no.23 train no.760  loss = 5.51401 avg_loss = 5.63583\n",
      "epoch no.23 train no.770  loss = 4.65637 avg_loss = 5.65576\n",
      "epoch no.23 train no.780  loss = 5.52636 avg_loss = 5.61567\n",
      "epoch no.23 train no.790  loss = 5.63029 avg_loss = 5.62202\n",
      "epoch no.23 train no.800  loss = 6.64240 avg_loss = 5.64284\n",
      "epoch no.23 train no.810  loss = 7.17969 avg_loss = 5.65119\n",
      "epoch no.23 train no.820  loss = 4.76172 avg_loss = 5.64937\n",
      "epoch no.23 train no.830  loss = 5.23257 avg_loss = 5.63613\n",
      "epoch no.23 train no.840  loss = 6.09456 avg_loss = 5.63099\n",
      "epoch no.23 train no.850  loss = 6.17107 avg_loss = 5.66058\n",
      "epoch no.23 train no.860  loss = 5.62478 avg_loss = 5.69718\n",
      "epoch no.23 train no.870  loss = 5.98304 avg_loss = 5.69451\n",
      "epoch no.23 train no.880  loss = 7.19868 avg_loss = 5.72301\n",
      "epoch no.23 train no.890  loss = 5.89187 avg_loss = 5.64763\n",
      "epoch no.23 train no.900  loss = 5.47450 avg_loss = 5.65961\n",
      "epoch no.23 train no.910  loss = 5.23959 avg_loss = 5.66827\n",
      "epoch no.23 train no.920  loss = 3.73528 avg_loss = 5.66424\n",
      "epoch no.23 train no.930  loss = 4.70292 avg_loss = 5.63581\n",
      "epoch no.23 train no.940  loss = 5.08951 avg_loss = 5.59640\n",
      "epoch no.23 train no.950  loss = 4.78817 avg_loss = 5.56914\n",
      "epoch no.23 train no.960  loss = 5.82225 avg_loss = 5.53665\n",
      "epoch no.23 train no.970  loss = 6.23528 avg_loss = 5.55304\n",
      "epoch no.23 train no.980  loss = 6.21238 avg_loss = 5.59724\n",
      "epoch no.23 train no.990  loss = 5.65913 avg_loss = 5.57039\n",
      "epoch no.23 train no.1000  loss = 6.46476 avg_loss = 5.55243\n",
      "to_tokens: ['</s>', '</s>', '</s>']\n",
      "뭐함</s>\n",
      "epoch no.23 train no.1010  loss = 4.45536 avg_loss = 5.52716\n",
      "epoch no.23 train no.1020  loss = 5.03477 avg_loss = 5.53069\n",
      "epoch no.23 train no.1030  loss = 6.55295 avg_loss = 5.52656\n",
      "epoch no.23 train no.1040  loss = 3.66432 avg_loss = 5.49331\n",
      "epoch no.23 train no.1050  loss = 5.40797 avg_loss = 5.51501\n",
      "epoch no.23 train no.1060  loss = 6.98971 avg_loss = 5.55289\n",
      "epoch no.23 train no.1070  loss = 4.14177 avg_loss = 5.52074\n",
      "epoch no.23 train no.1080  loss = 6.00073 avg_loss = 5.49500\n",
      "epoch no.23 train no.1090  loss = 5.03520 avg_loss = 5.49645\n",
      "epoch no.23 train no.1100  loss = 6.25642 avg_loss = 5.52759\n",
      "epoch no.23 train no.1110  loss = 6.33788 avg_loss = 5.53166\n",
      "epoch no.23 train no.1120  loss = 5.90966 avg_loss = 5.57644\n",
      "epoch no.23 train no.1130  loss = 5.74231 avg_loss = 5.49490\n",
      "epoch no.23 train no.1140  loss = 4.90490 avg_loss = 5.47957\n",
      "epoch no.23 train no.1150  loss = 5.08856 avg_loss = 5.43972\n",
      "epoch no.23 train no.1160  loss = 5.20602 avg_loss = 5.49554\n",
      "epoch no.23 train no.1170  loss = 5.00945 avg_loss = 5.45779\n",
      "epoch no.23 train no.1180  loss = 2.94411 avg_loss = 5.40453\n",
      "epoch no.23 train no.1190  loss = 5.45107 avg_loss = 5.40342\n",
      "epoch no.23 train no.1200  loss = 5.63948 avg_loss = 5.41547\n",
      "epoch no.23 train no.1210  loss = 6.79074 avg_loss = 5.43079\n",
      "epoch no.23 train no.1220  loss = 5.13029 avg_loss = 5.45963\n",
      "epoch no.23 train no.1230  loss = 4.91996 avg_loss = 5.49733\n",
      "epoch no.23 train no.1240  loss = 6.29105 avg_loss = 5.51651\n",
      "epoch no.23 train no.1250  loss = 5.34549 avg_loss = 5.51938\n",
      "epoch no.23 train no.1260  loss = 4.80137 avg_loss = 5.53468\n",
      "epoch no.23 train no.1270  loss = 5.76624 avg_loss = 5.55669\n",
      "epoch no.23 train no.1280  loss = 7.49189 avg_loss = 5.59173\n",
      "epoch no.23 train no.1290  loss = 6.18726 avg_loss = 5.60381\n",
      "epoch no.23 train no.1300  loss = 4.16811 avg_loss = 5.61313\n",
      "epoch no.23 train no.1310  loss = 5.16553 avg_loss = 5.55263\n",
      "epoch no.23 train no.1320  loss = 5.89895 avg_loss = 5.59592\n",
      "epoch no.23 train no.1330  loss = 6.28502 avg_loss = 5.64148\n",
      "epoch no.23 train no.1340  loss = 5.46966 avg_loss = 5.64452\n",
      "epoch no.23 train no.1350  loss = 6.09830 avg_loss = 5.63346\n",
      "epoch no.23 train no.1360  loss = 3.44621 avg_loss = 5.64344\n",
      "epoch no.23 train no.1370  loss = 5.35694 avg_loss = 5.67901\n",
      "epoch no.23 train no.1380  loss = 4.84942 avg_loss = 5.63534\n",
      "epoch no.23 train no.1390  loss = 6.58260 avg_loss = 5.65075\n",
      "epoch no.23 train no.1400  loss = 5.71874 avg_loss = 5.64951\n",
      "epoch no.23 train no.1410  loss = 6.55800 avg_loss = 5.67285\n",
      "epoch no.23 train no.1420  loss = 6.32742 avg_loss = 5.67993\n",
      "epoch no.23 train no.1430  loss = 6.04438 avg_loss = 5.60870\n",
      "epoch no.23 train no.1440  loss = 4.99410 avg_loss = 5.60482\n",
      "epoch no.23 train no.1450  loss = 7.24207 avg_loss = 5.60574\n",
      "epoch no.23 train no.1460  loss = 6.17617 avg_loss = 5.59973\n",
      "epoch no.23 train no.1470  loss = 5.72740 avg_loss = 5.58282\n",
      "epoch no.23 train no.1480  loss = 7.47812 avg_loss = 5.62705\n",
      "epoch no.23 train no.1490  loss = 5.07643 avg_loss = 5.60940\n",
      "epoch no.23 train no.1500  loss = 5.55966 avg_loss = 5.63168\n",
      "epoch no.23 train no.1510  loss = 5.29171 avg_loss = 5.58841\n",
      "epoch no.23 train no.1520  loss = 5.21961 avg_loss = 5.56642\n",
      "epoch no.23 train no.1530  loss = 5.48110 avg_loss = 5.56401\n",
      "epoch no.23 train no.1540  loss = 6.80028 avg_loss = 5.59184\n",
      "epoch no.23 train no.1550  loss = 6.44711 avg_loss = 5.56539\n",
      "epoch no.23 train no.1560  loss = 5.45308 avg_loss = 5.55667\n",
      "epoch no.23 train no.1570  loss = 6.69236 avg_loss = 5.56595\n",
      "epoch no.23 train no.1580  loss = 6.71279 avg_loss = 5.60090\n",
      "epoch no.23 train no.1590  loss = 5.78187 avg_loss = 5.58226\n",
      "epoch no.23 train no.1600  loss = 5.02899 avg_loss = 5.55013\n",
      "epoch no.23 train no.1610  loss = 4.50109 avg_loss = 5.55530\n",
      "epoch no.23 train no.1620  loss = 5.30389 avg_loss = 5.57148\n",
      "epoch no.23 train no.1630  loss = 5.99343 avg_loss = 5.58895\n",
      "epoch no.23 train no.1640  loss = 6.73646 avg_loss = 5.63748\n",
      "epoch no.23 train no.1650  loss = 6.64478 avg_loss = 5.60939\n",
      "epoch no.23 train no.1660  loss = 6.59962 avg_loss = 5.61581\n",
      "epoch no.23 train no.1670  loss = 7.17916 avg_loss = 5.64805\n",
      "epoch no.23 train no.1680  loss = 5.27764 avg_loss = 5.61247\n",
      "epoch no.23 train no.1690  loss = 5.65794 avg_loss = 5.59993\n",
      "epoch no.23 train no.1700  loss = 5.41039 avg_loss = 5.59544\n",
      "epoch no.23 train no.1710  loss = 6.51191 avg_loss = 5.60328\n",
      "epoch no.23 train no.1720  loss = 5.03643 avg_loss = 5.58238\n",
      "epoch no.23 train no.1730  loss = 5.65958 avg_loss = 5.58439\n",
      "epoch no.23 train no.1740  loss = 5.70615 avg_loss = 5.59087\n",
      "epoch no.23 train no.1750  loss = 5.76395 avg_loss = 5.61019\n",
      "epoch no.24 train no.0  loss = 5.79644 avg_loss = 5.62992\n",
      "epoch no.24 train no.10  loss = 5.89939 avg_loss = 5.66705\n",
      "epoch no.24 train no.20  loss = 5.21045 avg_loss = 5.68235\n",
      "epoch no.24 train no.30  loss = 5.59019 avg_loss = 5.70299\n",
      "epoch no.24 train no.40  loss = 5.49973 avg_loss = 5.65500\n",
      "epoch no.24 train no.50  loss = 5.69585 avg_loss = 5.67727\n",
      "epoch no.24 train no.60  loss = 3.57540 avg_loss = 5.68566\n",
      "epoch no.24 train no.70  loss = 5.64070 avg_loss = 5.66241\n",
      "epoch no.24 train no.80  loss = 4.10714 avg_loss = 5.68081\n",
      "epoch no.24 train no.90  loss = 5.49334 avg_loss = 5.68235\n",
      "epoch no.24 train no.100  loss = 5.56177 avg_loss = 5.64943\n",
      "epoch no.24 train no.110  loss = 4.53290 avg_loss = 5.65642\n",
      "epoch no.24 train no.120  loss = 5.18565 avg_loss = 5.66456\n",
      "epoch no.24 train no.130  loss = 5.46776 avg_loss = 5.69149\n",
      "epoch no.24 train no.140  loss = 6.69780 avg_loss = 5.69163\n",
      "epoch no.24 train no.150  loss = 7.04000 avg_loss = 5.67726\n",
      "epoch no.24 train no.160  loss = 5.92729 avg_loss = 5.64616\n",
      "epoch no.24 train no.170  loss = 4.61789 avg_loss = 5.64432\n",
      "epoch no.24 train no.180  loss = 6.17297 avg_loss = 5.65854\n",
      "epoch no.24 train no.190  loss = 5.03937 avg_loss = 5.66406\n",
      "epoch no.24 train no.200  loss = 5.38302 avg_loss = 5.69584\n",
      "epoch no.24 train no.210  loss = 6.70638 avg_loss = 5.66217\n",
      "epoch no.24 train no.220  loss = 3.77789 avg_loss = 5.61805\n",
      "epoch no.24 train no.230  loss = 4.13717 avg_loss = 5.58104\n",
      "epoch no.24 train no.240  loss = 6.19140 avg_loss = 5.60297\n",
      "epoch no.24 train no.250  loss = 5.48561 avg_loss = 5.63723\n",
      "epoch no.24 train no.260  loss = 4.22743 avg_loss = 5.63245\n",
      "epoch no.24 train no.270  loss = 6.99054 avg_loss = 5.66142\n",
      "epoch no.24 train no.280  loss = 5.78372 avg_loss = 5.66252\n",
      "epoch no.24 train no.290  loss = 6.00101 avg_loss = 5.69723\n",
      "epoch no.24 train no.300  loss = 5.16912 avg_loss = 5.70114\n",
      "epoch no.24 train no.310  loss = 4.60544 avg_loss = 5.75031\n",
      "epoch no.24 train no.320  loss = 5.28807 avg_loss = 5.75433\n",
      "epoch no.24 train no.330  loss = 6.22062 avg_loss = 5.77797\n",
      "epoch no.24 train no.340  loss = 5.54467 avg_loss = 5.75993\n",
      "epoch no.24 train no.350  loss = 5.95868 avg_loss = 5.78048\n",
      "epoch no.24 train no.360  loss = 5.95204 avg_loss = 5.77569\n",
      "epoch no.24 train no.370  loss = 6.52158 avg_loss = 5.75316\n",
      "epoch no.24 train no.380  loss = 6.45691 avg_loss = 5.73072\n",
      "epoch no.24 train no.390  loss = 6.24810 avg_loss = 5.73421\n",
      "epoch no.24 train no.400  loss = 5.76481 avg_loss = 5.68394\n",
      "epoch no.24 train no.410  loss = 6.37650 avg_loss = 5.70854\n",
      "epoch no.24 train no.420  loss = 5.63376 avg_loss = 5.66789\n",
      "epoch no.24 train no.430  loss = 5.47748 avg_loss = 5.65434\n",
      "epoch no.24 train no.440  loss = 4.59972 avg_loss = 5.67732\n",
      "epoch no.24 train no.450  loss = 5.20715 avg_loss = 5.63823\n",
      "epoch no.24 train no.460  loss = 5.89615 avg_loss = 5.66750\n",
      "epoch no.24 train no.470  loss = 6.70052 avg_loss = 5.62995\n",
      "epoch no.24 train no.480  loss = 5.03161 avg_loss = 5.60147\n",
      "epoch no.24 train no.490  loss = 4.51005 avg_loss = 5.56437\n",
      "epoch no.24 train no.500  loss = 3.92153 avg_loss = 5.57379\n",
      "epoch no.24 train no.510  loss = 6.94784 avg_loss = 5.60162\n",
      "epoch no.24 train no.520  loss = 5.43880 avg_loss = 5.58191\n",
      "epoch no.24 train no.530  loss = 5.28615 avg_loss = 5.54716\n",
      "epoch no.24 train no.540  loss = 5.22129 avg_loss = 5.56153\n",
      "epoch no.24 train no.550  loss = 5.53244 avg_loss = 5.56287\n",
      "epoch no.24 train no.560  loss = 4.69519 avg_loss = 5.56491\n",
      "epoch no.24 train no.570  loss = 4.21112 avg_loss = 5.55906\n",
      "epoch no.24 train no.580  loss = 4.87700 avg_loss = 5.57005\n",
      "epoch no.24 train no.590  loss = 5.66890 avg_loss = 5.56165\n",
      "epoch no.24 train no.600  loss = 5.97849 avg_loss = 5.56033\n",
      "epoch no.24 train no.610  loss = 6.79022 avg_loss = 5.60764\n",
      "epoch no.24 train no.620  loss = 6.52983 avg_loss = 5.63617\n",
      "epoch no.24 train no.630  loss = 6.67343 avg_loss = 5.66610\n",
      "epoch no.24 train no.640  loss = 3.14804 avg_loss = 5.62750\n",
      "epoch no.24 train no.650  loss = 7.04457 avg_loss = 5.67169\n",
      "epoch no.24 train no.660  loss = 5.71293 avg_loss = 5.66871\n",
      "epoch no.24 train no.670  loss = 5.80845 avg_loss = 5.66801\n",
      "epoch no.24 train no.680  loss = 5.85738 avg_loss = 5.66505\n",
      "epoch no.24 train no.690  loss = 5.12404 avg_loss = 5.62707\n",
      "epoch no.24 train no.700  loss = 5.31698 avg_loss = 5.63406\n",
      "epoch no.24 train no.710  loss = 4.52656 avg_loss = 5.61405\n",
      "epoch no.24 train no.720  loss = 6.29139 avg_loss = 5.64845\n",
      "epoch no.24 train no.730  loss = 4.64440 avg_loss = 5.64589\n",
      "epoch no.24 train no.740  loss = 5.08545 avg_loss = 5.65151\n",
      "epoch no.24 train no.750  loss = 5.00137 avg_loss = 5.61027\n",
      "epoch no.24 train no.760  loss = 5.95378 avg_loss = 5.58018\n",
      "epoch no.24 train no.770  loss = 4.29623 avg_loss = 5.57547\n",
      "epoch no.24 train no.780  loss = 7.54917 avg_loss = 5.56506\n",
      "epoch no.24 train no.790  loss = 5.03648 avg_loss = 5.61180\n",
      "epoch no.24 train no.800  loss = 4.82487 avg_loss = 5.59583\n",
      "epoch no.24 train no.810  loss = 5.28792 avg_loss = 5.56208\n",
      "epoch no.24 train no.820  loss = 6.85584 avg_loss = 5.54645\n",
      "epoch no.24 train no.830  loss = 5.64083 avg_loss = 5.56722\n",
      "epoch no.24 train no.840  loss = 4.18995 avg_loss = 5.49549\n",
      "epoch no.24 train no.850  loss = 4.71906 avg_loss = 5.45641\n",
      "epoch no.24 train no.860  loss = 4.61982 avg_loss = 5.50613\n",
      "epoch no.24 train no.870  loss = 7.32593 avg_loss = 5.54440\n",
      "epoch no.24 train no.880  loss = 4.81713 avg_loss = 5.53171\n",
      "epoch no.24 train no.890  loss = 5.90260 avg_loss = 5.56464\n",
      "epoch no.24 train no.900  loss = 6.79864 avg_loss = 5.56189\n",
      "epoch no.24 train no.910  loss = 5.35034 avg_loss = 5.54888\n",
      "epoch no.24 train no.920  loss = 6.42026 avg_loss = 5.51261\n",
      "epoch no.24 train no.930  loss = 5.80715 avg_loss = 5.50986\n",
      "epoch no.24 train no.940  loss = 5.54869 avg_loss = 5.47592\n",
      "epoch no.24 train no.950  loss = 6.24784 avg_loss = 5.45247\n",
      "epoch no.24 train no.960  loss = 3.12120 avg_loss = 5.45302\n",
      "epoch no.24 train no.970  loss = 5.89008 avg_loss = 5.45249\n",
      "epoch no.24 train no.980  loss = 7.06281 avg_loss = 5.46569\n",
      "epoch no.24 train no.990  loss = 5.88382 avg_loss = 5.48377\n",
      "epoch no.24 train no.1000  loss = 3.97659 avg_loss = 5.48095\n",
      "to_tokens: ['</s>', '</s>', '</s>']\n",
      "뭐함</s>\n",
      "epoch no.24 train no.1010  loss = 4.59176 avg_loss = 5.47988\n",
      "epoch no.24 train no.1020  loss = 7.38135 avg_loss = 5.51935\n",
      "epoch no.24 train no.1030  loss = 6.16627 avg_loss = 5.55054\n",
      "epoch no.24 train no.1040  loss = 6.63471 avg_loss = 5.49842\n",
      "epoch no.24 train no.1050  loss = 4.99048 avg_loss = 5.50596\n",
      "epoch no.24 train no.1060  loss = 4.98678 avg_loss = 5.51765\n",
      "epoch no.24 train no.1070  loss = 5.84169 avg_loss = 5.53728\n",
      "epoch no.24 train no.1080  loss = 5.80041 avg_loss = 5.55305\n",
      "epoch no.24 train no.1090  loss = 6.39325 avg_loss = 5.59265\n",
      "epoch no.24 train no.1100  loss = 5.39095 avg_loss = 5.62237\n",
      "epoch no.24 train no.1110  loss = 5.99343 avg_loss = 5.64770\n",
      "epoch no.24 train no.1120  loss = 5.07509 avg_loss = 5.63215\n",
      "epoch no.24 train no.1130  loss = 5.52805 avg_loss = 5.60608\n",
      "epoch no.24 train no.1140  loss = 5.40747 avg_loss = 5.59903\n",
      "epoch no.24 train no.1150  loss = 5.80125 avg_loss = 5.59020\n",
      "epoch no.24 train no.1160  loss = 6.79408 avg_loss = 5.55164\n",
      "epoch no.24 train no.1170  loss = 5.52232 avg_loss = 5.57622\n",
      "epoch no.24 train no.1180  loss = 5.51471 avg_loss = 5.58399\n",
      "epoch no.24 train no.1190  loss = 4.65537 avg_loss = 5.58313\n",
      "epoch no.24 train no.1200  loss = 6.18169 avg_loss = 5.61853\n",
      "epoch no.24 train no.1210  loss = 6.33796 avg_loss = 5.65472\n",
      "epoch no.24 train no.1220  loss = 6.33477 avg_loss = 5.64277\n",
      "epoch no.24 train no.1230  loss = 6.21388 avg_loss = 5.67657\n",
      "epoch no.24 train no.1240  loss = 5.31840 avg_loss = 5.65601\n",
      "epoch no.24 train no.1250  loss = 7.10711 avg_loss = 5.63467\n",
      "epoch no.24 train no.1260  loss = 5.93787 avg_loss = 5.63558\n",
      "epoch no.24 train no.1270  loss = 4.74378 avg_loss = 5.65084\n",
      "epoch no.24 train no.1280  loss = 4.53784 avg_loss = 5.63358\n",
      "epoch no.24 train no.1290  loss = 5.49066 avg_loss = 5.61840\n",
      "epoch no.24 train no.1300  loss = 5.81020 avg_loss = 5.62644\n",
      "epoch no.24 train no.1310  loss = 4.64021 avg_loss = 5.60286\n",
      "epoch no.24 train no.1320  loss = 3.80014 avg_loss = 5.57447\n",
      "epoch no.24 train no.1330  loss = 6.21339 avg_loss = 5.57598\n",
      "epoch no.24 train no.1340  loss = 6.19917 avg_loss = 5.60736\n",
      "epoch no.24 train no.1350  loss = 4.87507 avg_loss = 5.57329\n",
      "epoch no.24 train no.1360  loss = 3.49404 avg_loss = 5.57728\n",
      "epoch no.24 train no.1370  loss = 6.74104 avg_loss = 5.59049\n",
      "epoch no.24 train no.1380  loss = 6.82754 avg_loss = 5.60582\n",
      "epoch no.24 train no.1390  loss = 4.48103 avg_loss = 5.59016\n",
      "epoch no.24 train no.1400  loss = 5.54884 avg_loss = 5.58403\n",
      "epoch no.24 train no.1410  loss = 5.08583 avg_loss = 5.60527\n",
      "epoch no.24 train no.1420  loss = 6.31988 avg_loss = 5.63432\n",
      "epoch no.24 train no.1430  loss = 5.69360 avg_loss = 5.59070\n",
      "epoch no.24 train no.1440  loss = 6.68433 avg_loss = 5.61848\n",
      "epoch no.24 train no.1450  loss = 5.29018 avg_loss = 5.59469\n",
      "epoch no.24 train no.1460  loss = 6.91945 avg_loss = 5.60905\n",
      "epoch no.24 train no.1470  loss = 5.52711 avg_loss = 5.60275\n",
      "epoch no.24 train no.1480  loss = 6.66386 avg_loss = 5.59214\n",
      "epoch no.24 train no.1490  loss = 5.92582 avg_loss = 5.60959\n",
      "epoch no.24 train no.1500  loss = 6.88457 avg_loss = 5.63956\n",
      "epoch no.24 train no.1510  loss = 6.11737 avg_loss = 5.65692\n",
      "epoch no.24 train no.1520  loss = 5.77312 avg_loss = 5.65747\n",
      "epoch no.24 train no.1530  loss = 5.18261 avg_loss = 5.66768\n",
      "epoch no.24 train no.1540  loss = 5.11035 avg_loss = 5.62867\n",
      "epoch no.24 train no.1550  loss = 5.68009 avg_loss = 5.65153\n",
      "epoch no.24 train no.1560  loss = 5.40267 avg_loss = 5.61331\n",
      "epoch no.24 train no.1570  loss = 5.97830 avg_loss = 5.63795\n",
      "epoch no.24 train no.1580  loss = 5.03379 avg_loss = 5.63240\n",
      "epoch no.24 train no.1590  loss = 6.16231 avg_loss = 5.62629\n",
      "epoch no.24 train no.1600  loss = 6.12789 avg_loss = 5.59304\n",
      "epoch no.24 train no.1610  loss = 5.89777 avg_loss = 5.59289\n",
      "epoch no.24 train no.1620  loss = 2.30242 avg_loss = 5.52501\n",
      "epoch no.24 train no.1630  loss = 6.66594 avg_loss = 5.51394\n",
      "epoch no.24 train no.1640  loss = 4.69489 avg_loss = 5.50840\n",
      "epoch no.24 train no.1650  loss = 5.49476 avg_loss = 5.55348\n",
      "epoch no.24 train no.1660  loss = 4.76859 avg_loss = 5.56950\n",
      "epoch no.24 train no.1670  loss = 6.49418 avg_loss = 5.57242\n",
      "epoch no.24 train no.1680  loss = 4.76047 avg_loss = 5.58152\n",
      "epoch no.24 train no.1690  loss = 5.12869 avg_loss = 5.59431\n",
      "epoch no.24 train no.1700  loss = 4.59112 avg_loss = 5.59009\n",
      "epoch no.24 train no.1710  loss = 4.76354 avg_loss = 5.60241\n",
      "epoch no.24 train no.1720  loss = 5.93308 avg_loss = 5.62096\n",
      "epoch no.24 train no.1730  loss = 5.43595 avg_loss = 5.63383\n",
      "epoch no.24 train no.1740  loss = 5.26566 avg_loss = 5.64128\n",
      "epoch no.24 train no.1750  loss = 5.37401 avg_loss = 5.62970\n",
      "epoch no.25 train no.0  loss = 5.45017 avg_loss = 5.65090\n",
      "epoch no.25 train no.10  loss = 5.37771 avg_loss = 5.64265\n",
      "epoch no.25 train no.20  loss = 5.45860 avg_loss = 5.63426\n",
      "epoch no.25 train no.30  loss = 3.38480 avg_loss = 5.59287\n",
      "epoch no.25 train no.40  loss = 5.56917 avg_loss = 5.60478\n",
      "epoch no.25 train no.50  loss = 4.31779 avg_loss = 5.59117\n",
      "epoch no.25 train no.60  loss = 6.25552 avg_loss = 5.66406\n",
      "epoch no.25 train no.70  loss = 6.22004 avg_loss = 5.66573\n",
      "epoch no.25 train no.80  loss = 6.48132 avg_loss = 5.67106\n",
      "epoch no.25 train no.90  loss = 7.51859 avg_loss = 5.66947\n",
      "epoch no.25 train no.100  loss = 6.06482 avg_loss = 5.67438\n",
      "epoch no.25 train no.110  loss = 5.07055 avg_loss = 5.63106\n",
      "epoch no.25 train no.120  loss = 5.47667 avg_loss = 5.63346\n",
      "epoch no.25 train no.130  loss = 6.41367 avg_loss = 5.68634\n",
      "epoch no.25 train no.140  loss = 5.38392 avg_loss = 5.68437\n",
      "epoch no.25 train no.150  loss = 7.44676 avg_loss = 5.67560\n",
      "epoch no.25 train no.160  loss = 5.68395 avg_loss = 5.63203\n",
      "epoch no.25 train no.170  loss = 6.24496 avg_loss = 5.62386\n",
      "epoch no.25 train no.180  loss = 4.76836 avg_loss = 5.63925\n",
      "epoch no.25 train no.190  loss = 5.35042 avg_loss = 5.64387\n",
      "epoch no.25 train no.200  loss = 5.08975 avg_loss = 5.62640\n",
      "epoch no.25 train no.210  loss = 4.32937 avg_loss = 5.62959\n",
      "epoch no.25 train no.220  loss = 5.91603 avg_loss = 5.62643\n",
      "epoch no.25 train no.230  loss = 6.89088 avg_loss = 5.64822\n",
      "epoch no.25 train no.240  loss = 6.48186 avg_loss = 5.63895\n",
      "epoch no.25 train no.250  loss = 5.28700 avg_loss = 5.64357\n",
      "epoch no.25 train no.260  loss = 5.25045 avg_loss = 5.61003\n",
      "epoch no.25 train no.270  loss = 5.52048 avg_loss = 5.64447\n",
      "epoch no.25 train no.280  loss = 6.49774 avg_loss = 5.67629\n",
      "epoch no.25 train no.290  loss = 7.18076 avg_loss = 5.68139\n",
      "epoch no.25 train no.300  loss = 7.04469 avg_loss = 5.69295\n",
      "epoch no.25 train no.310  loss = 5.76623 avg_loss = 5.67562\n",
      "epoch no.25 train no.320  loss = 6.66928 avg_loss = 5.70859\n",
      "epoch no.25 train no.330  loss = 6.32046 avg_loss = 5.67944\n",
      "epoch no.25 train no.340  loss = 5.46048 avg_loss = 5.65465\n",
      "epoch no.25 train no.350  loss = 4.93638 avg_loss = 5.61144\n",
      "epoch no.25 train no.360  loss = 5.58467 avg_loss = 5.56145\n",
      "epoch no.25 train no.370  loss = 6.58377 avg_loss = 5.56535\n",
      "epoch no.25 train no.380  loss = 4.55770 avg_loss = 5.54079\n",
      "epoch no.25 train no.390  loss = 4.39522 avg_loss = 5.51926\n",
      "epoch no.25 train no.400  loss = 4.24804 avg_loss = 5.55742\n",
      "epoch no.25 train no.410  loss = 4.35180 avg_loss = 5.55718\n",
      "epoch no.25 train no.420  loss = 5.44776 avg_loss = 5.55868\n",
      "epoch no.25 train no.430  loss = 5.45852 avg_loss = 5.53495\n",
      "epoch no.25 train no.440  loss = 5.51542 avg_loss = 5.58700\n",
      "epoch no.25 train no.450  loss = 6.14386 avg_loss = 5.62105\n",
      "epoch no.25 train no.460  loss = 6.02966 avg_loss = 5.63442\n",
      "epoch no.25 train no.470  loss = 6.38026 avg_loss = 5.64677\n",
      "epoch no.25 train no.480  loss = 7.36863 avg_loss = 5.67012\n",
      "epoch no.25 train no.490  loss = 5.13057 avg_loss = 5.64058\n",
      "epoch no.25 train no.500  loss = 5.05210 avg_loss = 5.67337\n",
      "epoch no.25 train no.510  loss = 6.68474 avg_loss = 5.65901\n",
      "epoch no.25 train no.520  loss = 5.88117 avg_loss = 5.65100\n",
      "epoch no.25 train no.530  loss = 3.99850 avg_loss = 5.60583\n",
      "epoch no.25 train no.540  loss = 3.99246 avg_loss = 5.58913\n",
      "epoch no.25 train no.550  loss = 6.26693 avg_loss = 5.64742\n",
      "epoch no.25 train no.560  loss = 3.87457 avg_loss = 5.60947\n",
      "epoch no.25 train no.570  loss = 4.46582 avg_loss = 5.59194\n",
      "epoch no.25 train no.580  loss = 5.62149 avg_loss = 5.60575\n",
      "epoch no.25 train no.590  loss = 5.72343 avg_loss = 5.60236\n",
      "epoch no.25 train no.600  loss = 5.26403 avg_loss = 5.60705\n",
      "epoch no.25 train no.610  loss = 4.45745 avg_loss = 5.60555\n",
      "epoch no.25 train no.620  loss = 6.13745 avg_loss = 5.61877\n",
      "epoch no.25 train no.630  loss = 6.53309 avg_loss = 5.65753\n",
      "epoch no.25 train no.640  loss = 4.59588 avg_loss = 5.68506\n",
      "epoch no.25 train no.650  loss = 6.88035 avg_loss = 5.71017\n",
      "epoch no.25 train no.660  loss = 4.29831 avg_loss = 5.67744\n",
      "epoch no.25 train no.670  loss = 5.85474 avg_loss = 5.68360\n",
      "epoch no.25 train no.680  loss = 4.93748 avg_loss = 5.64501\n",
      "epoch no.25 train no.690  loss = 6.57428 avg_loss = 5.62516\n",
      "epoch no.25 train no.700  loss = 4.49142 avg_loss = 5.62182\n",
      "epoch no.25 train no.710  loss = 5.73976 avg_loss = 5.62695\n",
      "epoch no.25 train no.720  loss = 5.40565 avg_loss = 5.59543\n",
      "epoch no.25 train no.730  loss = 5.67536 avg_loss = 5.63465\n",
      "epoch no.25 train no.740  loss = 5.88536 avg_loss = 5.61317\n",
      "epoch no.25 train no.750  loss = 4.48956 avg_loss = 5.61282\n",
      "epoch no.25 train no.760  loss = 6.07542 avg_loss = 5.58998\n",
      "epoch no.25 train no.770  loss = 4.06566 avg_loss = 5.61592\n",
      "epoch no.25 train no.780  loss = 5.19632 avg_loss = 5.58978\n",
      "epoch no.25 train no.790  loss = 5.80450 avg_loss = 5.56248\n",
      "epoch no.25 train no.800  loss = 4.87987 avg_loss = 5.58100\n",
      "epoch no.25 train no.810  loss = 4.90417 avg_loss = 5.57330\n",
      "epoch no.25 train no.820  loss = 6.12019 avg_loss = 5.60963\n",
      "epoch no.25 train no.830  loss = 6.96474 avg_loss = 5.63105\n",
      "epoch no.25 train no.840  loss = 5.69464 avg_loss = 5.65083\n",
      "epoch no.25 train no.850  loss = 6.08439 avg_loss = 5.68631\n",
      "epoch no.25 train no.860  loss = 5.03115 avg_loss = 5.70299\n",
      "epoch no.25 train no.870  loss = 6.19664 avg_loss = 5.71393\n",
      "epoch no.25 train no.880  loss = 5.23973 avg_loss = 5.73938\n",
      "epoch no.25 train no.890  loss = 5.40432 avg_loss = 5.70962\n",
      "epoch no.25 train no.900  loss = 5.80553 avg_loss = 5.70943\n",
      "epoch no.25 train no.910  loss = 6.33690 avg_loss = 5.67225\n",
      "epoch no.25 train no.920  loss = 5.62923 avg_loss = 5.67975\n",
      "epoch no.25 train no.930  loss = 5.61317 avg_loss = 5.66028\n",
      "epoch no.25 train no.940  loss = 6.86689 avg_loss = 5.64679\n",
      "epoch no.25 train no.950  loss = 6.00230 avg_loss = 5.67271\n",
      "epoch no.25 train no.960  loss = 6.38285 avg_loss = 5.70983\n",
      "epoch no.25 train no.970  loss = 3.98433 avg_loss = 5.68923\n",
      "epoch no.25 train no.980  loss = 6.07161 avg_loss = 5.72205\n",
      "epoch no.25 train no.990  loss = 5.72216 avg_loss = 5.73254\n",
      "epoch no.25 train no.1000  loss = 5.20854 avg_loss = 5.72504\n",
      "to_tokens: ['</s>', '</s>', '</s>']\n",
      "뭐함</s>\n",
      "epoch no.25 train no.1010  loss = 7.52180 avg_loss = 5.71509\n",
      "epoch no.25 train no.1020  loss = 7.40158 avg_loss = 5.77017\n",
      "epoch no.25 train no.1030  loss = 5.63715 avg_loss = 5.73893\n",
      "epoch no.25 train no.1040  loss = 5.79059 avg_loss = 5.73103\n",
      "epoch no.25 train no.1050  loss = 3.85374 avg_loss = 5.66064\n",
      "epoch no.25 train no.1060  loss = 3.54112 avg_loss = 5.63450\n",
      "epoch no.25 train no.1070  loss = 6.49165 avg_loss = 5.68385\n",
      "epoch no.25 train no.1080  loss = 5.87810 avg_loss = 5.70036\n",
      "epoch no.25 train no.1090  loss = 4.15595 avg_loss = 5.66149\n",
      "epoch no.25 train no.1100  loss = 5.39897 avg_loss = 5.69955\n",
      "epoch no.25 train no.1110  loss = 4.55549 avg_loss = 5.68415\n",
      "epoch no.25 train no.1120  loss = 5.88517 avg_loss = 5.68912\n",
      "epoch no.25 train no.1130  loss = 6.75362 avg_loss = 5.64456\n",
      "epoch no.25 train no.1140  loss = 4.53382 avg_loss = 5.62622\n",
      "epoch no.25 train no.1150  loss = 5.63270 avg_loss = 5.59798\n",
      "epoch no.25 train no.1160  loss = 6.34830 avg_loss = 5.62149\n",
      "epoch no.25 train no.1170  loss = 4.13897 avg_loss = 5.56867\n",
      "epoch no.25 train no.1180  loss = 6.66749 avg_loss = 5.60205\n",
      "epoch no.25 train no.1190  loss = 6.17303 avg_loss = 5.55265\n",
      "epoch no.25 train no.1200  loss = 5.30455 avg_loss = 5.55356\n",
      "epoch no.25 train no.1210  loss = 6.32255 avg_loss = 5.57589\n",
      "epoch no.25 train no.1220  loss = 5.65831 avg_loss = 5.55775\n",
      "epoch no.25 train no.1230  loss = 4.99478 avg_loss = 5.49300\n",
      "epoch no.25 train no.1240  loss = 5.09606 avg_loss = 5.53195\n",
      "epoch no.25 train no.1250  loss = 5.40967 avg_loss = 5.53455\n",
      "epoch no.25 train no.1260  loss = 5.24389 avg_loss = 5.54622\n",
      "epoch no.25 train no.1270  loss = 5.58661 avg_loss = 5.54859\n",
      "epoch no.25 train no.1280  loss = 6.64123 avg_loss = 5.59476\n",
      "epoch no.25 train no.1290  loss = 4.93077 avg_loss = 5.62752\n",
      "epoch no.25 train no.1300  loss = 5.27647 avg_loss = 5.65044\n",
      "epoch no.25 train no.1310  loss = 5.74495 avg_loss = 5.66891\n",
      "epoch no.25 train no.1320  loss = 4.70924 avg_loss = 5.66730\n",
      "epoch no.25 train no.1330  loss = 5.82602 avg_loss = 5.68243\n",
      "epoch no.25 train no.1340  loss = 4.53427 avg_loss = 5.68560\n",
      "epoch no.25 train no.1350  loss = 5.87323 avg_loss = 5.68988\n",
      "epoch no.25 train no.1360  loss = 5.83451 avg_loss = 5.66364\n",
      "epoch no.25 train no.1370  loss = 6.28603 avg_loss = 5.63954\n",
      "epoch no.25 train no.1380  loss = 4.62065 avg_loss = 5.63743\n",
      "epoch no.25 train no.1390  loss = 4.58642 avg_loss = 5.65096\n",
      "epoch no.25 train no.1400  loss = 5.59547 avg_loss = 5.63864\n",
      "epoch no.25 train no.1410  loss = 5.26238 avg_loss = 5.62303\n",
      "epoch no.25 train no.1420  loss = 4.89616 avg_loss = 5.64170\n",
      "epoch no.25 train no.1430  loss = 5.17817 avg_loss = 5.61882\n",
      "epoch no.25 train no.1440  loss = 3.80391 avg_loss = 5.62135\n",
      "epoch no.25 train no.1450  loss = 5.40355 avg_loss = 5.61377\n",
      "epoch no.25 train no.1460  loss = 5.78394 avg_loss = 5.60878\n",
      "epoch no.25 train no.1470  loss = 5.05834 avg_loss = 5.60091\n",
      "epoch no.25 train no.1480  loss = 4.58881 avg_loss = 5.60662\n",
      "epoch no.25 train no.1490  loss = 5.20551 avg_loss = 5.63856\n",
      "epoch no.25 train no.1500  loss = 7.51768 avg_loss = 5.62071\n",
      "epoch no.25 train no.1510  loss = 4.90786 avg_loss = 5.59777\n",
      "epoch no.25 train no.1520  loss = 6.55100 avg_loss = 5.59227\n",
      "epoch no.25 train no.1530  loss = 4.63583 avg_loss = 5.62677\n",
      "epoch no.25 train no.1540  loss = 6.63976 avg_loss = 5.62247\n",
      "epoch no.25 train no.1550  loss = 6.20970 avg_loss = 5.66330\n",
      "epoch no.25 train no.1560  loss = 6.44632 avg_loss = 5.66845\n",
      "epoch no.25 train no.1570  loss = 6.21323 avg_loss = 5.68153\n",
      "epoch no.25 train no.1580  loss = 4.75946 avg_loss = 5.61509\n",
      "epoch no.25 train no.1590  loss = 4.64491 avg_loss = 5.62770\n",
      "epoch no.25 train no.1600  loss = 5.54151 avg_loss = 5.63175\n",
      "epoch no.25 train no.1610  loss = 5.48085 avg_loss = 5.61209\n",
      "epoch no.25 train no.1620  loss = 6.08373 avg_loss = 5.63592\n",
      "epoch no.25 train no.1630  loss = 5.80330 avg_loss = 5.64363\n",
      "epoch no.25 train no.1640  loss = 4.44213 avg_loss = 5.58849\n",
      "epoch no.25 train no.1650  loss = 5.89231 avg_loss = 5.55917\n",
      "epoch no.25 train no.1660  loss = 6.49591 avg_loss = 5.60005\n",
      "epoch no.25 train no.1670  loss = 5.22213 avg_loss = 5.56226\n",
      "epoch no.25 train no.1680  loss = 6.22390 avg_loss = 5.55375\n",
      "epoch no.25 train no.1690  loss = 4.87502 avg_loss = 5.54367\n",
      "epoch no.25 train no.1700  loss = 5.61269 avg_loss = 5.51439\n",
      "epoch no.25 train no.1710  loss = 5.43836 avg_loss = 5.53712\n",
      "epoch no.25 train no.1720  loss = 6.98217 avg_loss = 5.53340\n",
      "epoch no.25 train no.1730  loss = 5.68584 avg_loss = 5.56417\n",
      "epoch no.25 train no.1740  loss = 5.01022 avg_loss = 5.52088\n",
      "epoch no.25 train no.1750  loss = 4.55005 avg_loss = 5.52338\n",
      "epoch no.26 train no.0  loss = 4.75979 avg_loss = 5.54623\n",
      "epoch no.26 train no.10  loss = 6.95245 avg_loss = 5.58435\n",
      "epoch no.26 train no.20  loss = 5.09227 avg_loss = 5.57489\n",
      "epoch no.26 train no.30  loss = 5.62630 avg_loss = 5.57704\n",
      "epoch no.26 train no.40  loss = 7.02555 avg_loss = 5.58827\n",
      "epoch no.26 train no.50  loss = 4.97358 avg_loss = 5.58743\n",
      "epoch no.26 train no.60  loss = 5.80073 avg_loss = 5.60824\n",
      "epoch no.26 train no.70  loss = 4.86972 avg_loss = 5.64155\n",
      "epoch no.26 train no.80  loss = 5.19662 avg_loss = 5.63005\n",
      "epoch no.26 train no.90  loss = 6.14255 avg_loss = 5.60579\n",
      "epoch no.26 train no.100  loss = 5.66610 avg_loss = 5.57722\n",
      "epoch no.26 train no.110  loss = 6.42798 avg_loss = 5.55507\n",
      "epoch no.26 train no.120  loss = 5.81520 avg_loss = 5.56652\n",
      "epoch no.26 train no.130  loss = 5.55847 avg_loss = 5.60116\n",
      "epoch no.26 train no.140  loss = 5.73738 avg_loss = 5.64150\n",
      "epoch no.26 train no.150  loss = 2.37541 avg_loss = 5.55585\n",
      "epoch no.26 train no.160  loss = 6.50318 avg_loss = 5.54134\n",
      "epoch no.26 train no.170  loss = 4.61286 avg_loss = 5.56600\n",
      "epoch no.26 train no.180  loss = 5.58323 avg_loss = 5.52033\n",
      "epoch no.26 train no.190  loss = 5.58253 avg_loss = 5.58417\n",
      "epoch no.26 train no.200  loss = 6.70362 avg_loss = 5.62245\n",
      "epoch no.26 train no.210  loss = 4.51552 avg_loss = 5.61537\n",
      "epoch no.26 train no.220  loss = 5.80872 avg_loss = 5.64843\n",
      "epoch no.26 train no.230  loss = 5.41526 avg_loss = 5.63604\n",
      "epoch no.26 train no.240  loss = 6.90222 avg_loss = 5.64163\n",
      "epoch no.26 train no.250  loss = 6.18638 avg_loss = 5.67537\n",
      "epoch no.26 train no.260  loss = 6.51312 avg_loss = 5.65635\n",
      "epoch no.26 train no.270  loss = 5.92591 avg_loss = 5.67320\n",
      "epoch no.26 train no.280  loss = 5.07283 avg_loss = 5.64732\n",
      "epoch no.26 train no.290  loss = 5.62653 avg_loss = 5.62668\n",
      "epoch no.26 train no.300  loss = 4.06202 avg_loss = 5.59971\n",
      "epoch no.26 train no.310  loss = 5.30978 avg_loss = 5.61167\n",
      "epoch no.26 train no.320  loss = 6.86064 avg_loss = 5.60865\n",
      "epoch no.26 train no.330  loss = 5.84005 avg_loss = 5.62801\n",
      "epoch no.26 train no.340  loss = 6.56225 avg_loss = 5.66628\n",
      "epoch no.26 train no.350  loss = 3.68635 avg_loss = 5.61973\n",
      "epoch no.26 train no.360  loss = 6.12021 avg_loss = 5.61666\n",
      "epoch no.26 train no.370  loss = 6.17537 avg_loss = 5.65225\n",
      "epoch no.26 train no.380  loss = 5.22438 avg_loss = 5.67179\n",
      "epoch no.26 train no.390  loss = 2.96350 avg_loss = 5.65131\n",
      "epoch no.26 train no.400  loss = 6.34055 avg_loss = 5.65610\n",
      "epoch no.26 train no.410  loss = 5.41783 avg_loss = 5.64636\n",
      "epoch no.26 train no.420  loss = 5.41183 avg_loss = 5.62138\n",
      "epoch no.26 train no.430  loss = 4.89455 avg_loss = 5.65715\n",
      "epoch no.26 train no.440  loss = 5.93536 avg_loss = 5.67960\n",
      "epoch no.26 train no.450  loss = 5.99068 avg_loss = 5.67848\n",
      "epoch no.26 train no.460  loss = 5.69691 avg_loss = 5.71244\n",
      "epoch no.26 train no.470  loss = 5.72785 avg_loss = 5.69799\n",
      "epoch no.26 train no.480  loss = 5.01606 avg_loss = 5.62204\n",
      "epoch no.26 train no.490  loss = 5.30376 avg_loss = 5.61745\n",
      "epoch no.26 train no.500  loss = 5.30691 avg_loss = 5.64489\n",
      "epoch no.26 train no.510  loss = 5.43537 avg_loss = 5.58208\n",
      "epoch no.26 train no.520  loss = 5.01815 avg_loss = 5.59256\n",
      "epoch no.26 train no.530  loss = 5.96293 avg_loss = 5.60548\n",
      "epoch no.26 train no.540  loss = 6.90421 avg_loss = 5.61328\n",
      "epoch no.26 train no.550  loss = 5.11281 avg_loss = 5.64118\n",
      "epoch no.26 train no.560  loss = 6.12797 avg_loss = 5.60576\n",
      "epoch no.26 train no.570  loss = 4.38807 avg_loss = 5.58536\n",
      "epoch no.26 train no.580  loss = 5.15688 avg_loss = 5.61521\n",
      "epoch no.26 train no.590  loss = 4.74884 avg_loss = 5.58929\n",
      "epoch no.26 train no.600  loss = 5.32073 avg_loss = 5.59041\n",
      "epoch no.26 train no.610  loss = 2.68422 avg_loss = 5.59852\n",
      "epoch no.26 train no.620  loss = 5.93598 avg_loss = 5.54169\n",
      "epoch no.26 train no.630  loss = 6.05112 avg_loss = 5.53685\n",
      "epoch no.26 train no.640  loss = 5.37793 avg_loss = 5.53434\n",
      "epoch no.26 train no.650  loss = 7.48486 avg_loss = 5.57728\n",
      "epoch no.26 train no.660  loss = 5.86212 avg_loss = 5.51866\n",
      "epoch no.26 train no.670  loss = 4.59394 avg_loss = 5.53821\n",
      "epoch no.26 train no.680  loss = 5.46081 avg_loss = 5.56794\n",
      "epoch no.26 train no.690  loss = 5.29763 avg_loss = 5.58561\n",
      "epoch no.26 train no.700  loss = 3.77261 avg_loss = 5.55762\n",
      "epoch no.26 train no.710  loss = 6.13597 avg_loss = 5.56738\n",
      "epoch no.26 train no.720  loss = 4.06575 avg_loss = 5.54705\n",
      "epoch no.26 train no.730  loss = 6.04918 avg_loss = 5.51068\n",
      "epoch no.26 train no.740  loss = 6.35376 avg_loss = 5.52774\n",
      "epoch no.26 train no.750  loss = 4.85086 avg_loss = 5.52190\n",
      "epoch no.26 train no.760  loss = 5.04446 avg_loss = 5.54939\n",
      "epoch no.26 train no.770  loss = 6.53617 avg_loss = 5.57871\n",
      "epoch no.26 train no.780  loss = 6.58134 avg_loss = 5.62309\n",
      "epoch no.26 train no.790  loss = 6.83009 avg_loss = 5.64114\n",
      "epoch no.26 train no.800  loss = 6.44078 avg_loss = 5.59714\n",
      "epoch no.26 train no.810  loss = 3.81032 avg_loss = 5.59254\n",
      "epoch no.26 train no.820  loss = 5.96085 avg_loss = 5.60255\n",
      "epoch no.26 train no.830  loss = 5.92265 avg_loss = 5.62544\n",
      "epoch no.26 train no.840  loss = 6.00885 avg_loss = 5.61251\n",
      "epoch no.26 train no.850  loss = 4.74499 avg_loss = 5.60697\n",
      "epoch no.26 train no.860  loss = 5.25286 avg_loss = 5.58772\n",
      "epoch no.26 train no.870  loss = 6.89347 avg_loss = 5.56787\n",
      "epoch no.26 train no.880  loss = 4.49635 avg_loss = 5.59043\n",
      "epoch no.26 train no.890  loss = 5.08318 avg_loss = 5.64689\n",
      "epoch no.26 train no.900  loss = 6.21431 avg_loss = 5.67423\n",
      "epoch no.26 train no.910  loss = 5.28903 avg_loss = 5.66312\n",
      "epoch no.26 train no.920  loss = 5.42180 avg_loss = 5.65363\n",
      "epoch no.26 train no.930  loss = 4.83602 avg_loss = 5.62864\n",
      "epoch no.26 train no.940  loss = 5.92650 avg_loss = 5.62139\n",
      "epoch no.26 train no.950  loss = 2.99645 avg_loss = 5.62829\n",
      "epoch no.26 train no.960  loss = 6.00082 avg_loss = 5.63251\n",
      "epoch no.26 train no.970  loss = 5.08004 avg_loss = 5.60713\n",
      "epoch no.26 train no.980  loss = 5.62486 avg_loss = 5.62190\n",
      "epoch no.26 train no.990  loss = 6.11722 avg_loss = 5.62158\n",
      "epoch no.26 train no.1000  loss = 4.50288 avg_loss = 5.60133\n",
      "to_tokens: ['</s>', '</s>', '</s>']\n",
      "뭐함</s>\n",
      "epoch no.26 train no.1010  loss = 5.57689 avg_loss = 5.56715\n",
      "epoch no.26 train no.1020  loss = 6.93126 avg_loss = 5.59011\n",
      "epoch no.26 train no.1030  loss = 5.44399 avg_loss = 5.52467\n",
      "epoch no.26 train no.1040  loss = 5.96130 avg_loss = 5.53344\n",
      "epoch no.26 train no.1050  loss = 5.71906 avg_loss = 5.55132\n",
      "epoch no.26 train no.1060  loss = 4.30373 avg_loss = 5.51793\n",
      "epoch no.26 train no.1070  loss = 4.80056 avg_loss = 5.50942\n",
      "epoch no.26 train no.1080  loss = 7.17778 avg_loss = 5.49159\n",
      "epoch no.26 train no.1090  loss = 4.21316 avg_loss = 5.51048\n",
      "epoch no.26 train no.1100  loss = 5.84323 avg_loss = 5.55406\n",
      "epoch no.26 train no.1110  loss = 3.51148 avg_loss = 5.51324\n",
      "epoch no.26 train no.1120  loss = 6.57146 avg_loss = 5.55940\n",
      "epoch no.26 train no.1130  loss = 5.61738 avg_loss = 5.57914\n",
      "epoch no.26 train no.1140  loss = 5.89924 avg_loss = 5.59329\n",
      "epoch no.26 train no.1150  loss = 4.14195 avg_loss = 5.57644\n",
      "epoch no.26 train no.1160  loss = 3.32094 avg_loss = 5.53661\n",
      "epoch no.26 train no.1170  loss = 6.98688 avg_loss = 5.58355\n",
      "epoch no.26 train no.1180  loss = 5.37643 avg_loss = 5.57080\n",
      "epoch no.26 train no.1190  loss = 5.55262 avg_loss = 5.55097\n",
      "epoch no.26 train no.1200  loss = 5.92396 avg_loss = 5.58064\n",
      "epoch no.26 train no.1210  loss = 6.64427 avg_loss = 5.61070\n",
      "epoch no.26 train no.1220  loss = 5.58379 avg_loss = 5.61268\n",
      "epoch no.26 train no.1230  loss = 4.41089 avg_loss = 5.61834\n",
      "epoch no.26 train no.1240  loss = 5.45377 avg_loss = 5.62197\n",
      "epoch no.26 train no.1250  loss = 4.86546 avg_loss = 5.57876\n",
      "epoch no.26 train no.1260  loss = 5.27315 avg_loss = 5.60685\n",
      "epoch no.26 train no.1270  loss = 6.90222 avg_loss = 5.66290\n",
      "epoch no.26 train no.1280  loss = 4.50584 avg_loss = 5.65044\n",
      "epoch no.26 train no.1290  loss = 5.60799 avg_loss = 5.69826\n",
      "epoch no.26 train no.1300  loss = 6.56205 avg_loss = 5.71282\n",
      "epoch no.26 train no.1310  loss = 6.85436 avg_loss = 5.73603\n",
      "epoch no.26 train no.1320  loss = 5.91420 avg_loss = 5.75611\n",
      "epoch no.26 train no.1330  loss = 5.59999 avg_loss = 5.77035\n",
      "epoch no.26 train no.1340  loss = 3.77092 avg_loss = 5.75369\n",
      "epoch no.26 train no.1350  loss = 5.33363 avg_loss = 5.72473\n",
      "epoch no.26 train no.1360  loss = 5.33472 avg_loss = 5.73571\n",
      "epoch no.26 train no.1370  loss = 2.89067 avg_loss = 5.69061\n",
      "epoch no.26 train no.1380  loss = 6.41208 avg_loss = 5.65932\n",
      "epoch no.26 train no.1390  loss = 6.16636 avg_loss = 5.58132\n",
      "epoch no.26 train no.1400  loss = 5.18649 avg_loss = 5.54567\n",
      "epoch no.26 train no.1410  loss = 5.80454 avg_loss = 5.53888\n",
      "epoch no.26 train no.1420  loss = 4.07020 avg_loss = 5.57159\n",
      "epoch no.26 train no.1430  loss = 7.43261 avg_loss = 5.58909\n",
      "epoch no.26 train no.1440  loss = 5.46563 avg_loss = 5.54413\n",
      "epoch no.26 train no.1450  loss = 6.39722 avg_loss = 5.59505\n",
      "epoch no.26 train no.1460  loss = 5.39012 avg_loss = 5.59781\n",
      "epoch no.26 train no.1470  loss = 3.90835 avg_loss = 5.58414\n",
      "epoch no.26 train no.1480  loss = 4.89894 avg_loss = 5.58325\n",
      "epoch no.26 train no.1490  loss = 6.46561 avg_loss = 5.63006\n",
      "epoch no.26 train no.1500  loss = 6.62308 avg_loss = 5.62812\n",
      "epoch no.26 train no.1510  loss = 6.26521 avg_loss = 5.64978\n",
      "epoch no.26 train no.1520  loss = 6.58720 avg_loss = 5.63216\n",
      "epoch no.26 train no.1530  loss = 6.15374 avg_loss = 5.67759\n",
      "epoch no.26 train no.1540  loss = 6.60493 avg_loss = 5.70769\n",
      "epoch no.26 train no.1550  loss = 4.47451 avg_loss = 5.69729\n",
      "epoch no.26 train no.1560  loss = 4.34989 avg_loss = 5.68069\n",
      "epoch no.26 train no.1570  loss = 4.84983 avg_loss = 5.71028\n",
      "epoch no.26 train no.1580  loss = 6.28087 avg_loss = 5.74292\n",
      "epoch no.26 train no.1590  loss = 5.65281 avg_loss = 5.72716\n",
      "epoch no.26 train no.1600  loss = 6.37907 avg_loss = 5.69793\n",
      "epoch no.26 train no.1610  loss = 4.29827 avg_loss = 5.69177\n",
      "epoch no.26 train no.1620  loss = 5.72510 avg_loss = 5.71994\n",
      "epoch no.26 train no.1630  loss = 3.90444 avg_loss = 5.69232\n",
      "epoch no.26 train no.1640  loss = 5.02844 avg_loss = 5.72299\n",
      "epoch no.26 train no.1650  loss = 6.94889 avg_loss = 5.73794\n",
      "epoch no.26 train no.1660  loss = 5.04177 avg_loss = 5.71066\n",
      "epoch no.26 train no.1670  loss = 4.90916 avg_loss = 5.69953\n",
      "epoch no.26 train no.1680  loss = 5.97848 avg_loss = 5.70742\n",
      "epoch no.26 train no.1690  loss = 6.33215 avg_loss = 5.70027\n",
      "epoch no.26 train no.1700  loss = 5.44106 avg_loss = 5.62271\n",
      "epoch no.26 train no.1710  loss = 4.70326 avg_loss = 5.59268\n",
      "epoch no.26 train no.1720  loss = 5.29917 avg_loss = 5.60381\n",
      "epoch no.26 train no.1730  loss = 4.70206 avg_loss = 5.58411\n",
      "epoch no.26 train no.1740  loss = 4.40533 avg_loss = 5.53687\n",
      "epoch no.26 train no.1750  loss = 6.05413 avg_loss = 5.54467\n",
      "epoch no.27 train no.0  loss = 4.92348 avg_loss = 5.55233\n",
      "epoch no.27 train no.10  loss = 5.49580 avg_loss = 5.55357\n",
      "epoch no.27 train no.20  loss = 4.39149 avg_loss = 5.55108\n",
      "epoch no.27 train no.30  loss = 4.40311 avg_loss = 5.51250\n",
      "epoch no.27 train no.40  loss = 5.44813 avg_loss = 5.51928\n",
      "epoch no.27 train no.50  loss = 6.03662 avg_loss = 5.49036\n",
      "epoch no.27 train no.60  loss = 5.56369 avg_loss = 5.54784\n",
      "epoch no.27 train no.70  loss = 5.24661 avg_loss = 5.54949\n",
      "epoch no.27 train no.80  loss = 3.66883 avg_loss = 5.52641\n",
      "epoch no.27 train no.90  loss = 7.57459 avg_loss = 5.55627\n",
      "epoch no.27 train no.100  loss = 6.58146 avg_loss = 5.54941\n",
      "epoch no.27 train no.110  loss = 7.00071 avg_loss = 5.60798\n",
      "epoch no.27 train no.120  loss = 6.34938 avg_loss = 5.64643\n",
      "epoch no.27 train no.130  loss = 4.13868 avg_loss = 5.63746\n",
      "epoch no.27 train no.140  loss = 5.95760 avg_loss = 5.65577\n",
      "epoch no.27 train no.150  loss = 6.50093 avg_loss = 5.71494\n",
      "epoch no.27 train no.160  loss = 6.82918 avg_loss = 5.66779\n",
      "epoch no.27 train no.170  loss = 6.63890 avg_loss = 5.70384\n",
      "epoch no.27 train no.180  loss = 5.33757 avg_loss = 5.72120\n",
      "epoch no.27 train no.190  loss = 5.54505 avg_loss = 5.70522\n",
      "epoch no.27 train no.200  loss = 6.96669 avg_loss = 5.70466\n",
      "epoch no.27 train no.210  loss = 6.54508 avg_loss = 5.66712\n",
      "epoch no.27 train no.220  loss = 6.22655 avg_loss = 5.64894\n",
      "epoch no.27 train no.230  loss = 5.08785 avg_loss = 5.65458\n",
      "epoch no.27 train no.240  loss = 5.20717 avg_loss = 5.59785\n",
      "epoch no.27 train no.250  loss = 5.47033 avg_loss = 5.59191\n",
      "epoch no.27 train no.260  loss = 6.14107 avg_loss = 5.59446\n",
      "epoch no.27 train no.270  loss = 6.54785 avg_loss = 5.59449\n",
      "epoch no.27 train no.280  loss = 5.74600 avg_loss = 5.60591\n",
      "epoch no.27 train no.290  loss = 5.26938 avg_loss = 5.63627\n",
      "epoch no.27 train no.300  loss = 6.71103 avg_loss = 5.64560\n",
      "epoch no.27 train no.310  loss = 6.22787 avg_loss = 5.66318\n",
      "epoch no.27 train no.320  loss = 5.65762 avg_loss = 5.62490\n",
      "epoch no.27 train no.330  loss = 4.43476 avg_loss = 5.58911\n",
      "epoch no.27 train no.340  loss = 5.86043 avg_loss = 5.61171\n",
      "epoch no.27 train no.350  loss = 4.95149 avg_loss = 5.62389\n",
      "epoch no.27 train no.360  loss = 5.28321 avg_loss = 5.60108\n",
      "epoch no.27 train no.370  loss = 4.77421 avg_loss = 5.60915\n",
      "epoch no.27 train no.380  loss = 6.44712 avg_loss = 5.61742\n",
      "epoch no.27 train no.390  loss = 5.48106 avg_loss = 5.62616\n",
      "epoch no.27 train no.400  loss = 6.98485 avg_loss = 5.62069\n",
      "epoch no.27 train no.410  loss = 6.97996 avg_loss = 5.64452\n",
      "epoch no.27 train no.420  loss = 5.30557 avg_loss = 5.60257\n",
      "epoch no.27 train no.430  loss = 5.62500 avg_loss = 5.62989\n",
      "epoch no.27 train no.440  loss = 6.11379 avg_loss = 5.63357\n",
      "epoch no.27 train no.450  loss = 5.44548 avg_loss = 5.58358\n",
      "epoch no.27 train no.460  loss = 5.71559 avg_loss = 5.55728\n",
      "epoch no.27 train no.470  loss = 5.13634 avg_loss = 5.55025\n",
      "epoch no.27 train no.480  loss = 5.53119 avg_loss = 5.52833\n",
      "epoch no.27 train no.490  loss = 5.54689 avg_loss = 5.53129\n",
      "epoch no.27 train no.500  loss = 5.13818 avg_loss = 5.47043\n",
      "epoch no.27 train no.510  loss = 5.09889 avg_loss = 5.46929\n",
      "epoch no.27 train no.520  loss = 6.68133 avg_loss = 5.50526\n",
      "epoch no.27 train no.530  loss = 5.76658 avg_loss = 5.49732\n",
      "epoch no.27 train no.540  loss = 6.09166 avg_loss = 5.53600\n",
      "epoch no.27 train no.550  loss = 6.54195 avg_loss = 5.51825\n",
      "epoch no.27 train no.560  loss = 6.51075 avg_loss = 5.57812\n",
      "epoch no.27 train no.570  loss = 6.45641 avg_loss = 5.55209\n",
      "epoch no.27 train no.580  loss = 5.28535 avg_loss = 5.53638\n",
      "epoch no.27 train no.590  loss = 5.46872 avg_loss = 5.54601\n",
      "epoch no.27 train no.600  loss = 5.95716 avg_loss = 5.51631\n",
      "epoch no.27 train no.610  loss = 5.69614 avg_loss = 5.56403\n",
      "epoch no.27 train no.620  loss = 4.01718 avg_loss = 5.53260\n",
      "epoch no.27 train no.630  loss = 5.97032 avg_loss = 5.57186\n",
      "epoch no.27 train no.640  loss = 5.88558 avg_loss = 5.56997\n",
      "epoch no.27 train no.650  loss = 6.82919 avg_loss = 5.59052\n",
      "epoch no.27 train no.660  loss = 4.13335 avg_loss = 5.55424\n",
      "epoch no.27 train no.670  loss = 4.60317 avg_loss = 5.54602\n",
      "epoch no.27 train no.680  loss = 7.15706 avg_loss = 5.59910\n",
      "epoch no.27 train no.690  loss = 4.51761 avg_loss = 5.62895\n",
      "epoch no.27 train no.700  loss = 7.07082 avg_loss = 5.64146\n",
      "epoch no.27 train no.710  loss = 5.94774 avg_loss = 5.65006\n",
      "epoch no.27 train no.720  loss = 6.94905 avg_loss = 5.66848\n",
      "epoch no.27 train no.730  loss = 5.99881 avg_loss = 5.70066\n",
      "epoch no.27 train no.740  loss = 5.57846 avg_loss = 5.69609\n",
      "epoch no.27 train no.750  loss = 4.96858 avg_loss = 5.70795\n",
      "epoch no.27 train no.760  loss = 6.24094 avg_loss = 5.71210\n",
      "epoch no.27 train no.770  loss = 5.12108 avg_loss = 5.66959\n",
      "epoch no.27 train no.780  loss = 4.67417 avg_loss = 5.66170\n",
      "epoch no.27 train no.790  loss = 4.53051 avg_loss = 5.62670\n",
      "epoch no.27 train no.800  loss = 4.66879 avg_loss = 5.64213\n",
      "epoch no.27 train no.810  loss = 5.87368 avg_loss = 5.59673\n",
      "epoch no.27 train no.820  loss = 6.49828 avg_loss = 5.61829\n",
      "epoch no.27 train no.830  loss = 4.59381 avg_loss = 5.57563\n",
      "epoch no.27 train no.840  loss = 6.21164 avg_loss = 5.59791\n",
      "epoch no.27 train no.850  loss = 4.68870 avg_loss = 5.57572\n",
      "epoch no.27 train no.860  loss = 3.58437 avg_loss = 5.56109\n",
      "epoch no.27 train no.870  loss = 3.78097 avg_loss = 5.52929\n",
      "epoch no.27 train no.880  loss = 5.06589 avg_loss = 5.52264\n",
      "epoch no.27 train no.890  loss = 6.05023 avg_loss = 5.53682\n",
      "epoch no.27 train no.900  loss = 5.91297 avg_loss = 5.54598\n",
      "epoch no.27 train no.910  loss = 5.09205 avg_loss = 5.55321\n",
      "epoch no.27 train no.920  loss = 7.04643 avg_loss = 5.55737\n",
      "epoch no.27 train no.930  loss = 5.37775 avg_loss = 5.55833\n",
      "epoch no.27 train no.940  loss = 6.37599 avg_loss = 5.58574\n",
      "epoch no.27 train no.950  loss = 3.41834 avg_loss = 5.55172\n",
      "epoch no.27 train no.960  loss = 5.77962 avg_loss = 5.54943\n",
      "epoch no.27 train no.970  loss = 5.71912 avg_loss = 5.55610\n",
      "epoch no.27 train no.980  loss = 4.86143 avg_loss = 5.54052\n",
      "epoch no.27 train no.990  loss = 6.40144 avg_loss = 5.61061\n",
      "epoch no.27 train no.1000  loss = 5.21753 avg_loss = 5.58901\n",
      "to_tokens: ['</s>', '</s>', '</s>']\n",
      "뭐함</s>\n",
      "epoch no.27 train no.1010  loss = 6.05420 avg_loss = 5.58417\n",
      "epoch no.27 train no.1020  loss = 5.13838 avg_loss = 5.60558\n",
      "epoch no.27 train no.1030  loss = 6.72510 avg_loss = 5.62378\n",
      "epoch no.27 train no.1040  loss = 5.59808 avg_loss = 5.61132\n",
      "epoch no.27 train no.1050  loss = 5.92304 avg_loss = 5.60719\n",
      "epoch no.27 train no.1060  loss = 4.87949 avg_loss = 5.57820\n",
      "epoch no.27 train no.1070  loss = 5.16250 avg_loss = 5.57718\n",
      "epoch no.27 train no.1080  loss = 6.93679 avg_loss = 5.55026\n",
      "epoch no.27 train no.1090  loss = 4.91648 avg_loss = 5.55031\n",
      "epoch no.27 train no.1100  loss = 5.83348 avg_loss = 5.59139\n",
      "epoch no.27 train no.1110  loss = 5.39113 avg_loss = 5.61962\n",
      "epoch no.27 train no.1120  loss = 6.20099 avg_loss = 5.61174\n",
      "epoch no.27 train no.1130  loss = 6.23507 avg_loss = 5.61661\n",
      "epoch no.27 train no.1140  loss = 5.00423 avg_loss = 5.61312\n",
      "epoch no.27 train no.1150  loss = 4.64056 avg_loss = 5.61299\n",
      "epoch no.27 train no.1160  loss = 6.37047 avg_loss = 5.60784\n",
      "epoch no.27 train no.1170  loss = 5.72241 avg_loss = 5.58739\n",
      "epoch no.27 train no.1180  loss = 5.90492 avg_loss = 5.59547\n",
      "epoch no.27 train no.1190  loss = 5.33871 avg_loss = 5.62485\n",
      "epoch no.27 train no.1200  loss = 5.93333 avg_loss = 5.60187\n",
      "epoch no.27 train no.1210  loss = 5.87575 avg_loss = 5.58904\n",
      "epoch no.27 train no.1220  loss = 7.06058 avg_loss = 5.58172\n",
      "epoch no.27 train no.1230  loss = 6.26205 avg_loss = 5.63224\n",
      "epoch no.27 train no.1240  loss = 7.17941 avg_loss = 5.62763\n",
      "epoch no.27 train no.1250  loss = 5.36157 avg_loss = 5.59568\n",
      "epoch no.27 train no.1260  loss = 6.24397 avg_loss = 5.60353\n",
      "epoch no.27 train no.1270  loss = 7.24622 avg_loss = 5.62438\n",
      "epoch no.27 train no.1280  loss = 6.31347 avg_loss = 5.62154\n",
      "epoch no.27 train no.1290  loss = 7.20040 avg_loss = 5.59790\n",
      "epoch no.27 train no.1300  loss = 5.19950 avg_loss = 5.62356\n",
      "epoch no.27 train no.1310  loss = 6.09608 avg_loss = 5.61220\n",
      "epoch no.27 train no.1320  loss = 5.25608 avg_loss = 5.59663\n",
      "epoch no.27 train no.1330  loss = 6.31421 avg_loss = 5.65101\n",
      "epoch no.27 train no.1340  loss = 4.59401 avg_loss = 5.67154\n",
      "epoch no.27 train no.1350  loss = 4.35124 avg_loss = 5.66725\n",
      "epoch no.27 train no.1360  loss = 5.92231 avg_loss = 5.68758\n",
      "epoch no.27 train no.1370  loss = 7.39434 avg_loss = 5.64309\n",
      "epoch no.27 train no.1380  loss = 6.66341 avg_loss = 5.61625\n",
      "epoch no.27 train no.1390  loss = 5.29239 avg_loss = 5.62322\n",
      "epoch no.27 train no.1400  loss = 6.59250 avg_loss = 5.60821\n",
      "epoch no.27 train no.1410  loss = 4.93314 avg_loss = 5.65019\n",
      "epoch no.27 train no.1420  loss = 4.01188 avg_loss = 5.60359\n",
      "epoch no.27 train no.1430  loss = 6.16602 avg_loss = 5.58452\n",
      "epoch no.27 train no.1440  loss = 6.50656 avg_loss = 5.60104\n",
      "epoch no.27 train no.1450  loss = 5.87448 avg_loss = 5.57350\n",
      "epoch no.27 train no.1460  loss = 4.18221 avg_loss = 5.55920\n",
      "epoch no.27 train no.1470  loss = 5.13840 avg_loss = 5.52207\n",
      "epoch no.27 train no.1480  loss = 6.30010 avg_loss = 5.49734\n",
      "epoch no.27 train no.1490  loss = 5.36181 avg_loss = 5.50157\n",
      "epoch no.27 train no.1500  loss = 4.69594 avg_loss = 5.51426\n",
      "epoch no.27 train no.1510  loss = 7.18241 avg_loss = 5.52143\n",
      "epoch no.27 train no.1520  loss = 5.60517 avg_loss = 5.56447\n",
      "epoch no.27 train no.1530  loss = 5.98569 avg_loss = 5.60911\n",
      "epoch no.27 train no.1540  loss = 3.18497 avg_loss = 5.58149\n",
      "epoch no.27 train no.1550  loss = 7.06163 avg_loss = 5.60454\n",
      "epoch no.27 train no.1560  loss = 6.55724 avg_loss = 5.63726\n",
      "epoch no.27 train no.1570  loss = 7.50991 avg_loss = 5.64573\n",
      "epoch no.27 train no.1580  loss = 5.54530 avg_loss = 5.64264\n",
      "epoch no.27 train no.1590  loss = 5.68131 avg_loss = 5.63354\n",
      "epoch no.27 train no.1600  loss = 5.87547 avg_loss = 5.65987\n",
      "epoch no.27 train no.1610  loss = 7.00665 avg_loss = 5.70494\n",
      "epoch no.27 train no.1620  loss = 5.45710 avg_loss = 5.66985\n",
      "epoch no.27 train no.1630  loss = 5.21281 avg_loss = 5.68498\n",
      "epoch no.27 train no.1640  loss = 6.43089 avg_loss = 5.63407\n",
      "epoch no.27 train no.1650  loss = 5.75860 avg_loss = 5.64219\n",
      "epoch no.27 train no.1660  loss = 4.96811 avg_loss = 5.62451\n",
      "epoch no.27 train no.1670  loss = 5.50363 avg_loss = 5.62756\n",
      "epoch no.27 train no.1680  loss = 5.99021 avg_loss = 5.60374\n",
      "epoch no.27 train no.1690  loss = 5.56893 avg_loss = 5.63455\n",
      "epoch no.27 train no.1700  loss = 6.33254 avg_loss = 5.66851\n",
      "epoch no.27 train no.1710  loss = 5.47398 avg_loss = 5.63549\n",
      "epoch no.27 train no.1720  loss = 5.19873 avg_loss = 5.65036\n",
      "epoch no.27 train no.1730  loss = 6.07933 avg_loss = 5.67623\n",
      "epoch no.27 train no.1740  loss = 4.65885 avg_loss = 5.67254\n",
      "epoch no.27 train no.1750  loss = 3.07581 avg_loss = 5.63417\n",
      "epoch no.28 train no.0  loss = 6.15433 avg_loss = 5.65684\n",
      "epoch no.28 train no.10  loss = 6.51400 avg_loss = 5.60738\n",
      "epoch no.28 train no.20  loss = 7.50418 avg_loss = 5.60262\n",
      "epoch no.28 train no.30  loss = 5.53974 avg_loss = 5.56115\n",
      "epoch no.28 train no.40  loss = 4.95405 avg_loss = 5.55738\n",
      "epoch no.28 train no.50  loss = 7.01547 avg_loss = 5.57136\n",
      "epoch no.28 train no.60  loss = 5.87703 avg_loss = 5.57162\n",
      "epoch no.28 train no.70  loss = 4.13085 avg_loss = 5.57702\n",
      "epoch no.28 train no.80  loss = 5.49793 avg_loss = 5.57433\n",
      "epoch no.28 train no.90  loss = 5.78684 avg_loss = 5.56722\n",
      "epoch no.28 train no.100  loss = 4.75505 avg_loss = 5.52100\n",
      "epoch no.28 train no.110  loss = 5.67354 avg_loss = 5.55434\n",
      "epoch no.28 train no.120  loss = 5.13276 avg_loss = 5.57764\n",
      "epoch no.28 train no.130  loss = 5.47103 avg_loss = 5.54225\n",
      "epoch no.28 train no.140  loss = 6.54193 avg_loss = 5.54863\n",
      "epoch no.28 train no.150  loss = 3.98019 avg_loss = 5.51744\n",
      "epoch no.28 train no.160  loss = 6.19962 avg_loss = 5.50632\n",
      "epoch no.28 train no.170  loss = 4.75665 avg_loss = 5.50936\n",
      "epoch no.28 train no.180  loss = 5.70328 avg_loss = 5.49344\n",
      "epoch no.28 train no.190  loss = 5.57308 avg_loss = 5.54884\n",
      "epoch no.28 train no.200  loss = 5.43779 avg_loss = 5.56147\n",
      "epoch no.28 train no.210  loss = 6.04937 avg_loss = 5.57801\n",
      "epoch no.28 train no.220  loss = 6.47994 avg_loss = 5.55587\n",
      "epoch no.28 train no.230  loss = 5.35989 avg_loss = 5.52636\n",
      "epoch no.28 train no.240  loss = 6.03729 avg_loss = 5.53491\n",
      "epoch no.28 train no.250  loss = 5.49139 avg_loss = 5.51459\n",
      "epoch no.28 train no.260  loss = 5.46973 avg_loss = 5.53520\n",
      "epoch no.28 train no.270  loss = 5.77366 avg_loss = 5.55051\n",
      "epoch no.28 train no.280  loss = 4.21147 avg_loss = 5.54370\n",
      "epoch no.28 train no.290  loss = 4.96121 avg_loss = 5.51757\n",
      "epoch no.28 train no.300  loss = 5.59209 avg_loss = 5.55826\n",
      "epoch no.28 train no.310  loss = 7.07833 avg_loss = 5.58223\n",
      "epoch no.28 train no.320  loss = 6.43447 avg_loss = 5.56197\n",
      "epoch no.28 train no.330  loss = 4.58828 avg_loss = 5.58340\n",
      "epoch no.28 train no.340  loss = 4.70741 avg_loss = 5.55615\n",
      "epoch no.28 train no.350  loss = 6.90123 avg_loss = 5.59160\n",
      "epoch no.28 train no.360  loss = 3.87762 avg_loss = 5.58029\n",
      "epoch no.28 train no.370  loss = 4.88663 avg_loss = 5.51651\n",
      "epoch no.28 train no.380  loss = 6.62248 avg_loss = 5.53273\n",
      "epoch no.28 train no.390  loss = 4.69907 avg_loss = 5.53827\n",
      "epoch no.28 train no.400  loss = 6.61041 avg_loss = 5.48705\n",
      "epoch no.28 train no.410  loss = 4.52764 avg_loss = 5.44702\n",
      "epoch no.28 train no.420  loss = 4.71635 avg_loss = 5.45106\n",
      "epoch no.28 train no.430  loss = 5.46808 avg_loss = 5.48268\n",
      "epoch no.28 train no.440  loss = 5.67336 avg_loss = 5.52153\n",
      "epoch no.28 train no.450  loss = 7.55077 avg_loss = 5.51644\n",
      "epoch no.28 train no.460  loss = 6.01762 avg_loss = 5.54337\n",
      "epoch no.28 train no.470  loss = 3.57520 avg_loss = 5.50836\n",
      "epoch no.28 train no.480  loss = 5.04655 avg_loss = 5.48988\n",
      "epoch no.28 train no.490  loss = 6.71151 avg_loss = 5.50219\n",
      "epoch no.28 train no.500  loss = 4.56754 avg_loss = 5.52180\n",
      "epoch no.28 train no.510  loss = 5.76642 avg_loss = 5.56298\n",
      "epoch no.28 train no.520  loss = 6.26056 avg_loss = 5.58591\n",
      "epoch no.28 train no.530  loss = 4.84223 avg_loss = 5.58964\n",
      "epoch no.28 train no.540  loss = 5.46633 avg_loss = 5.60448\n",
      "epoch no.28 train no.550  loss = 6.55994 avg_loss = 5.62617\n",
      "epoch no.28 train no.560  loss = 6.91971 avg_loss = 5.63698\n",
      "epoch no.28 train no.570  loss = 5.56672 avg_loss = 5.64608\n",
      "epoch no.28 train no.580  loss = 5.23360 avg_loss = 5.64384\n",
      "epoch no.28 train no.590  loss = 5.94016 avg_loss = 5.64987\n",
      "epoch no.28 train no.600  loss = 5.36189 avg_loss = 5.58670\n",
      "epoch no.28 train no.610  loss = 4.34317 avg_loss = 5.56593\n",
      "epoch no.28 train no.620  loss = 7.00719 avg_loss = 5.54250\n",
      "epoch no.28 train no.630  loss = 6.83866 avg_loss = 5.53222\n",
      "epoch no.28 train no.640  loss = 6.25690 avg_loss = 5.55144\n",
      "epoch no.28 train no.650  loss = 5.75384 avg_loss = 5.54792\n",
      "epoch no.28 train no.660  loss = 4.77217 avg_loss = 5.54828\n",
      "epoch no.28 train no.670  loss = 5.68353 avg_loss = 5.52076\n",
      "epoch no.28 train no.680  loss = 5.34332 avg_loss = 5.52674\n",
      "epoch no.28 train no.690  loss = 5.32812 avg_loss = 5.50593\n",
      "epoch no.28 train no.700  loss = 5.62240 avg_loss = 5.52272\n",
      "epoch no.28 train no.710  loss = 5.77351 avg_loss = 5.53436\n",
      "epoch no.28 train no.720  loss = 6.59369 avg_loss = 5.50855\n",
      "epoch no.28 train no.730  loss = 4.70807 avg_loss = 5.53652\n",
      "epoch no.28 train no.740  loss = 7.10258 avg_loss = 5.57038\n",
      "epoch no.28 train no.750  loss = 5.80904 avg_loss = 5.58189\n",
      "epoch no.28 train no.760  loss = 6.30663 avg_loss = 5.57111\n",
      "epoch no.28 train no.770  loss = 5.46617 avg_loss = 5.58045\n",
      "epoch no.28 train no.780  loss = 6.21847 avg_loss = 5.56838\n",
      "epoch no.28 train no.790  loss = 6.46404 avg_loss = 5.56632\n",
      "epoch no.28 train no.800  loss = 6.94416 avg_loss = 5.61753\n",
      "epoch no.28 train no.810  loss = 5.62526 avg_loss = 5.58275\n",
      "epoch no.28 train no.820  loss = 6.63682 avg_loss = 5.60354\n",
      "epoch no.28 train no.830  loss = 5.81713 avg_loss = 5.63254\n",
      "epoch no.28 train no.840  loss = 6.28604 avg_loss = 5.60356\n",
      "epoch no.28 train no.850  loss = 6.38216 avg_loss = 5.56763\n",
      "epoch no.28 train no.860  loss = 4.99802 avg_loss = 5.54340\n",
      "epoch no.28 train no.870  loss = 4.35289 avg_loss = 5.57680\n",
      "epoch no.28 train no.880  loss = 5.97932 avg_loss = 5.60898\n",
      "epoch no.28 train no.890  loss = 4.74750 avg_loss = 5.62312\n",
      "epoch no.28 train no.900  loss = 5.91639 avg_loss = 5.65241\n",
      "epoch no.28 train no.910  loss = 4.86709 avg_loss = 5.65158\n",
      "epoch no.28 train no.920  loss = 5.05799 avg_loss = 5.66636\n",
      "epoch no.28 train no.930  loss = 6.85473 avg_loss = 5.66995\n",
      "epoch no.28 train no.940  loss = 5.94351 avg_loss = 5.67839\n",
      "epoch no.28 train no.950  loss = 6.41776 avg_loss = 5.67760\n",
      "epoch no.28 train no.960  loss = 5.77844 avg_loss = 5.64783\n",
      "epoch no.28 train no.970  loss = 7.26226 avg_loss = 5.70369\n",
      "epoch no.28 train no.980  loss = 5.61620 avg_loss = 5.71029\n",
      "epoch no.28 train no.990  loss = 4.88149 avg_loss = 5.66545\n",
      "epoch no.28 train no.1000  loss = 5.58343 avg_loss = 5.66341\n",
      "to_tokens: ['</s>', '</s>', '</s>']\n",
      "뭐함</s>\n",
      "epoch no.28 train no.1010  loss = 6.95649 avg_loss = 5.70688\n",
      "epoch no.28 train no.1020  loss = 6.01341 avg_loss = 5.68408\n",
      "epoch no.28 train no.1030  loss = 6.36389 avg_loss = 5.72287\n",
      "epoch no.28 train no.1040  loss = 6.22434 avg_loss = 5.77715\n",
      "epoch no.28 train no.1050  loss = 4.96833 avg_loss = 5.78102\n",
      "epoch no.28 train no.1060  loss = 6.67454 avg_loss = 5.76249\n",
      "epoch no.28 train no.1070  loss = 6.21813 avg_loss = 5.75462\n",
      "epoch no.28 train no.1080  loss = 5.08039 avg_loss = 5.70065\n",
      "epoch no.28 train no.1090  loss = 4.83995 avg_loss = 5.71669\n",
      "epoch no.28 train no.1100  loss = 5.82280 avg_loss = 5.68058\n",
      "epoch no.28 train no.1110  loss = 5.37461 avg_loss = 5.64254\n",
      "epoch no.28 train no.1120  loss = 5.59414 avg_loss = 5.61417\n",
      "epoch no.28 train no.1130  loss = 5.97223 avg_loss = 5.60572\n",
      "epoch no.28 train no.1140  loss = 4.45279 avg_loss = 5.61435\n",
      "epoch no.28 train no.1150  loss = 6.57301 avg_loss = 5.65281\n",
      "epoch no.28 train no.1160  loss = 5.37201 avg_loss = 5.67007\n",
      "epoch no.28 train no.1170  loss = 5.57695 avg_loss = 5.66210\n",
      "epoch no.28 train no.1180  loss = 4.80420 avg_loss = 5.64808\n",
      "epoch no.28 train no.1190  loss = 5.33041 avg_loss = 5.64641\n",
      "epoch no.28 train no.1200  loss = 5.67331 avg_loss = 5.58354\n",
      "epoch no.28 train no.1210  loss = 4.99643 avg_loss = 5.57132\n",
      "epoch no.28 train no.1220  loss = 5.55667 avg_loss = 5.57490\n",
      "epoch no.28 train no.1230  loss = 6.50250 avg_loss = 5.60933\n",
      "epoch no.28 train no.1240  loss = 4.70699 avg_loss = 5.60878\n",
      "epoch no.28 train no.1250  loss = 5.95357 avg_loss = 5.60746\n",
      "epoch no.28 train no.1260  loss = 5.07058 avg_loss = 5.61559\n",
      "epoch no.28 train no.1270  loss = 6.77092 avg_loss = 5.60028\n",
      "epoch no.28 train no.1280  loss = 6.64334 avg_loss = 5.58756\n",
      "epoch no.28 train no.1290  loss = 5.56699 avg_loss = 5.60413\n",
      "epoch no.28 train no.1300  loss = 4.36686 avg_loss = 5.55153\n",
      "epoch no.28 train no.1310  loss = 3.38563 avg_loss = 5.54395\n",
      "epoch no.28 train no.1320  loss = 5.39836 avg_loss = 5.56596\n",
      "epoch no.28 train no.1330  loss = 5.25908 avg_loss = 5.59447\n",
      "epoch no.28 train no.1340  loss = 5.87123 avg_loss = 5.63296\n",
      "epoch no.28 train no.1350  loss = 6.07209 avg_loss = 5.62041\n",
      "epoch no.28 train no.1360  loss = 5.35214 avg_loss = 5.60211\n",
      "epoch no.28 train no.1370  loss = 6.66900 avg_loss = 5.60681\n",
      "epoch no.28 train no.1380  loss = 3.84958 avg_loss = 5.55272\n",
      "epoch no.28 train no.1390  loss = 6.25028 avg_loss = 5.54838\n",
      "epoch no.28 train no.1400  loss = 3.86418 avg_loss = 5.53230\n",
      "epoch no.28 train no.1410  loss = 5.23585 avg_loss = 5.55202\n",
      "epoch no.28 train no.1420  loss = 4.58401 avg_loss = 5.53995\n",
      "epoch no.28 train no.1430  loss = 5.01445 avg_loss = 5.54430\n",
      "epoch no.28 train no.1440  loss = 4.05273 avg_loss = 5.54021\n",
      "epoch no.28 train no.1450  loss = 6.15523 avg_loss = 5.51841\n",
      "epoch no.28 train no.1460  loss = 5.80585 avg_loss = 5.53575\n",
      "epoch no.28 train no.1470  loss = 4.53298 avg_loss = 5.50755\n",
      "epoch no.28 train no.1480  loss = 5.94955 avg_loss = 5.51789\n",
      "epoch no.28 train no.1490  loss = 3.87241 avg_loss = 5.54902\n",
      "epoch no.28 train no.1500  loss = 6.98945 avg_loss = 5.59014\n",
      "epoch no.28 train no.1510  loss = 6.54568 avg_loss = 5.56817\n",
      "epoch no.28 train no.1520  loss = 5.70382 avg_loss = 5.57396\n",
      "epoch no.28 train no.1530  loss = 5.62921 avg_loss = 5.57625\n",
      "epoch no.28 train no.1540  loss = 5.74965 avg_loss = 5.54831\n",
      "epoch no.28 train no.1550  loss = 5.19588 avg_loss = 5.49201\n",
      "epoch no.28 train no.1560  loss = 4.62128 avg_loss = 5.46058\n",
      "epoch no.28 train no.1570  loss = 4.72949 avg_loss = 5.46265\n",
      "epoch no.28 train no.1580  loss = 5.25542 avg_loss = 5.43688\n",
      "epoch no.28 train no.1590  loss = 5.99716 avg_loss = 5.45831\n",
      "epoch no.28 train no.1600  loss = 5.24402 avg_loss = 5.46955\n",
      "epoch no.28 train no.1610  loss = 5.30538 avg_loss = 5.48072\n",
      "epoch no.28 train no.1620  loss = 5.90485 avg_loss = 5.51644\n",
      "epoch no.28 train no.1630  loss = 5.08106 avg_loss = 5.52123\n",
      "epoch no.28 train no.1640  loss = 5.39604 avg_loss = 5.57899\n",
      "epoch no.28 train no.1650  loss = 6.65293 avg_loss = 5.53427\n",
      "epoch no.28 train no.1660  loss = 6.29197 avg_loss = 5.58077\n",
      "epoch no.28 train no.1670  loss = 4.91146 avg_loss = 5.60385\n",
      "epoch no.28 train no.1680  loss = 6.08061 avg_loss = 5.64212\n",
      "epoch no.28 train no.1690  loss = 4.82090 avg_loss = 5.64079\n",
      "epoch no.28 train no.1700  loss = 5.35753 avg_loss = 5.60943\n",
      "epoch no.28 train no.1710  loss = 4.94657 avg_loss = 5.59529\n",
      "epoch no.28 train no.1720  loss = 5.32291 avg_loss = 5.57987\n",
      "epoch no.28 train no.1730  loss = 4.83900 avg_loss = 5.58521\n",
      "epoch no.28 train no.1740  loss = 3.41871 avg_loss = 5.57303\n",
      "epoch no.28 train no.1750  loss = 6.24787 avg_loss = 5.54532\n",
      "epoch no.29 train no.0  loss = 5.54230 avg_loss = 5.56228\n",
      "epoch no.29 train no.10  loss = 5.51416 avg_loss = 5.59777\n",
      "epoch no.29 train no.20  loss = 6.61697 avg_loss = 5.60674\n",
      "epoch no.29 train no.30  loss = 6.96445 avg_loss = 5.59641\n",
      "epoch no.29 train no.40  loss = 6.15843 avg_loss = 5.59353\n",
      "epoch no.29 train no.50  loss = 6.76073 avg_loss = 5.58797\n",
      "epoch no.29 train no.60  loss = 4.33461 avg_loss = 5.55457\n",
      "epoch no.29 train no.70  loss = 6.00749 avg_loss = 5.54688\n",
      "epoch no.29 train no.80  loss = 6.36298 avg_loss = 5.57864\n",
      "epoch no.29 train no.90  loss = 6.82737 avg_loss = 5.57322\n",
      "epoch no.29 train no.100  loss = 6.48797 avg_loss = 5.55866\n",
      "epoch no.29 train no.110  loss = 6.53352 avg_loss = 5.58538\n",
      "epoch no.29 train no.120  loss = 5.43828 avg_loss = 5.62600\n",
      "epoch no.29 train no.130  loss = 5.54433 avg_loss = 5.61618\n",
      "epoch no.29 train no.140  loss = 3.18295 avg_loss = 5.61683\n",
      "epoch no.29 train no.150  loss = 3.98767 avg_loss = 5.56471\n",
      "epoch no.29 train no.160  loss = 5.27924 avg_loss = 5.56595\n",
      "epoch no.29 train no.170  loss = 6.03181 avg_loss = 5.59031\n",
      "epoch no.29 train no.180  loss = 6.28236 avg_loss = 5.60380\n",
      "epoch no.29 train no.190  loss = 4.98234 avg_loss = 5.56658\n",
      "epoch no.29 train no.200  loss = 4.54762 avg_loss = 5.56834\n",
      "epoch no.29 train no.210  loss = 6.41672 avg_loss = 5.55914\n",
      "epoch no.29 train no.220  loss = 6.38260 avg_loss = 5.56599\n",
      "epoch no.29 train no.230  loss = 5.97183 avg_loss = 5.61743\n",
      "epoch no.29 train no.240  loss = 6.02900 avg_loss = 5.63151\n",
      "epoch no.29 train no.250  loss = 4.15769 avg_loss = 5.60828\n",
      "epoch no.29 train no.260  loss = 6.30984 avg_loss = 5.62976\n",
      "epoch no.29 train no.270  loss = 4.79416 avg_loss = 5.55939\n",
      "epoch no.29 train no.280  loss = 5.30468 avg_loss = 5.56221\n",
      "epoch no.29 train no.290  loss = 5.92380 avg_loss = 5.55624\n",
      "epoch no.29 train no.300  loss = 7.21168 avg_loss = 5.60272\n",
      "epoch no.29 train no.310  loss = 6.01362 avg_loss = 5.60043\n",
      "epoch no.29 train no.320  loss = 5.73373 avg_loss = 5.57572\n",
      "epoch no.29 train no.330  loss = 5.76583 avg_loss = 5.63331\n",
      "epoch no.29 train no.340  loss = 6.53939 avg_loss = 5.61581\n",
      "epoch no.29 train no.350  loss = 5.03950 avg_loss = 5.61115\n",
      "epoch no.29 train no.360  loss = 4.72187 avg_loss = 5.62422\n",
      "epoch no.29 train no.370  loss = 5.34092 avg_loss = 5.62079\n",
      "epoch no.29 train no.380  loss = 6.05693 avg_loss = 5.65156\n",
      "epoch no.29 train no.390  loss = 3.21534 avg_loss = 5.62252\n",
      "epoch no.29 train no.400  loss = 6.58908 avg_loss = 5.59738\n",
      "epoch no.29 train no.410  loss = 6.55236 avg_loss = 5.60436\n",
      "epoch no.29 train no.420  loss = 6.63493 avg_loss = 5.66067\n",
      "epoch no.29 train no.430  loss = 3.94477 avg_loss = 5.64629\n",
      "epoch no.29 train no.440  loss = 6.41553 avg_loss = 5.67030\n",
      "epoch no.29 train no.450  loss = 4.57725 avg_loss = 5.64419\n",
      "epoch no.29 train no.460  loss = 5.65580 avg_loss = 5.66095\n",
      "epoch no.29 train no.470  loss = 4.33893 avg_loss = 5.62561\n",
      "epoch no.29 train no.480  loss = 7.29222 avg_loss = 5.64190\n",
      "epoch no.29 train no.490  loss = 6.59692 avg_loss = 5.66592\n",
      "epoch no.29 train no.500  loss = 5.40346 avg_loss = 5.63971\n",
      "epoch no.29 train no.510  loss = 6.26260 avg_loss = 5.61092\n",
      "epoch no.29 train no.520  loss = 6.12658 avg_loss = 5.62848\n",
      "epoch no.29 train no.530  loss = 6.43265 avg_loss = 5.62601\n",
      "epoch no.29 train no.540  loss = 5.44458 avg_loss = 5.66008\n",
      "epoch no.29 train no.550  loss = 4.85842 avg_loss = 5.59472\n",
      "epoch no.29 train no.560  loss = 5.10648 avg_loss = 5.60086\n",
      "epoch no.29 train no.570  loss = 5.30538 avg_loss = 5.56838\n",
      "epoch no.29 train no.580  loss = 6.85529 avg_loss = 5.58550\n",
      "epoch no.29 train no.590  loss = 6.75088 avg_loss = 5.55263\n",
      "epoch no.29 train no.600  loss = 6.50286 avg_loss = 5.59003\n",
      "epoch no.29 train no.610  loss = 4.88401 avg_loss = 5.52457\n",
      "epoch no.29 train no.620  loss = 7.73390 avg_loss = 5.54288\n",
      "epoch no.29 train no.630  loss = 5.66271 avg_loss = 5.54937\n",
      "epoch no.29 train no.640  loss = 6.18814 avg_loss = 5.56513\n",
      "epoch no.29 train no.650  loss = 6.03759 avg_loss = 5.60828\n",
      "epoch no.29 train no.660  loss = 5.34099 avg_loss = 5.64659\n",
      "epoch no.29 train no.670  loss = 6.63282 avg_loss = 5.66655\n",
      "epoch no.29 train no.680  loss = 5.71682 avg_loss = 5.65234\n",
      "epoch no.29 train no.690  loss = 6.06715 avg_loss = 5.65671\n",
      "epoch no.29 train no.700  loss = 5.78320 avg_loss = 5.65434\n",
      "epoch no.29 train no.710  loss = 4.40161 avg_loss = 5.57431\n",
      "epoch no.29 train no.720  loss = 5.71942 avg_loss = 5.59218\n",
      "epoch no.29 train no.730  loss = 5.37538 avg_loss = 5.62982\n",
      "epoch no.29 train no.740  loss = 6.79134 avg_loss = 5.64245\n",
      "epoch no.29 train no.750  loss = 5.48796 avg_loss = 5.62350\n",
      "epoch no.29 train no.760  loss = 5.94912 avg_loss = 5.60013\n",
      "epoch no.29 train no.770  loss = 5.40286 avg_loss = 5.59884\n",
      "epoch no.29 train no.780  loss = 7.75938 avg_loss = 5.62544\n",
      "epoch no.29 train no.790  loss = 6.45114 avg_loss = 5.61066\n",
      "epoch no.29 train no.800  loss = 6.91708 avg_loss = 5.65052\n",
      "epoch no.29 train no.810  loss = 6.32338 avg_loss = 5.62513\n",
      "epoch no.29 train no.820  loss = 4.79612 avg_loss = 5.58914\n",
      "epoch no.29 train no.830  loss = 4.99909 avg_loss = 5.53219\n",
      "epoch no.29 train no.840  loss = 5.74905 avg_loss = 5.50222\n",
      "epoch no.29 train no.850  loss = 6.98245 avg_loss = 5.48236\n",
      "epoch no.29 train no.860  loss = 6.52728 avg_loss = 5.50613\n",
      "epoch no.29 train no.870  loss = 7.04740 avg_loss = 5.52199\n",
      "epoch no.29 train no.880  loss = 6.74688 avg_loss = 5.56604\n",
      "epoch no.29 train no.890  loss = 5.50692 avg_loss = 5.56383\n",
      "epoch no.29 train no.900  loss = 4.66438 avg_loss = 5.57021\n",
      "epoch no.29 train no.910  loss = 5.03495 avg_loss = 5.58360\n",
      "epoch no.29 train no.920  loss = 6.14716 avg_loss = 5.62064\n",
      "epoch no.29 train no.930  loss = 4.82298 avg_loss = 5.59357\n",
      "epoch no.29 train no.940  loss = 3.72061 avg_loss = 5.57692\n",
      "epoch no.29 train no.950  loss = 6.68611 avg_loss = 5.57277\n",
      "epoch no.29 train no.960  loss = 5.21558 avg_loss = 5.56665\n",
      "epoch no.29 train no.970  loss = 5.20246 avg_loss = 5.54626\n",
      "epoch no.29 train no.980  loss = 5.71152 avg_loss = 5.53656\n",
      "epoch no.29 train no.990  loss = 6.21386 avg_loss = 5.56901\n",
      "epoch no.29 train no.1000  loss = 6.15214 avg_loss = 5.61901\n",
      "to_tokens: ['</s>', '</s>', '</s>']\n",
      "뭐함</s>\n",
      "epoch no.29 train no.1010  loss = 6.06879 avg_loss = 5.56589\n",
      "epoch no.29 train no.1020  loss = 3.74081 avg_loss = 5.51594\n",
      "epoch no.29 train no.1030  loss = 3.88752 avg_loss = 5.46623\n",
      "epoch no.29 train no.1040  loss = 5.34327 avg_loss = 5.46737\n",
      "epoch no.29 train no.1050  loss = 7.09174 avg_loss = 5.48037\n",
      "epoch no.29 train no.1060  loss = 5.95393 avg_loss = 5.44151\n",
      "epoch no.29 train no.1070  loss = 6.32321 avg_loss = 5.48905\n",
      "epoch no.29 train no.1080  loss = 7.30122 avg_loss = 5.50182\n",
      "epoch no.29 train no.1090  loss = 4.28910 avg_loss = 5.46194\n",
      "epoch no.29 train no.1100  loss = 5.36122 avg_loss = 5.43748\n",
      "epoch no.29 train no.1110  loss = 6.05881 avg_loss = 5.49089\n",
      "epoch no.29 train no.1120  loss = 5.73221 avg_loss = 5.53530\n",
      "epoch no.29 train no.1130  loss = 4.71746 avg_loss = 5.49892\n",
      "epoch no.29 train no.1140  loss = 4.59854 avg_loss = 5.47222\n",
      "epoch no.29 train no.1150  loss = 5.93570 avg_loss = 5.51675\n",
      "epoch no.29 train no.1160  loss = 5.72160 avg_loss = 5.51580\n",
      "epoch no.29 train no.1170  loss = 6.74032 avg_loss = 5.57566\n",
      "epoch no.29 train no.1180  loss = 4.93376 avg_loss = 5.55271\n",
      "epoch no.29 train no.1190  loss = 6.41456 avg_loss = 5.56655\n",
      "epoch no.29 train no.1200  loss = 6.66786 avg_loss = 5.59709\n",
      "epoch no.29 train no.1210  loss = 4.81383 avg_loss = 5.60129\n",
      "epoch no.29 train no.1220  loss = 5.93122 avg_loss = 5.61525\n",
      "epoch no.29 train no.1230  loss = 7.20219 avg_loss = 5.58334\n",
      "epoch no.29 train no.1240  loss = 7.28422 avg_loss = 5.62199\n",
      "epoch no.29 train no.1250  loss = 5.35372 avg_loss = 5.59879\n",
      "epoch no.29 train no.1260  loss = 5.32652 avg_loss = 5.60915\n",
      "epoch no.29 train no.1270  loss = 6.19389 avg_loss = 5.59420\n",
      "epoch no.29 train no.1280  loss = 3.92003 avg_loss = 5.57252\n",
      "epoch no.29 train no.1290  loss = 6.24911 avg_loss = 5.61217\n",
      "epoch no.29 train no.1300  loss = 5.35016 avg_loss = 5.58875\n",
      "epoch no.29 train no.1310  loss = 6.05054 avg_loss = 5.59563\n",
      "epoch no.29 train no.1320  loss = 4.35938 avg_loss = 5.55124\n",
      "epoch no.29 train no.1330  loss = 5.77990 avg_loss = 5.59063\n",
      "epoch no.29 train no.1340  loss = 6.26793 avg_loss = 5.59752\n",
      "epoch no.29 train no.1350  loss = 5.96742 avg_loss = 5.62036\n",
      "epoch no.29 train no.1360  loss = 5.66144 avg_loss = 5.60216\n",
      "epoch no.29 train no.1370  loss = 6.57938 avg_loss = 5.62062\n",
      "epoch no.29 train no.1380  loss = 5.88407 avg_loss = 5.62724\n",
      "epoch no.29 train no.1390  loss = 5.39555 avg_loss = 5.60223\n",
      "epoch no.29 train no.1400  loss = 5.24101 avg_loss = 5.60854\n",
      "epoch no.29 train no.1410  loss = 4.89425 avg_loss = 5.62844\n",
      "epoch no.29 train no.1420  loss = 6.77561 avg_loss = 5.63442\n",
      "epoch no.29 train no.1430  loss = 6.96561 avg_loss = 5.60977\n",
      "epoch no.29 train no.1440  loss = 5.88174 avg_loss = 5.58384\n",
      "epoch no.29 train no.1450  loss = 5.58840 avg_loss = 5.59216\n",
      "epoch no.29 train no.1460  loss = 5.42398 avg_loss = 5.59259\n",
      "epoch no.29 train no.1470  loss = 5.55508 avg_loss = 5.62930\n",
      "epoch no.29 train no.1480  loss = 4.75805 avg_loss = 5.60311\n",
      "epoch no.29 train no.1490  loss = 4.84514 avg_loss = 5.59111\n",
      "epoch no.29 train no.1500  loss = 5.31702 avg_loss = 5.59300\n",
      "epoch no.29 train no.1510  loss = 5.94114 avg_loss = 5.59203\n",
      "epoch no.29 train no.1520  loss = 6.33611 avg_loss = 5.61866\n",
      "epoch no.29 train no.1530  loss = 5.47146 avg_loss = 5.61514\n",
      "epoch no.29 train no.1540  loss = 5.88830 avg_loss = 5.65208\n",
      "epoch no.29 train no.1550  loss = 6.08526 avg_loss = 5.65618\n",
      "epoch no.29 train no.1560  loss = 4.53028 avg_loss = 5.61578\n",
      "epoch no.29 train no.1570  loss = 6.33011 avg_loss = 5.60418\n",
      "epoch no.29 train no.1580  loss = 6.62259 avg_loss = 5.64201\n",
      "epoch no.29 train no.1590  loss = 5.75579 avg_loss = 5.62336\n",
      "epoch no.29 train no.1600  loss = 6.83189 avg_loss = 5.59121\n",
      "epoch no.29 train no.1610  loss = 7.10420 avg_loss = 5.56212\n",
      "epoch no.29 train no.1620  loss = 4.75106 avg_loss = 5.56793\n",
      "epoch no.29 train no.1630  loss = 5.07573 avg_loss = 5.59217\n",
      "epoch no.29 train no.1640  loss = 5.40431 avg_loss = 5.61454\n",
      "epoch no.29 train no.1650  loss = 5.50371 avg_loss = 5.66290\n",
      "epoch no.29 train no.1660  loss = 4.23214 avg_loss = 5.60126\n",
      "epoch no.29 train no.1670  loss = 5.87395 avg_loss = 5.61913\n",
      "epoch no.29 train no.1680  loss = 6.92848 avg_loss = 5.60550\n",
      "epoch no.29 train no.1690  loss = 6.01326 avg_loss = 5.62250\n",
      "epoch no.29 train no.1700  loss = 4.95345 avg_loss = 5.65223\n",
      "epoch no.29 train no.1710  loss = 4.19537 avg_loss = 5.69773\n",
      "epoch no.29 train no.1720  loss = 5.22855 avg_loss = 5.67768\n",
      "epoch no.29 train no.1730  loss = 5.98673 avg_loss = 5.68615\n",
      "epoch no.29 train no.1740  loss = 4.70780 avg_loss = 5.69173\n",
      "epoch no.29 train no.1750  loss = 5.77283 avg_loss = 5.71938\n",
      "epoch no.30 train no.0  loss = 5.84617 avg_loss = 5.71212\n",
      "epoch no.30 train no.10  loss = 5.68270 avg_loss = 5.71828\n",
      "epoch no.30 train no.20  loss = 5.31657 avg_loss = 5.73648\n",
      "epoch no.30 train no.30  loss = 5.71707 avg_loss = 5.73865\n",
      "epoch no.30 train no.40  loss = 4.65868 avg_loss = 5.78744\n",
      "epoch no.30 train no.50  loss = 4.05303 avg_loss = 5.73899\n",
      "epoch no.30 train no.60  loss = 6.49647 avg_loss = 5.76638\n",
      "epoch no.30 train no.70  loss = 4.58019 avg_loss = 5.71120\n",
      "epoch no.30 train no.80  loss = 5.56778 avg_loss = 5.68134\n",
      "epoch no.30 train no.90  loss = 7.34017 avg_loss = 5.67119\n",
      "epoch no.30 train no.100  loss = 5.84884 avg_loss = 5.66608\n",
      "epoch no.30 train no.110  loss = 4.15810 avg_loss = 5.61816\n",
      "epoch no.30 train no.120  loss = 4.94286 avg_loss = 5.59874\n",
      "epoch no.30 train no.130  loss = 4.96655 avg_loss = 5.57965\n",
      "epoch no.30 train no.140  loss = 6.21983 avg_loss = 5.55936\n",
      "epoch no.30 train no.150  loss = 5.69309 avg_loss = 5.53636\n",
      "epoch no.30 train no.160  loss = 5.10451 avg_loss = 5.53579\n",
      "epoch no.30 train no.170  loss = 6.96366 avg_loss = 5.56615\n",
      "epoch no.30 train no.180  loss = 5.55053 avg_loss = 5.62402\n",
      "epoch no.30 train no.190  loss = 6.61563 avg_loss = 5.63205\n",
      "epoch no.30 train no.200  loss = 6.08274 avg_loss = 5.61980\n",
      "epoch no.30 train no.210  loss = 5.21744 avg_loss = 5.60579\n",
      "epoch no.30 train no.220  loss = 5.17437 avg_loss = 5.55512\n",
      "epoch no.30 train no.230  loss = 5.33620 avg_loss = 5.49705\n",
      "epoch no.30 train no.240  loss = 5.67417 avg_loss = 5.48383\n",
      "epoch no.30 train no.250  loss = 5.49373 avg_loss = 5.51156\n",
      "epoch no.30 train no.260  loss = 5.47957 avg_loss = 5.48957\n",
      "epoch no.30 train no.270  loss = 3.94767 avg_loss = 5.48055\n",
      "epoch no.30 train no.280  loss = 6.39644 avg_loss = 5.49739\n",
      "epoch no.30 train no.290  loss = 5.88829 avg_loss = 5.51830\n",
      "epoch no.30 train no.300  loss = 5.34152 avg_loss = 5.52539\n",
      "epoch no.30 train no.310  loss = 5.09475 avg_loss = 5.53858\n",
      "epoch no.30 train no.320  loss = 7.29404 avg_loss = 5.55804\n",
      "epoch no.30 train no.330  loss = 4.82349 avg_loss = 5.55858\n",
      "epoch no.30 train no.340  loss = 5.17064 avg_loss = 5.52114\n",
      "epoch no.30 train no.350  loss = 6.29616 avg_loss = 5.51818\n",
      "epoch no.30 train no.360  loss = 4.14028 avg_loss = 5.50499\n",
      "epoch no.30 train no.370  loss = 5.52612 avg_loss = 5.51112\n",
      "epoch no.30 train no.380  loss = 6.53910 avg_loss = 5.52440\n",
      "epoch no.30 train no.390  loss = 6.16593 avg_loss = 5.58555\n",
      "epoch no.30 train no.400  loss = 5.69179 avg_loss = 5.57841\n",
      "epoch no.30 train no.410  loss = 4.35243 avg_loss = 5.57619\n",
      "epoch no.30 train no.420  loss = 4.47487 avg_loss = 5.59056\n",
      "epoch no.30 train no.430  loss = 4.71603 avg_loss = 5.61325\n",
      "epoch no.30 train no.440  loss = 5.74605 avg_loss = 5.63549\n",
      "epoch no.30 train no.450  loss = 4.63847 avg_loss = 5.65754\n",
      "epoch no.30 train no.460  loss = 5.36418 avg_loss = 5.65513\n",
      "epoch no.30 train no.470  loss = 6.38919 avg_loss = 5.64643\n",
      "epoch no.30 train no.480  loss = 6.39637 avg_loss = 5.64151\n",
      "epoch no.30 train no.490  loss = 6.44534 avg_loss = 5.65208\n",
      "epoch no.30 train no.500  loss = 5.55181 avg_loss = 5.61833\n",
      "epoch no.30 train no.510  loss = 4.80849 avg_loss = 5.64966\n",
      "epoch no.30 train no.520  loss = 3.62362 avg_loss = 5.58381\n",
      "epoch no.30 train no.530  loss = 6.39293 avg_loss = 5.62146\n",
      "epoch no.30 train no.540  loss = 5.93886 avg_loss = 5.63515\n",
      "epoch no.30 train no.550  loss = 6.20545 avg_loss = 5.62750\n",
      "epoch no.30 train no.560  loss = 6.57410 avg_loss = 5.62506\n",
      "epoch no.30 train no.570  loss = 5.42605 avg_loss = 5.64862\n",
      "epoch no.30 train no.580  loss = 4.81484 avg_loss = 5.61539\n",
      "epoch no.30 train no.590  loss = 5.78057 avg_loss = 5.63065\n",
      "epoch no.30 train no.600  loss = 5.60590 avg_loss = 5.64610\n",
      "epoch no.30 train no.610  loss = 5.96694 avg_loss = 5.68354\n",
      "epoch no.30 train no.620  loss = 5.89401 avg_loss = 5.71230\n",
      "epoch no.30 train no.630  loss = 6.21952 avg_loss = 5.72434\n",
      "epoch no.30 train no.640  loss = 5.51689 avg_loss = 5.71644\n",
      "epoch no.30 train no.650  loss = 5.60721 avg_loss = 5.68293\n",
      "epoch no.30 train no.660  loss = 4.81125 avg_loss = 5.64856\n",
      "epoch no.30 train no.670  loss = 5.59752 avg_loss = 5.63927\n",
      "epoch no.30 train no.680  loss = 5.69563 avg_loss = 5.66146\n",
      "epoch no.30 train no.690  loss = 4.35730 avg_loss = 5.62629\n",
      "epoch no.30 train no.700  loss = 4.02489 avg_loss = 5.62251\n",
      "epoch no.30 train no.710  loss = 4.23615 avg_loss = 5.58578\n",
      "epoch no.30 train no.720  loss = 4.81366 avg_loss = 5.59262\n",
      "epoch no.30 train no.730  loss = 5.42685 avg_loss = 5.58620\n",
      "epoch no.30 train no.740  loss = 5.48396 avg_loss = 5.53438\n",
      "epoch no.30 train no.750  loss = 6.29271 avg_loss = 5.57776\n",
      "epoch no.30 train no.760  loss = 6.85887 avg_loss = 5.60187\n",
      "epoch no.30 train no.770  loss = 5.56513 avg_loss = 5.61060\n",
      "epoch no.30 train no.780  loss = 5.29651 avg_loss = 5.61444\n",
      "epoch no.30 train no.790  loss = 5.52404 avg_loss = 5.64251\n",
      "epoch no.30 train no.800  loss = 6.40494 avg_loss = 5.62252\n",
      "epoch no.30 train no.810  loss = 4.79037 avg_loss = 5.66891\n",
      "epoch no.30 train no.820  loss = 5.85162 avg_loss = 5.61435\n",
      "epoch no.30 train no.830  loss = 5.81351 avg_loss = 5.57848\n",
      "epoch no.30 train no.840  loss = 6.53005 avg_loss = 5.59051\n",
      "epoch no.30 train no.850  loss = 6.17038 avg_loss = 5.66704\n",
      "epoch no.30 train no.860  loss = 5.35854 avg_loss = 5.68037\n",
      "epoch no.30 train no.870  loss = 6.83649 avg_loss = 5.65138\n",
      "epoch no.30 train no.880  loss = 5.77616 avg_loss = 5.65626\n",
      "epoch no.30 train no.890  loss = 4.84637 avg_loss = 5.63545\n",
      "epoch no.30 train no.900  loss = 5.02745 avg_loss = 5.59893\n",
      "epoch no.30 train no.910  loss = 5.64503 avg_loss = 5.57973\n",
      "epoch no.30 train no.920  loss = 5.62355 avg_loss = 5.52645\n",
      "epoch no.30 train no.930  loss = 6.94372 avg_loss = 5.50085\n",
      "epoch no.30 train no.940  loss = 4.03178 avg_loss = 5.48416\n",
      "epoch no.30 train no.950  loss = 7.74607 avg_loss = 5.50004\n",
      "epoch no.30 train no.960  loss = 5.50101 avg_loss = 5.53478\n",
      "epoch no.30 train no.970  loss = 5.08276 avg_loss = 5.48990\n",
      "epoch no.30 train no.980  loss = 5.29434 avg_loss = 5.53263\n",
      "epoch no.30 train no.990  loss = 4.81459 avg_loss = 5.53277\n",
      "epoch no.30 train no.1000  loss = 5.60934 avg_loss = 5.55504\n",
      "to_tokens: ['</s>', '</s>', '</s>']\n",
      "뭐함</s>\n",
      "epoch no.30 train no.1010  loss = 5.53214 avg_loss = 5.57730\n",
      "epoch no.30 train no.1020  loss = 6.10511 avg_loss = 5.57696\n",
      "epoch no.30 train no.1030  loss = 4.80182 avg_loss = 5.53220\n",
      "epoch no.30 train no.1040  loss = 6.13380 avg_loss = 5.52511\n",
      "epoch no.30 train no.1050  loss = 4.27134 avg_loss = 5.51339\n",
      "epoch no.30 train no.1060  loss = 6.65472 avg_loss = 5.51554\n",
      "epoch no.30 train no.1070  loss = 5.35390 avg_loss = 5.51854\n",
      "epoch no.30 train no.1080  loss = 6.27748 avg_loss = 5.50297\n",
      "epoch no.30 train no.1090  loss = 6.52277 avg_loss = 5.52673\n",
      "epoch no.30 train no.1100  loss = 5.47603 avg_loss = 5.51693\n",
      "epoch no.30 train no.1110  loss = 6.24207 avg_loss = 5.52175\n",
      "epoch no.30 train no.1120  loss = 6.18357 avg_loss = 5.54561\n",
      "epoch no.30 train no.1130  loss = 5.79281 avg_loss = 5.55698\n",
      "epoch no.30 train no.1140  loss = 3.88037 avg_loss = 5.56016\n",
      "epoch no.30 train no.1150  loss = 5.87128 avg_loss = 5.55601\n",
      "epoch no.30 train no.1160  loss = 6.33628 avg_loss = 5.53796\n",
      "epoch no.30 train no.1170  loss = 4.95725 avg_loss = 5.53099\n",
      "epoch no.30 train no.1180  loss = 6.40707 avg_loss = 5.55345\n",
      "epoch no.30 train no.1190  loss = 5.13523 avg_loss = 5.54787\n",
      "epoch no.30 train no.1200  loss = 4.30215 avg_loss = 5.49551\n",
      "epoch no.30 train no.1210  loss = 4.39170 avg_loss = 5.46501\n",
      "epoch no.30 train no.1220  loss = 3.99021 avg_loss = 5.46834\n",
      "epoch no.30 train no.1230  loss = 5.74575 avg_loss = 5.48475\n",
      "epoch no.30 train no.1240  loss = 5.17485 avg_loss = 5.52400\n",
      "epoch no.30 train no.1250  loss = 6.56026 avg_loss = 5.52044\n",
      "epoch no.30 train no.1260  loss = 6.09509 avg_loss = 5.53149\n",
      "epoch no.30 train no.1270  loss = 7.17731 avg_loss = 5.53642\n",
      "epoch no.30 train no.1280  loss = 5.49523 avg_loss = 5.54550\n",
      "epoch no.30 train no.1290  loss = 4.75483 avg_loss = 5.50094\n",
      "epoch no.30 train no.1300  loss = 4.02619 avg_loss = 5.52794\n",
      "epoch no.30 train no.1310  loss = 5.98302 avg_loss = 5.53722\n",
      "epoch no.30 train no.1320  loss = 5.49053 avg_loss = 5.54264\n",
      "epoch no.30 train no.1330  loss = 5.86122 avg_loss = 5.53538\n",
      "epoch no.30 train no.1340  loss = 5.51763 avg_loss = 5.55881\n",
      "epoch no.30 train no.1350  loss = 3.10044 avg_loss = 5.47008\n",
      "epoch no.30 train no.1360  loss = 4.38841 avg_loss = 5.54307\n",
      "epoch no.30 train no.1370  loss = 5.72574 avg_loss = 5.57102\n",
      "epoch no.30 train no.1380  loss = 5.97530 avg_loss = 5.60764\n",
      "epoch no.30 train no.1390  loss = 5.62635 avg_loss = 5.59895\n",
      "epoch no.30 train no.1400  loss = 4.89967 avg_loss = 5.59619\n",
      "epoch no.30 train no.1410  loss = 5.13805 avg_loss = 5.55895\n",
      "epoch no.30 train no.1420  loss = 4.28714 avg_loss = 5.55431\n",
      "epoch no.30 train no.1430  loss = 4.76188 avg_loss = 5.56236\n",
      "epoch no.30 train no.1440  loss = 5.32565 avg_loss = 5.54299\n",
      "epoch no.30 train no.1450  loss = 6.20090 avg_loss = 5.57397\n",
      "epoch no.30 train no.1460  loss = 6.20032 avg_loss = 5.53333\n",
      "epoch no.30 train no.1470  loss = 5.39995 avg_loss = 5.57468\n",
      "epoch no.30 train no.1480  loss = 5.06898 avg_loss = 5.57006\n",
      "epoch no.30 train no.1490  loss = 6.01490 avg_loss = 5.52808\n",
      "epoch no.30 train no.1500  loss = 4.55866 avg_loss = 5.54778\n",
      "epoch no.30 train no.1510  loss = 5.93802 avg_loss = 5.59089\n",
      "epoch no.30 train no.1520  loss = 4.92894 avg_loss = 5.59662\n",
      "epoch no.30 train no.1530  loss = 5.95573 avg_loss = 5.57377\n",
      "epoch no.30 train no.1540  loss = 6.02975 avg_loss = 5.58185\n",
      "epoch no.30 train no.1550  loss = 4.98966 avg_loss = 5.57164\n",
      "epoch no.30 train no.1560  loss = 5.52620 avg_loss = 5.53208\n",
      "epoch no.30 train no.1570  loss = 6.70856 avg_loss = 5.54216\n",
      "epoch no.30 train no.1580  loss = 6.42061 avg_loss = 5.54918\n",
      "epoch no.30 train no.1590  loss = 5.34215 avg_loss = 5.52631\n",
      "epoch no.30 train no.1600  loss = 5.98148 avg_loss = 5.54986\n",
      "epoch no.30 train no.1610  loss = 4.32586 avg_loss = 5.52905\n",
      "epoch no.30 train no.1620  loss = 4.32639 avg_loss = 5.50771\n",
      "epoch no.30 train no.1630  loss = 4.22007 avg_loss = 5.50810\n",
      "epoch no.30 train no.1640  loss = 6.88912 avg_loss = 5.54433\n",
      "epoch no.30 train no.1650  loss = 6.15196 avg_loss = 5.59513\n",
      "epoch no.30 train no.1660  loss = 6.19055 avg_loss = 5.57622\n",
      "epoch no.30 train no.1670  loss = 4.44852 avg_loss = 5.58339\n",
      "epoch no.30 train no.1680  loss = 7.10353 avg_loss = 5.67421\n",
      "epoch no.30 train no.1690  loss = 5.63232 avg_loss = 5.67331\n",
      "epoch no.30 train no.1700  loss = 5.97954 avg_loss = 5.67116\n",
      "epoch no.30 train no.1710  loss = 4.75944 avg_loss = 5.65430\n",
      "epoch no.30 train no.1720  loss = 4.43400 avg_loss = 5.68007\n",
      "epoch no.30 train no.1730  loss = 5.56532 avg_loss = 5.70322\n",
      "epoch no.30 train no.1740  loss = 5.70626 avg_loss = 5.68613\n",
      "epoch no.30 train no.1750  loss = 7.00832 avg_loss = 5.70138\n",
      "epoch no.31 train no.0  loss = 5.25982 avg_loss = 5.72313\n",
      "epoch no.31 train no.10  loss = 5.41117 avg_loss = 5.69883\n",
      "epoch no.31 train no.20  loss = 5.11575 avg_loss = 5.66446\n",
      "epoch no.31 train no.30  loss = 4.82170 avg_loss = 5.61849\n",
      "epoch no.31 train no.40  loss = 6.35358 avg_loss = 5.63835\n",
      "epoch no.31 train no.50  loss = 5.75035 avg_loss = 5.61598\n",
      "epoch no.31 train no.60  loss = 6.30931 avg_loss = 5.59039\n",
      "epoch no.31 train no.70  loss = 5.08424 avg_loss = 5.62674\n",
      "epoch no.31 train no.80  loss = 5.05277 avg_loss = 5.61466\n",
      "epoch no.31 train no.90  loss = 4.96161 avg_loss = 5.57311\n",
      "epoch no.31 train no.100  loss = 3.98013 avg_loss = 5.53406\n",
      "epoch no.31 train no.110  loss = 4.37974 avg_loss = 5.48856\n",
      "epoch no.31 train no.120  loss = 5.83811 avg_loss = 5.49490\n",
      "epoch no.31 train no.130  loss = 5.50205 avg_loss = 5.49713\n",
      "epoch no.31 train no.140  loss = 4.32608 avg_loss = 5.48225\n",
      "epoch no.31 train no.150  loss = 6.06485 avg_loss = 5.47774\n",
      "epoch no.31 train no.160  loss = 4.52058 avg_loss = 5.48597\n",
      "epoch no.31 train no.170  loss = 6.45413 avg_loss = 5.49705\n",
      "epoch no.31 train no.180  loss = 6.18429 avg_loss = 5.49406\n",
      "epoch no.31 train no.190  loss = 7.14840 avg_loss = 5.54514\n",
      "epoch no.31 train no.200  loss = 5.27981 avg_loss = 5.54596\n",
      "epoch no.31 train no.210  loss = 6.37585 avg_loss = 5.54070\n",
      "epoch no.31 train no.220  loss = 6.85782 avg_loss = 5.57233\n",
      "epoch no.31 train no.230  loss = 5.65659 avg_loss = 5.57035\n",
      "epoch no.31 train no.240  loss = 5.97738 avg_loss = 5.61170\n",
      "epoch no.31 train no.250  loss = 5.42187 avg_loss = 5.59123\n",
      "epoch no.31 train no.260  loss = 5.30848 avg_loss = 5.57827\n",
      "epoch no.31 train no.270  loss = 6.23162 avg_loss = 5.54711\n",
      "epoch no.31 train no.280  loss = 7.47686 avg_loss = 5.53106\n",
      "epoch no.31 train no.290  loss = 4.91051 avg_loss = 5.54037\n",
      "epoch no.31 train no.300  loss = 4.68625 avg_loss = 5.56910\n",
      "epoch no.31 train no.310  loss = 5.37229 avg_loss = 5.56682\n",
      "epoch no.31 train no.320  loss = 6.37198 avg_loss = 5.55864\n",
      "epoch no.31 train no.330  loss = 5.29208 avg_loss = 5.57397\n",
      "epoch no.31 train no.340  loss = 4.43117 avg_loss = 5.52522\n",
      "epoch no.31 train no.350  loss = 4.78180 avg_loss = 5.52258\n",
      "epoch no.31 train no.360  loss = 6.38067 avg_loss = 5.54813\n",
      "epoch no.31 train no.370  loss = 7.41241 avg_loss = 5.57817\n",
      "epoch no.31 train no.380  loss = 6.41573 avg_loss = 5.59096\n",
      "epoch no.31 train no.390  loss = 4.88932 avg_loss = 5.58219\n",
      "epoch no.31 train no.400  loss = 5.08686 avg_loss = 5.57007\n",
      "epoch no.31 train no.410  loss = 6.54184 avg_loss = 5.59447\n",
      "epoch no.31 train no.420  loss = 4.62246 avg_loss = 5.55194\n",
      "epoch no.31 train no.430  loss = 5.27627 avg_loss = 5.52209\n",
      "epoch no.31 train no.440  loss = 4.93284 avg_loss = 5.53412\n",
      "epoch no.31 train no.450  loss = 4.91255 avg_loss = 5.51043\n",
      "epoch no.31 train no.460  loss = 4.63303 avg_loss = 5.51430\n",
      "epoch no.31 train no.470  loss = 6.58550 avg_loss = 5.55091\n",
      "epoch no.31 train no.480  loss = 6.27467 avg_loss = 5.56981\n",
      "epoch no.31 train no.490  loss = 6.27965 avg_loss = 5.55060\n",
      "epoch no.31 train no.500  loss = 5.14029 avg_loss = 5.54422\n",
      "epoch no.31 train no.510  loss = 6.64016 avg_loss = 5.58021\n",
      "epoch no.31 train no.520  loss = 6.26520 avg_loss = 5.56424\n",
      "epoch no.31 train no.530  loss = 5.65656 avg_loss = 5.58714\n",
      "epoch no.31 train no.540  loss = 4.33644 avg_loss = 5.57122\n",
      "epoch no.31 train no.550  loss = 5.28234 avg_loss = 5.55449\n",
      "epoch no.31 train no.560  loss = 5.05582 avg_loss = 5.57919\n",
      "epoch no.31 train no.570  loss = 6.25407 avg_loss = 5.58307\n",
      "epoch no.31 train no.580  loss = 6.15541 avg_loss = 5.54159\n",
      "epoch no.31 train no.590  loss = 6.08615 avg_loss = 5.56148\n",
      "epoch no.31 train no.600  loss = 6.19414 avg_loss = 5.55431\n",
      "epoch no.31 train no.610  loss = 5.37667 avg_loss = 5.52709\n",
      "epoch no.31 train no.620  loss = 6.63918 avg_loss = 5.49933\n",
      "epoch no.31 train no.630  loss = 6.42855 avg_loss = 5.45326\n",
      "epoch no.31 train no.640  loss = 5.01540 avg_loss = 5.46608\n",
      "epoch no.31 train no.650  loss = 6.68475 avg_loss = 5.46048\n",
      "epoch no.31 train no.660  loss = 6.19977 avg_loss = 5.50252\n",
      "epoch no.31 train no.670  loss = 5.66849 avg_loss = 5.53030\n",
      "epoch no.31 train no.680  loss = 5.17238 avg_loss = 5.51991\n",
      "epoch no.31 train no.690  loss = 6.48692 avg_loss = 5.52490\n",
      "epoch no.31 train no.700  loss = 5.02575 avg_loss = 5.58657\n",
      "epoch no.31 train no.710  loss = 5.55274 avg_loss = 5.57252\n",
      "epoch no.31 train no.720  loss = 3.44585 avg_loss = 5.52034\n",
      "epoch no.31 train no.730  loss = 7.06674 avg_loss = 5.52894\n",
      "epoch no.31 train no.740  loss = 4.97026 avg_loss = 5.52792\n",
      "epoch no.31 train no.750  loss = 6.29682 avg_loss = 5.51910\n",
      "epoch no.31 train no.760  loss = 6.19671 avg_loss = 5.53329\n",
      "epoch no.31 train no.770  loss = 6.62100 avg_loss = 5.56842\n",
      "epoch no.31 train no.780  loss = 5.42750 avg_loss = 5.58320\n",
      "epoch no.31 train no.790  loss = 6.05398 avg_loss = 5.58670\n",
      "epoch no.31 train no.800  loss = 3.88105 avg_loss = 5.54385\n",
      "epoch no.31 train no.810  loss = 6.94486 avg_loss = 5.55836\n",
      "epoch no.31 train no.820  loss = 6.50184 avg_loss = 5.53982\n",
      "epoch no.31 train no.830  loss = 6.29896 avg_loss = 5.54959\n",
      "epoch no.31 train no.840  loss = 5.93251 avg_loss = 5.54228\n",
      "epoch no.31 train no.850  loss = 4.36445 avg_loss = 5.52536\n",
      "epoch no.31 train no.860  loss = 6.58016 avg_loss = 5.51081\n",
      "epoch no.31 train no.870  loss = 5.99097 avg_loss = 5.55702\n",
      "epoch no.31 train no.880  loss = 6.79234 avg_loss = 5.60058\n",
      "epoch no.31 train no.890  loss = 4.73346 avg_loss = 5.58994\n",
      "epoch no.31 train no.900  loss = 4.45605 avg_loss = 5.57211\n",
      "epoch no.31 train no.910  loss = 6.32715 avg_loss = 5.62921\n",
      "epoch no.31 train no.920  loss = 6.38731 avg_loss = 5.65795\n",
      "epoch no.31 train no.930  loss = 6.69400 avg_loss = 5.69027\n",
      "epoch no.31 train no.940  loss = 5.12452 avg_loss = 5.69689\n",
      "epoch no.31 train no.950  loss = 5.05906 avg_loss = 5.63601\n",
      "epoch no.31 train no.960  loss = 6.36654 avg_loss = 5.65698\n",
      "epoch no.31 train no.970  loss = 4.81942 avg_loss = 5.62070\n",
      "epoch no.31 train no.980  loss = 5.45626 avg_loss = 5.63470\n",
      "epoch no.31 train no.990  loss = 5.45207 avg_loss = 5.66326\n",
      "epoch no.31 train no.1000  loss = 4.68067 avg_loss = 5.66221\n",
      "to_tokens: ['</s>', '</s>', '</s>']\n",
      "뭐함</s>\n",
      "epoch no.31 train no.1010  loss = 5.31153 avg_loss = 5.65030\n",
      "epoch no.31 train no.1020  loss = 4.45231 avg_loss = 5.61124\n",
      "epoch no.31 train no.1030  loss = 5.48463 avg_loss = 5.61802\n",
      "epoch no.31 train no.1040  loss = 3.99394 avg_loss = 5.61670\n",
      "epoch no.31 train no.1050  loss = 5.37580 avg_loss = 5.61659\n",
      "epoch no.31 train no.1060  loss = 5.17649 avg_loss = 5.59509\n",
      "epoch no.31 train no.1070  loss = 5.69355 avg_loss = 5.59269\n",
      "epoch no.31 train no.1080  loss = 6.50616 avg_loss = 5.57389\n",
      "epoch no.31 train no.1090  loss = 6.71071 avg_loss = 5.58235\n",
      "epoch no.31 train no.1100  loss = 4.29052 avg_loss = 5.57821\n",
      "epoch no.31 train no.1110  loss = 6.43956 avg_loss = 5.57895\n",
      "epoch no.31 train no.1120  loss = 5.10722 avg_loss = 5.59068\n",
      "epoch no.31 train no.1130  loss = 6.52590 avg_loss = 5.59323\n",
      "epoch no.31 train no.1140  loss = 4.79251 avg_loss = 5.61525\n",
      "epoch no.31 train no.1150  loss = 6.29117 avg_loss = 5.59559\n",
      "epoch no.31 train no.1160  loss = 5.23662 avg_loss = 5.57154\n",
      "epoch no.31 train no.1170  loss = 6.08853 avg_loss = 5.55845\n",
      "epoch no.31 train no.1180  loss = 6.43116 avg_loss = 5.53127\n",
      "epoch no.31 train no.1190  loss = 5.17298 avg_loss = 5.53515\n",
      "epoch no.31 train no.1200  loss = 4.90099 avg_loss = 5.51099\n",
      "epoch no.31 train no.1210  loss = 6.30901 avg_loss = 5.53576\n",
      "epoch no.31 train no.1220  loss = 6.10227 avg_loss = 5.59975\n",
      "epoch no.31 train no.1230  loss = 4.42198 avg_loss = 5.55980\n",
      "epoch no.31 train no.1240  loss = 5.33701 avg_loss = 5.55861\n",
      "epoch no.31 train no.1250  loss = 3.55755 avg_loss = 5.55276\n",
      "epoch no.31 train no.1260  loss = 4.51762 avg_loss = 5.56672\n",
      "epoch no.31 train no.1270  loss = 6.77843 avg_loss = 5.58939\n",
      "epoch no.31 train no.1280  loss = 4.84741 avg_loss = 5.57553\n",
      "epoch no.31 train no.1290  loss = 5.57163 avg_loss = 5.59947\n",
      "epoch no.31 train no.1300  loss = 5.67599 avg_loss = 5.57940\n",
      "epoch no.31 train no.1310  loss = 3.50723 avg_loss = 5.52406\n",
      "epoch no.31 train no.1320  loss = 5.40231 avg_loss = 5.51746\n",
      "epoch no.31 train no.1330  loss = 5.67725 avg_loss = 5.54498\n",
      "epoch no.31 train no.1340  loss = 5.07898 avg_loss = 5.54630\n",
      "epoch no.31 train no.1350  loss = 6.94292 avg_loss = 5.54814\n",
      "epoch no.31 train no.1360  loss = 5.66473 avg_loss = 5.55804\n",
      "epoch no.31 train no.1370  loss = 6.35751 avg_loss = 5.52866\n",
      "epoch no.31 train no.1380  loss = 6.16570 avg_loss = 5.55039\n",
      "epoch no.31 train no.1390  loss = 4.27200 avg_loss = 5.58737\n",
      "epoch no.31 train no.1400  loss = 5.85161 avg_loss = 5.59460\n",
      "epoch no.31 train no.1410  loss = 5.65201 avg_loss = 5.63690\n",
      "epoch no.31 train no.1420  loss = 6.55388 avg_loss = 5.60213\n",
      "epoch no.31 train no.1430  loss = 6.92292 avg_loss = 5.65050\n",
      "epoch no.31 train no.1440  loss = 5.82236 avg_loss = 5.66554\n",
      "epoch no.31 train no.1450  loss = 6.59512 avg_loss = 5.69277\n",
      "epoch no.31 train no.1460  loss = 5.73774 avg_loss = 5.71934\n",
      "epoch no.31 train no.1470  loss = 5.78850 avg_loss = 5.72168\n",
      "epoch no.31 train no.1480  loss = 5.96340 avg_loss = 5.71693\n",
      "epoch no.31 train no.1490  loss = 5.03157 avg_loss = 5.67517\n",
      "epoch no.31 train no.1500  loss = 3.41666 avg_loss = 5.64318\n",
      "epoch no.31 train no.1510  loss = 5.33173 avg_loss = 5.63989\n",
      "epoch no.31 train no.1520  loss = 4.07291 avg_loss = 5.60436\n",
      "epoch no.31 train no.1530  loss = 5.44004 avg_loss = 5.59547\n",
      "epoch no.31 train no.1540  loss = 6.56180 avg_loss = 5.61031\n",
      "epoch no.31 train no.1550  loss = 4.50267 avg_loss = 5.57746\n",
      "epoch no.31 train no.1560  loss = 5.57347 avg_loss = 5.56613\n",
      "epoch no.31 train no.1570  loss = 6.96912 avg_loss = 5.58749\n",
      "epoch no.31 train no.1580  loss = 4.53923 avg_loss = 5.55309\n",
      "epoch no.31 train no.1590  loss = 6.00771 avg_loss = 5.61166\n",
      "epoch no.31 train no.1600  loss = 5.06633 avg_loss = 5.62155\n",
      "epoch no.31 train no.1610  loss = 5.72302 avg_loss = 5.65130\n",
      "epoch no.31 train no.1620  loss = 6.16388 avg_loss = 5.64569\n",
      "epoch no.31 train no.1630  loss = 5.40495 avg_loss = 5.63351\n",
      "epoch no.31 train no.1640  loss = 6.33922 avg_loss = 5.67598\n",
      "epoch no.31 train no.1650  loss = 5.96044 avg_loss = 5.70676\n",
      "epoch no.31 train no.1660  loss = 3.63698 avg_loss = 5.70594\n",
      "epoch no.31 train no.1670  loss = 6.23304 avg_loss = 5.69915\n",
      "epoch no.31 train no.1680  loss = 5.93561 avg_loss = 5.67322\n",
      "epoch no.31 train no.1690  loss = 5.00919 avg_loss = 5.68203\n",
      "epoch no.31 train no.1700  loss = 5.58583 avg_loss = 5.67762\n",
      "epoch no.31 train no.1710  loss = 5.00351 avg_loss = 5.70759\n",
      "epoch no.31 train no.1720  loss = 6.04939 avg_loss = 5.70425\n",
      "epoch no.31 train no.1730  loss = 6.63621 avg_loss = 5.71472\n",
      "epoch no.31 train no.1740  loss = 6.51999 avg_loss = 5.66468\n",
      "epoch no.31 train no.1750  loss = 6.47540 avg_loss = 5.69196\n",
      "epoch no.32 train no.0  loss = 5.17636 avg_loss = 5.67799\n",
      "epoch no.32 train no.10  loss = 6.15967 avg_loss = 5.61658\n",
      "epoch no.32 train no.20  loss = 5.95975 avg_loss = 5.62795\n",
      "epoch no.32 train no.30  loss = 4.11508 avg_loss = 5.61713\n",
      "epoch no.32 train no.40  loss = 4.16651 avg_loss = 5.62124\n",
      "epoch no.32 train no.50  loss = 6.03523 avg_loss = 5.63234\n",
      "epoch no.32 train no.60  loss = 5.98774 avg_loss = 5.65183\n",
      "epoch no.32 train no.70  loss = 5.58463 avg_loss = 5.63955\n",
      "epoch no.32 train no.80  loss = 6.26894 avg_loss = 5.62341\n",
      "epoch no.32 train no.90  loss = 6.38605 avg_loss = 5.64739\n",
      "epoch no.32 train no.100  loss = 4.26329 avg_loss = 5.61839\n",
      "epoch no.32 train no.110  loss = 5.39383 avg_loss = 5.61926\n",
      "epoch no.32 train no.120  loss = 4.46925 avg_loss = 5.61375\n",
      "epoch no.32 train no.130  loss = 4.83533 avg_loss = 5.63855\n",
      "epoch no.32 train no.140  loss = 5.37038 avg_loss = 5.65900\n",
      "epoch no.32 train no.150  loss = 6.14676 avg_loss = 5.61772\n",
      "epoch no.32 train no.160  loss = 3.23133 avg_loss = 5.60520\n",
      "epoch no.32 train no.170  loss = 5.19390 avg_loss = 5.63308\n",
      "epoch no.32 train no.180  loss = 5.18334 avg_loss = 5.63840\n",
      "epoch no.32 train no.190  loss = 4.95384 avg_loss = 5.56937\n",
      "epoch no.32 train no.200  loss = 5.64417 avg_loss = 5.57069\n",
      "epoch no.32 train no.210  loss = 6.57674 avg_loss = 5.58605\n",
      "epoch no.32 train no.220  loss = 6.24110 avg_loss = 5.58881\n",
      "epoch no.32 train no.230  loss = 3.94748 avg_loss = 5.57899\n",
      "epoch no.32 train no.240  loss = 7.01770 avg_loss = 5.54811\n",
      "epoch no.32 train no.250  loss = 4.52001 avg_loss = 5.53869\n",
      "epoch no.32 train no.260  loss = 4.76600 avg_loss = 5.54598\n",
      "epoch no.32 train no.270  loss = 6.73194 avg_loss = 5.53703\n",
      "epoch no.32 train no.280  loss = 6.76308 avg_loss = 5.57480\n",
      "epoch no.32 train no.290  loss = 6.26256 avg_loss = 5.61336\n",
      "epoch no.32 train no.300  loss = 5.54830 avg_loss = 5.61823\n",
      "epoch no.32 train no.310  loss = 5.76936 avg_loss = 5.67254\n",
      "epoch no.32 train no.320  loss = 4.46043 avg_loss = 5.66497\n",
      "epoch no.32 train no.330  loss = 6.86959 avg_loss = 5.67504\n",
      "epoch no.32 train no.340  loss = 4.32455 avg_loss = 5.67956\n",
      "epoch no.32 train no.350  loss = 6.81339 avg_loss = 5.73093\n",
      "epoch no.32 train no.360  loss = 6.21747 avg_loss = 5.64936\n",
      "epoch no.32 train no.370  loss = 6.16160 avg_loss = 5.66759\n",
      "epoch no.32 train no.380  loss = 5.64349 avg_loss = 5.65436\n",
      "epoch no.32 train no.390  loss = 4.19555 avg_loss = 5.65016\n",
      "epoch no.32 train no.400  loss = 6.55073 avg_loss = 5.63820\n",
      "epoch no.32 train no.410  loss = 5.43398 avg_loss = 5.60550\n",
      "epoch no.32 train no.420  loss = 3.91446 avg_loss = 5.59530\n",
      "epoch no.32 train no.430  loss = 5.10438 avg_loss = 5.57479\n",
      "epoch no.32 train no.440  loss = 5.82730 avg_loss = 5.56757\n",
      "epoch no.32 train no.450  loss = 5.86396 avg_loss = 5.49907\n",
      "epoch no.32 train no.460  loss = 4.85204 avg_loss = 5.51292\n",
      "epoch no.32 train no.470  loss = 4.33886 avg_loss = 5.46775\n",
      "epoch no.32 train no.480  loss = 6.37816 avg_loss = 5.49941\n",
      "epoch no.32 train no.490  loss = 4.39750 avg_loss = 5.54520\n",
      "epoch no.32 train no.500  loss = 5.58001 avg_loss = 5.51290\n",
      "epoch no.32 train no.510  loss = 5.80394 avg_loss = 5.53189\n",
      "epoch no.32 train no.520  loss = 6.78220 avg_loss = 5.57649\n",
      "epoch no.32 train no.530  loss = 6.35718 avg_loss = 5.58493\n",
      "epoch no.32 train no.540  loss = 5.97835 avg_loss = 5.58834\n",
      "epoch no.32 train no.550  loss = 5.85035 avg_loss = 5.61072\n",
      "epoch no.32 train no.560  loss = 5.94666 avg_loss = 5.65946\n",
      "epoch no.32 train no.570  loss = 4.45887 avg_loss = 5.66443\n",
      "epoch no.32 train no.580  loss = 5.41705 avg_loss = 5.66180\n",
      "epoch no.32 train no.590  loss = 5.73513 avg_loss = 5.70188\n",
      "epoch no.32 train no.600  loss = 5.27141 avg_loss = 5.66411\n",
      "epoch no.32 train no.610  loss = 6.24288 avg_loss = 5.64807\n",
      "epoch no.32 train no.620  loss = 5.86901 avg_loss = 5.62579\n",
      "epoch no.32 train no.630  loss = 6.41205 avg_loss = 5.67099\n",
      "epoch no.32 train no.640  loss = 3.95113 avg_loss = 5.62029\n",
      "epoch no.32 train no.650  loss = 5.32392 avg_loss = 5.61792\n",
      "epoch no.32 train no.660  loss = 5.94238 avg_loss = 5.57638\n",
      "epoch no.32 train no.670  loss = 4.98701 avg_loss = 5.58349\n",
      "epoch no.32 train no.680  loss = 5.40891 avg_loss = 5.54934\n",
      "epoch no.32 train no.690  loss = 5.08696 avg_loss = 5.58053\n",
      "epoch no.32 train no.700  loss = 5.97017 avg_loss = 5.58681\n",
      "epoch no.32 train no.710  loss = 6.81131 avg_loss = 5.65101\n",
      "epoch no.32 train no.720  loss = 6.79100 avg_loss = 5.66276\n",
      "epoch no.32 train no.730  loss = 5.94525 avg_loss = 5.66921\n",
      "epoch no.32 train no.740  loss = 5.32398 avg_loss = 5.63489\n",
      "epoch no.32 train no.750  loss = 6.27067 avg_loss = 5.67059\n",
      "epoch no.32 train no.760  loss = 3.92102 avg_loss = 5.62012\n",
      "epoch no.32 train no.770  loss = 4.36490 avg_loss = 5.60051\n",
      "epoch no.32 train no.780  loss = 4.51751 avg_loss = 5.58932\n",
      "epoch no.32 train no.790  loss = 5.07262 avg_loss = 5.57218\n",
      "epoch no.32 train no.800  loss = 3.79610 avg_loss = 5.57911\n",
      "epoch no.32 train no.810  loss = 4.33414 avg_loss = 5.54519\n",
      "epoch no.32 train no.820  loss = 6.49511 avg_loss = 5.56073\n",
      "epoch no.32 train no.830  loss = 5.77441 avg_loss = 5.54274\n",
      "epoch no.32 train no.840  loss = 4.67879 avg_loss = 5.54887\n",
      "epoch no.32 train no.850  loss = 5.65557 avg_loss = 5.60806\n",
      "epoch no.32 train no.860  loss = 5.37685 avg_loss = 5.58020\n",
      "epoch no.32 train no.870  loss = 6.47650 avg_loss = 5.60423\n",
      "epoch no.32 train no.880  loss = 4.51274 avg_loss = 5.60351\n",
      "epoch no.32 train no.890  loss = 5.42004 avg_loss = 5.59490\n",
      "epoch no.32 train no.900  loss = 4.88855 avg_loss = 5.58320\n",
      "epoch no.32 train no.910  loss = 6.50931 avg_loss = 5.54545\n",
      "epoch no.32 train no.920  loss = 7.19577 avg_loss = 5.60162\n",
      "epoch no.32 train no.930  loss = 6.16987 avg_loss = 5.60447\n",
      "epoch no.32 train no.940  loss = 5.73442 avg_loss = 5.62579\n",
      "epoch no.32 train no.950  loss = 5.81321 avg_loss = 5.66495\n",
      "epoch no.32 train no.960  loss = 5.62711 avg_loss = 5.64385\n",
      "epoch no.32 train no.970  loss = 4.24044 avg_loss = 5.60279\n",
      "epoch no.32 train no.980  loss = 6.18309 avg_loss = 5.59168\n",
      "epoch no.32 train no.990  loss = 5.22597 avg_loss = 5.60036\n",
      "epoch no.32 train no.1000  loss = 5.91214 avg_loss = 5.56184\n",
      "to_tokens: ['</s>', '</s>', '</s>']\n",
      "뭐함</s>\n",
      "epoch no.32 train no.1010  loss = 6.69900 avg_loss = 5.56782\n",
      "epoch no.32 train no.1020  loss = 5.73785 avg_loss = 5.57980\n",
      "epoch no.32 train no.1030  loss = 4.70068 avg_loss = 5.60825\n",
      "epoch no.32 train no.1040  loss = 5.71294 avg_loss = 5.61554\n",
      "epoch no.32 train no.1050  loss = 5.15685 avg_loss = 5.60051\n",
      "epoch no.32 train no.1060  loss = 6.38461 avg_loss = 5.57694\n",
      "epoch no.32 train no.1070  loss = 5.43576 avg_loss = 5.61400\n",
      "epoch no.32 train no.1080  loss = 6.02082 avg_loss = 5.62437\n",
      "epoch no.32 train no.1090  loss = 4.97563 avg_loss = 5.52507\n",
      "epoch no.32 train no.1100  loss = 5.95218 avg_loss = 5.53972\n",
      "epoch no.32 train no.1110  loss = 4.78809 avg_loss = 5.48199\n",
      "epoch no.32 train no.1120  loss = 5.84759 avg_loss = 5.48213\n",
      "epoch no.32 train no.1130  loss = 6.98117 avg_loss = 5.51675\n",
      "epoch no.32 train no.1140  loss = 6.59218 avg_loss = 5.57449\n",
      "epoch no.32 train no.1150  loss = 5.91644 avg_loss = 5.58687\n",
      "epoch no.32 train no.1160  loss = 5.54676 avg_loss = 5.60227\n",
      "epoch no.32 train no.1170  loss = 6.64431 avg_loss = 5.64393\n",
      "epoch no.32 train no.1180  loss = 5.48332 avg_loss = 5.65052\n",
      "epoch no.32 train no.1190  loss = 4.90513 avg_loss = 5.59694\n",
      "epoch no.32 train no.1200  loss = 5.43073 avg_loss = 5.62862\n",
      "epoch no.32 train no.1210  loss = 5.29572 avg_loss = 5.64636\n",
      "epoch no.32 train no.1220  loss = 5.03554 avg_loss = 5.67926\n",
      "epoch no.32 train no.1230  loss = 5.87093 avg_loss = 5.67137\n",
      "epoch no.32 train no.1240  loss = 4.78779 avg_loss = 5.62081\n",
      "epoch no.32 train no.1250  loss = 5.72310 avg_loss = 5.63539\n",
      "epoch no.32 train no.1260  loss = 5.46096 avg_loss = 5.63551\n",
      "epoch no.32 train no.1270  loss = 6.46302 avg_loss = 5.67152\n",
      "epoch no.32 train no.1280  loss = 5.05491 avg_loss = 5.59696\n",
      "epoch no.32 train no.1290  loss = 5.18306 avg_loss = 5.62095\n",
      "epoch no.32 train no.1300  loss = 5.42150 avg_loss = 5.64239\n",
      "epoch no.32 train no.1310  loss = 3.53952 avg_loss = 5.64297\n",
      "epoch no.32 train no.1320  loss = 4.00836 avg_loss = 5.67263\n",
      "epoch no.32 train no.1330  loss = 6.30322 avg_loss = 5.61955\n",
      "epoch no.32 train no.1340  loss = 5.19782 avg_loss = 5.63039\n",
      "epoch no.32 train no.1350  loss = 5.57121 avg_loss = 5.62458\n",
      "epoch no.32 train no.1360  loss = 5.42781 avg_loss = 5.58169\n",
      "epoch no.32 train no.1370  loss = 5.88631 avg_loss = 5.57669\n",
      "epoch no.32 train no.1380  loss = 7.38306 avg_loss = 5.62991\n",
      "epoch no.32 train no.1390  loss = 5.92595 avg_loss = 5.59047\n",
      "epoch no.32 train no.1400  loss = 6.61851 avg_loss = 5.56922\n",
      "epoch no.32 train no.1410  loss = 5.00034 avg_loss = 5.58668\n",
      "epoch no.32 train no.1420  loss = 7.48359 avg_loss = 5.58845\n",
      "epoch no.32 train no.1430  loss = 4.37650 avg_loss = 5.53332\n",
      "epoch no.32 train no.1440  loss = 5.75914 avg_loss = 5.50105\n",
      "epoch no.32 train no.1450  loss = 4.61139 avg_loss = 5.54291\n",
      "epoch no.32 train no.1460  loss = 7.05561 avg_loss = 5.57582\n",
      "epoch no.32 train no.1470  loss = 5.65705 avg_loss = 5.58474\n",
      "epoch no.32 train no.1480  loss = 4.93694 avg_loss = 5.62621\n",
      "epoch no.32 train no.1490  loss = 6.28193 avg_loss = 5.61626\n",
      "epoch no.32 train no.1500  loss = 4.42785 avg_loss = 5.58364\n",
      "epoch no.32 train no.1510  loss = 5.43418 avg_loss = 5.56313\n",
      "epoch no.32 train no.1520  loss = 6.32362 avg_loss = 5.55268\n",
      "epoch no.32 train no.1530  loss = 6.61067 avg_loss = 5.57278\n",
      "epoch no.32 train no.1540  loss = 5.44327 avg_loss = 5.57689\n",
      "epoch no.32 train no.1550  loss = 5.35997 avg_loss = 5.55256\n",
      "epoch no.32 train no.1560  loss = 5.42972 avg_loss = 5.54902\n",
      "epoch no.32 train no.1570  loss = 6.17088 avg_loss = 5.50766\n",
      "epoch no.32 train no.1580  loss = 4.90760 avg_loss = 5.53405\n",
      "epoch no.32 train no.1590  loss = 4.84688 avg_loss = 5.51158\n",
      "epoch no.32 train no.1600  loss = 6.26970 avg_loss = 5.53908\n",
      "epoch no.32 train no.1610  loss = 6.00946 avg_loss = 5.58464\n",
      "epoch no.32 train no.1620  loss = 4.52929 avg_loss = 5.60613\n",
      "epoch no.32 train no.1630  loss = 3.98843 avg_loss = 5.59921\n",
      "epoch no.32 train no.1640  loss = 5.15897 avg_loss = 5.57783\n",
      "epoch no.32 train no.1650  loss = 6.94453 avg_loss = 5.55921\n",
      "epoch no.32 train no.1660  loss = 5.50722 avg_loss = 5.62003\n",
      "epoch no.32 train no.1670  loss = 6.95822 avg_loss = 5.64977\n",
      "epoch no.32 train no.1680  loss = 5.03772 avg_loss = 5.63473\n",
      "epoch no.32 train no.1690  loss = 5.69038 avg_loss = 5.60079\n",
      "epoch no.32 train no.1700  loss = 3.81659 avg_loss = 5.61212\n",
      "epoch no.32 train no.1710  loss = 4.89285 avg_loss = 5.60670\n",
      "epoch no.32 train no.1720  loss = 5.07979 avg_loss = 5.62279\n",
      "epoch no.32 train no.1730  loss = 4.46874 avg_loss = 5.57827\n",
      "epoch no.32 train no.1740  loss = 6.77003 avg_loss = 5.59639\n",
      "epoch no.32 train no.1750  loss = 5.42812 avg_loss = 5.58513\n",
      "epoch no.33 train no.0  loss = 5.59761 avg_loss = 5.60223\n",
      "epoch no.33 train no.10  loss = 5.12042 avg_loss = 5.61402\n",
      "epoch no.33 train no.20  loss = 4.89763 avg_loss = 5.62629\n",
      "epoch no.33 train no.30  loss = 5.91325 avg_loss = 5.60011\n",
      "epoch no.33 train no.40  loss = 5.72960 avg_loss = 5.62387\n",
      "epoch no.33 train no.50  loss = 5.46322 avg_loss = 5.65346\n",
      "epoch no.33 train no.60  loss = 6.88651 avg_loss = 5.68241\n",
      "epoch no.33 train no.70  loss = 5.68916 avg_loss = 5.69404\n",
      "epoch no.33 train no.80  loss = 5.92665 avg_loss = 5.74746\n",
      "epoch no.33 train no.90  loss = 5.03540 avg_loss = 5.71402\n",
      "epoch no.33 train no.100  loss = 3.94448 avg_loss = 5.72085\n",
      "epoch no.33 train no.110  loss = 5.15344 avg_loss = 5.65493\n",
      "epoch no.33 train no.120  loss = 4.87128 avg_loss = 5.64550\n",
      "epoch no.33 train no.130  loss = 6.12242 avg_loss = 5.62187\n",
      "epoch no.33 train no.140  loss = 5.68966 avg_loss = 5.62137\n",
      "epoch no.33 train no.150  loss = 6.05718 avg_loss = 5.63440\n",
      "epoch no.33 train no.160  loss = 5.97951 avg_loss = 5.66622\n",
      "epoch no.33 train no.170  loss = 7.56017 avg_loss = 5.62948\n",
      "epoch no.33 train no.180  loss = 6.31987 avg_loss = 5.62469\n",
      "epoch no.33 train no.190  loss = 6.60855 avg_loss = 5.63900\n",
      "epoch no.33 train no.200  loss = 6.44646 avg_loss = 5.58347\n",
      "epoch no.33 train no.210  loss = 5.84261 avg_loss = 5.58414\n",
      "epoch no.33 train no.220  loss = 4.61894 avg_loss = 5.60013\n",
      "epoch no.33 train no.230  loss = 5.60056 avg_loss = 5.59004\n",
      "epoch no.33 train no.240  loss = 6.86547 avg_loss = 5.54167\n",
      "epoch no.33 train no.250  loss = 7.31504 avg_loss = 5.55557\n",
      "epoch no.33 train no.260  loss = 4.81527 avg_loss = 5.53543\n",
      "epoch no.33 train no.270  loss = 6.08774 avg_loss = 5.52872\n",
      "epoch no.33 train no.280  loss = 5.33604 avg_loss = 5.51865\n",
      "epoch no.33 train no.290  loss = 6.18581 avg_loss = 5.54693\n",
      "epoch no.33 train no.300  loss = 6.51905 avg_loss = 5.55256\n",
      "epoch no.33 train no.310  loss = 4.90364 avg_loss = 5.54568\n",
      "epoch no.33 train no.320  loss = 5.18589 avg_loss = 5.56517\n",
      "epoch no.33 train no.330  loss = 4.57979 avg_loss = 5.50769\n",
      "epoch no.33 train no.340  loss = 6.44793 avg_loss = 5.53474\n",
      "epoch no.33 train no.350  loss = 3.92514 avg_loss = 5.55404\n",
      "epoch no.33 train no.360  loss = 5.31382 avg_loss = 5.57211\n",
      "epoch no.33 train no.370  loss = 4.74804 avg_loss = 5.54412\n",
      "epoch no.33 train no.380  loss = 6.57793 avg_loss = 5.53829\n",
      "epoch no.33 train no.390  loss = 6.57081 avg_loss = 5.52342\n",
      "epoch no.33 train no.400  loss = 5.51131 avg_loss = 5.52907\n",
      "epoch no.33 train no.410  loss = 7.32802 avg_loss = 5.58666\n",
      "epoch no.33 train no.420  loss = 6.64513 avg_loss = 5.58576\n",
      "epoch no.33 train no.430  loss = 4.90703 avg_loss = 5.55939\n",
      "epoch no.33 train no.440  loss = 5.82268 avg_loss = 5.56071\n",
      "epoch no.33 train no.450  loss = 5.26202 avg_loss = 5.56137\n",
      "epoch no.33 train no.460  loss = 5.68499 avg_loss = 5.55623\n",
      "epoch no.33 train no.470  loss = 4.95305 avg_loss = 5.59137\n",
      "epoch no.33 train no.480  loss = 5.61507 avg_loss = 5.57850\n",
      "epoch no.33 train no.490  loss = 3.49200 avg_loss = 5.56319\n",
      "epoch no.33 train no.500  loss = 4.47434 avg_loss = 5.57370\n",
      "epoch no.33 train no.510  loss = 5.28822 avg_loss = 5.55872\n",
      "epoch no.33 train no.520  loss = 5.31937 avg_loss = 5.54013\n",
      "epoch no.33 train no.530  loss = 7.54757 avg_loss = 5.59266\n",
      "epoch no.33 train no.540  loss = 4.47689 avg_loss = 5.60432\n",
      "epoch no.33 train no.550  loss = 6.70146 avg_loss = 5.60363\n",
      "epoch no.33 train no.560  loss = 7.27700 avg_loss = 5.59529\n",
      "epoch no.33 train no.570  loss = 4.22025 avg_loss = 5.59671\n",
      "epoch no.33 train no.580  loss = 4.79084 avg_loss = 5.56114\n",
      "epoch no.33 train no.590  loss = 4.96217 avg_loss = 5.58199\n",
      "epoch no.33 train no.600  loss = 5.80330 avg_loss = 5.53780\n",
      "epoch no.33 train no.610  loss = 6.16149 avg_loss = 5.52617\n",
      "epoch no.33 train no.620  loss = 4.41637 avg_loss = 5.52996\n",
      "epoch no.33 train no.630  loss = 6.44753 avg_loss = 5.57442\n",
      "epoch no.33 train no.640  loss = 7.38221 avg_loss = 5.58584\n",
      "epoch no.33 train no.650  loss = 4.30855 avg_loss = 5.56863\n",
      "epoch no.33 train no.660  loss = 6.55631 avg_loss = 5.57258\n",
      "epoch no.33 train no.670  loss = 5.46544 avg_loss = 5.63032\n",
      "epoch no.33 train no.680  loss = 6.18517 avg_loss = 5.60989\n",
      "epoch no.33 train no.690  loss = 3.48452 avg_loss = 5.59985\n",
      "epoch no.33 train no.700  loss = 4.31132 avg_loss = 5.57685\n",
      "epoch no.33 train no.710  loss = 3.80168 avg_loss = 5.51076\n",
      "epoch no.33 train no.720  loss = 5.09639 avg_loss = 5.51841\n",
      "epoch no.33 train no.730  loss = 4.85784 avg_loss = 5.50516\n",
      "epoch no.33 train no.740  loss = 7.18651 avg_loss = 5.55506\n",
      "epoch no.33 train no.750  loss = 6.66242 avg_loss = 5.50265\n",
      "epoch no.33 train no.760  loss = 5.40970 avg_loss = 5.53629\n",
      "epoch no.33 train no.770  loss = 5.64255 avg_loss = 5.51076\n",
      "epoch no.33 train no.780  loss = 6.58634 avg_loss = 5.48276\n",
      "epoch no.33 train no.790  loss = 6.67190 avg_loss = 5.51280\n",
      "epoch no.33 train no.800  loss = 5.36820 avg_loss = 5.52591\n",
      "epoch no.33 train no.810  loss = 4.35725 avg_loss = 5.53929\n",
      "epoch no.33 train no.820  loss = 6.90853 avg_loss = 5.53367\n",
      "epoch no.33 train no.830  loss = 6.32616 avg_loss = 5.56436\n",
      "epoch no.33 train no.840  loss = 5.43616 avg_loss = 5.64674\n",
      "epoch no.33 train no.850  loss = 5.88803 avg_loss = 5.60574\n",
      "epoch no.33 train no.860  loss = 6.71501 avg_loss = 5.61957\n",
      "epoch no.33 train no.870  loss = 4.70244 avg_loss = 5.62971\n",
      "epoch no.33 train no.880  loss = 6.98167 avg_loss = 5.63727\n",
      "epoch no.33 train no.890  loss = 5.98334 avg_loss = 5.63837\n",
      "epoch no.33 train no.900  loss = 5.56742 avg_loss = 5.62929\n",
      "epoch no.33 train no.910  loss = 6.61590 avg_loss = 5.60548\n",
      "epoch no.33 train no.920  loss = 4.72483 avg_loss = 5.60829\n",
      "epoch no.33 train no.930  loss = 7.13915 avg_loss = 5.61914\n",
      "epoch no.33 train no.940  loss = 5.27606 avg_loss = 5.64674\n",
      "epoch no.33 train no.950  loss = 6.82544 avg_loss = 5.68090\n",
      "epoch no.33 train no.960  loss = 6.21570 avg_loss = 5.65802\n",
      "epoch no.33 train no.970  loss = 4.25119 avg_loss = 5.63521\n",
      "epoch no.33 train no.980  loss = 6.19288 avg_loss = 5.56857\n",
      "epoch no.33 train no.990  loss = 6.29503 avg_loss = 5.58917\n",
      "epoch no.33 train no.1000  loss = 5.75360 avg_loss = 5.57714\n",
      "to_tokens: ['</s>', '</s>', '</s>']\n",
      "뭐함</s>\n",
      "epoch no.33 train no.1010  loss = 6.65178 avg_loss = 5.57666\n",
      "epoch no.33 train no.1020  loss = 4.48614 avg_loss = 5.57957\n",
      "epoch no.33 train no.1030  loss = 6.46101 avg_loss = 5.64904\n",
      "epoch no.33 train no.1040  loss = 6.73908 avg_loss = 5.65633\n",
      "epoch no.33 train no.1050  loss = 4.45573 avg_loss = 5.64865\n",
      "epoch no.33 train no.1060  loss = 4.73997 avg_loss = 5.62128\n",
      "epoch no.33 train no.1070  loss = 5.25678 avg_loss = 5.60739\n",
      "epoch no.33 train no.1080  loss = 4.12722 avg_loss = 5.55089\n",
      "epoch no.33 train no.1090  loss = 5.28389 avg_loss = 5.61103\n",
      "epoch no.33 train no.1100  loss = 5.91167 avg_loss = 5.60262\n",
      "epoch no.33 train no.1110  loss = 5.45113 avg_loss = 5.58624\n",
      "epoch no.33 train no.1120  loss = 5.08460 avg_loss = 5.62521\n",
      "epoch no.33 train no.1130  loss = 6.14621 avg_loss = 5.62698\n",
      "epoch no.33 train no.1140  loss = 5.48127 avg_loss = 5.58830\n",
      "epoch no.33 train no.1150  loss = 6.37918 avg_loss = 5.55895\n",
      "epoch no.33 train no.1160  loss = 7.11105 avg_loss = 5.59805\n",
      "epoch no.33 train no.1170  loss = 5.58189 avg_loss = 5.59222\n",
      "epoch no.33 train no.1180  loss = 5.52168 avg_loss = 5.64156\n",
      "epoch no.33 train no.1190  loss = 5.74001 avg_loss = 5.66914\n",
      "epoch no.33 train no.1200  loss = 3.72927 avg_loss = 5.65258\n",
      "epoch no.33 train no.1210  loss = 5.18322 avg_loss = 5.66488\n",
      "epoch no.33 train no.1220  loss = 5.29637 avg_loss = 5.64273\n",
      "epoch no.33 train no.1230  loss = 5.58201 avg_loss = 5.66025\n",
      "epoch no.33 train no.1240  loss = 5.90002 avg_loss = 5.64692\n",
      "epoch no.33 train no.1250  loss = 7.06254 avg_loss = 5.66657\n",
      "epoch no.33 train no.1260  loss = 6.37137 avg_loss = 5.68836\n",
      "epoch no.33 train no.1270  loss = 6.01558 avg_loss = 5.67352\n",
      "epoch no.33 train no.1280  loss = 3.82003 avg_loss = 5.61626\n",
      "epoch no.33 train no.1290  loss = 4.89116 avg_loss = 5.63818\n",
      "epoch no.33 train no.1300  loss = 5.65569 avg_loss = 5.60307\n",
      "epoch no.33 train no.1310  loss = 5.38795 avg_loss = 5.58000\n",
      "epoch no.33 train no.1320  loss = 4.45913 avg_loss = 5.59825\n",
      "epoch no.33 train no.1330  loss = 7.02183 avg_loss = 5.61122\n",
      "epoch no.33 train no.1340  loss = 5.79944 avg_loss = 5.61428\n",
      "epoch no.33 train no.1350  loss = 5.62198 avg_loss = 5.56767\n",
      "epoch no.33 train no.1360  loss = 5.88336 avg_loss = 5.56844\n",
      "epoch no.33 train no.1370  loss = 5.75096 avg_loss = 5.60751\n",
      "epoch no.33 train no.1380  loss = 6.30128 avg_loss = 5.59822\n",
      "epoch no.33 train no.1390  loss = 6.86762 avg_loss = 5.62597\n",
      "epoch no.33 train no.1400  loss = 6.36679 avg_loss = 5.63565\n",
      "epoch no.33 train no.1410  loss = 6.12594 avg_loss = 5.66728\n",
      "epoch no.33 train no.1420  loss = 5.80352 avg_loss = 5.66744\n",
      "epoch no.33 train no.1430  loss = 4.76159 avg_loss = 5.68645\n",
      "epoch no.33 train no.1440  loss = 4.34173 avg_loss = 5.65863\n",
      "epoch no.33 train no.1450  loss = 5.73648 avg_loss = 5.63243\n",
      "epoch no.33 train no.1460  loss = 4.83848 avg_loss = 5.65614\n",
      "epoch no.33 train no.1470  loss = 6.65987 avg_loss = 5.67007\n",
      "epoch no.33 train no.1480  loss = 6.20026 avg_loss = 5.66316\n",
      "epoch no.33 train no.1490  loss = 6.71360 avg_loss = 5.63726\n",
      "epoch no.33 train no.1500  loss = 6.35882 avg_loss = 5.64692\n",
      "epoch no.33 train no.1510  loss = 4.74429 avg_loss = 5.64765\n",
      "epoch no.33 train no.1520  loss = 3.88443 avg_loss = 5.61545\n",
      "epoch no.33 train no.1530  loss = 5.58164 avg_loss = 5.65502\n",
      "epoch no.33 train no.1540  loss = 6.47307 avg_loss = 5.68827\n",
      "epoch no.33 train no.1550  loss = 5.97500 avg_loss = 5.66721\n",
      "epoch no.33 train no.1560  loss = 5.48860 avg_loss = 5.61709\n",
      "epoch no.33 train no.1570  loss = 5.68739 avg_loss = 5.65094\n",
      "epoch no.33 train no.1580  loss = 6.90235 avg_loss = 5.68670\n",
      "epoch no.33 train no.1590  loss = 6.09406 avg_loss = 5.69391\n",
      "epoch no.33 train no.1600  loss = 6.84371 avg_loss = 5.65622\n",
      "epoch no.33 train no.1610  loss = 6.11567 avg_loss = 5.68855\n",
      "epoch no.33 train no.1620  loss = 6.19451 avg_loss = 5.69704\n",
      "epoch no.33 train no.1630  loss = 5.75812 avg_loss = 5.66744\n",
      "epoch no.33 train no.1640  loss = 5.85007 avg_loss = 5.65243\n",
      "epoch no.33 train no.1650  loss = 4.74772 avg_loss = 5.68382\n",
      "epoch no.33 train no.1660  loss = 4.94851 avg_loss = 5.69098\n",
      "epoch no.33 train no.1670  loss = 5.25534 avg_loss = 5.68545\n",
      "epoch no.33 train no.1680  loss = 5.76073 avg_loss = 5.68192\n",
      "epoch no.33 train no.1690  loss = 6.66853 avg_loss = 5.66802\n",
      "epoch no.33 train no.1700  loss = 5.61417 avg_loss = 5.68868\n",
      "epoch no.33 train no.1710  loss = 5.15555 avg_loss = 5.66490\n",
      "epoch no.33 train no.1720  loss = 6.42201 avg_loss = 5.66610\n",
      "epoch no.33 train no.1730  loss = 6.10576 avg_loss = 5.66935\n",
      "epoch no.33 train no.1740  loss = 5.25175 avg_loss = 5.66840\n",
      "epoch no.33 train no.1750  loss = 5.26051 avg_loss = 5.68408\n",
      "epoch no.34 train no.0  loss = 3.47736 avg_loss = 5.64893\n",
      "epoch no.34 train no.10  loss = 5.58122 avg_loss = 5.65827\n",
      "epoch no.34 train no.20  loss = 5.04769 avg_loss = 5.63030\n",
      "epoch no.34 train no.30  loss = 5.83657 avg_loss = 5.63863\n",
      "epoch no.34 train no.40  loss = 2.78263 avg_loss = 5.61143\n",
      "epoch no.34 train no.50  loss = 3.68217 avg_loss = 5.62393\n",
      "epoch no.34 train no.60  loss = 5.64662 avg_loss = 5.63162\n",
      "epoch no.34 train no.70  loss = 6.37479 avg_loss = 5.63600\n",
      "epoch no.34 train no.80  loss = 3.49141 avg_loss = 5.62398\n",
      "epoch no.34 train no.90  loss = 4.68481 avg_loss = 5.59979\n",
      "epoch no.34 train no.100  loss = 6.10383 avg_loss = 5.58224\n",
      "epoch no.34 train no.110  loss = 6.70511 avg_loss = 5.62609\n",
      "epoch no.34 train no.120  loss = 6.20434 avg_loss = 5.62687\n",
      "epoch no.34 train no.130  loss = 6.30441 avg_loss = 5.61168\n",
      "epoch no.34 train no.140  loss = 4.94924 avg_loss = 5.61969\n",
      "epoch no.34 train no.150  loss = 5.79354 avg_loss = 5.64002\n",
      "epoch no.34 train no.160  loss = 3.51749 avg_loss = 5.61874\n",
      "epoch no.34 train no.170  loss = 4.03126 avg_loss = 5.61801\n",
      "epoch no.34 train no.180  loss = 5.64350 avg_loss = 5.61447\n",
      "epoch no.34 train no.190  loss = 6.23218 avg_loss = 5.61494\n",
      "epoch no.34 train no.200  loss = 5.55272 avg_loss = 5.61442\n",
      "epoch no.34 train no.210  loss = 6.18521 avg_loss = 5.65147\n",
      "epoch no.34 train no.220  loss = 4.63335 avg_loss = 5.61374\n",
      "epoch no.34 train no.230  loss = 5.80800 avg_loss = 5.62449\n",
      "epoch no.34 train no.240  loss = 7.38375 avg_loss = 5.63086\n",
      "epoch no.34 train no.250  loss = 6.37977 avg_loss = 5.64096\n",
      "epoch no.34 train no.260  loss = 4.97453 avg_loss = 5.62447\n",
      "epoch no.34 train no.270  loss = 5.79715 avg_loss = 5.63618\n",
      "epoch no.34 train no.280  loss = 4.03603 avg_loss = 5.57433\n",
      "epoch no.34 train no.290  loss = 4.44906 avg_loss = 5.56286\n",
      "epoch no.34 train no.300  loss = 5.08904 avg_loss = 5.57978\n",
      "epoch no.34 train no.310  loss = 5.36195 avg_loss = 5.58010\n",
      "epoch no.34 train no.320  loss = 7.33831 avg_loss = 5.61297\n",
      "epoch no.34 train no.330  loss = 5.30037 avg_loss = 5.60928\n",
      "epoch no.34 train no.340  loss = 5.12650 avg_loss = 5.60937\n",
      "epoch no.34 train no.350  loss = 4.67635 avg_loss = 5.61506\n",
      "epoch no.34 train no.360  loss = 6.40870 avg_loss = 5.60499\n",
      "epoch no.34 train no.370  loss = 5.81098 avg_loss = 5.61815\n",
      "epoch no.34 train no.380  loss = 6.17085 avg_loss = 5.55750\n",
      "epoch no.34 train no.390  loss = 5.80907 avg_loss = 5.55257\n",
      "epoch no.34 train no.400  loss = 5.40633 avg_loss = 5.54472\n",
      "epoch no.34 train no.410  loss = 6.23791 avg_loss = 5.56318\n",
      "epoch no.34 train no.420  loss = 4.64076 avg_loss = 5.50328\n",
      "epoch no.34 train no.430  loss = 6.93398 avg_loss = 5.54814\n",
      "epoch no.34 train no.440  loss = 5.67757 avg_loss = 5.57871\n",
      "epoch no.34 train no.450  loss = 5.76814 avg_loss = 5.57652\n",
      "epoch no.34 train no.460  loss = 4.52637 avg_loss = 5.56693\n",
      "epoch no.34 train no.470  loss = 5.10298 avg_loss = 5.51775\n",
      "epoch no.34 train no.480  loss = 5.68071 avg_loss = 5.50036\n",
      "epoch no.34 train no.490  loss = 7.13145 avg_loss = 5.48785\n",
      "epoch no.34 train no.500  loss = 2.54871 avg_loss = 5.49359\n",
      "epoch no.34 train no.510  loss = 7.04497 avg_loss = 5.50532\n",
      "epoch no.34 train no.520  loss = 4.68913 avg_loss = 5.47131\n",
      "epoch no.34 train no.530  loss = 6.15218 avg_loss = 5.50739\n",
      "epoch no.34 train no.540  loss = 6.41593 avg_loss = 5.47741\n",
      "epoch no.34 train no.550  loss = 6.01978 avg_loss = 5.46998\n",
      "epoch no.34 train no.560  loss = 5.82080 avg_loss = 5.51001\n",
      "epoch no.34 train no.570  loss = 5.15854 avg_loss = 5.54924\n",
      "epoch no.34 train no.580  loss = 5.06950 avg_loss = 5.54462\n",
      "epoch no.34 train no.590  loss = 6.50827 avg_loss = 5.56847\n",
      "epoch no.34 train no.600  loss = 6.55546 avg_loss = 5.55167\n",
      "epoch no.34 train no.610  loss = 4.80625 avg_loss = 5.55580\n",
      "epoch no.34 train no.620  loss = 5.22083 avg_loss = 5.58505\n",
      "epoch no.34 train no.630  loss = 4.44723 avg_loss = 5.56346\n",
      "epoch no.34 train no.640  loss = 5.43552 avg_loss = 5.55816\n",
      "epoch no.34 train no.650  loss = 5.29504 avg_loss = 5.57090\n",
      "epoch no.34 train no.660  loss = 5.84681 avg_loss = 5.57385\n",
      "epoch no.34 train no.670  loss = 5.68023 avg_loss = 5.56673\n",
      "epoch no.34 train no.680  loss = 5.88582 avg_loss = 5.57295\n",
      "epoch no.34 train no.690  loss = 4.73422 avg_loss = 5.55360\n",
      "epoch no.34 train no.700  loss = 6.89680 avg_loss = 5.56978\n",
      "epoch no.34 train no.710  loss = 6.83963 avg_loss = 5.52509\n",
      "epoch no.34 train no.720  loss = 5.03379 avg_loss = 5.58010\n",
      "epoch no.34 train no.730  loss = 4.65152 avg_loss = 5.57661\n",
      "epoch no.34 train no.740  loss = 5.31256 avg_loss = 5.54890\n",
      "epoch no.34 train no.750  loss = 6.33229 avg_loss = 5.59524\n",
      "epoch no.34 train no.760  loss = 5.19233 avg_loss = 5.56007\n",
      "epoch no.34 train no.770  loss = 4.89632 avg_loss = 5.54690\n",
      "epoch no.34 train no.780  loss = 6.02670 avg_loss = 5.59394\n",
      "epoch no.34 train no.790  loss = 7.17498 avg_loss = 5.67132\n",
      "epoch no.34 train no.800  loss = 4.52058 avg_loss = 5.66631\n",
      "epoch no.34 train no.810  loss = 4.89683 avg_loss = 5.68377\n",
      "epoch no.34 train no.820  loss = 5.58525 avg_loss = 5.64009\n",
      "epoch no.34 train no.830  loss = 4.18965 avg_loss = 5.65186\n",
      "epoch no.34 train no.840  loss = 4.75090 avg_loss = 5.58977\n",
      "epoch no.34 train no.850  loss = 6.82253 avg_loss = 5.62477\n",
      "epoch no.34 train no.860  loss = 5.49190 avg_loss = 5.62122\n",
      "epoch no.34 train no.870  loss = 6.59269 avg_loss = 5.65765\n",
      "epoch no.34 train no.880  loss = 6.38465 avg_loss = 5.67518\n",
      "epoch no.34 train no.890  loss = 6.90390 avg_loss = 5.68770\n",
      "epoch no.34 train no.900  loss = 6.51507 avg_loss = 5.65623\n",
      "epoch no.34 train no.910  loss = 5.33626 avg_loss = 5.65915\n",
      "epoch no.34 train no.920  loss = 4.46999 avg_loss = 5.62153\n",
      "epoch no.34 train no.930  loss = 4.73769 avg_loss = 5.57552\n",
      "epoch no.34 train no.940  loss = 6.09254 avg_loss = 5.54822\n",
      "epoch no.34 train no.950  loss = 6.10988 avg_loss = 5.54696\n",
      "epoch no.34 train no.960  loss = 4.39695 avg_loss = 5.53646\n",
      "epoch no.34 train no.970  loss = 5.61205 avg_loss = 5.57539\n",
      "epoch no.34 train no.980  loss = 4.51964 avg_loss = 5.54509\n",
      "epoch no.34 train no.990  loss = 7.00791 avg_loss = 5.57095\n",
      "epoch no.34 train no.1000  loss = 6.87887 avg_loss = 5.58135\n",
      "to_tokens: ['</s>', '</s>', '</s>']\n",
      "뭐함</s>\n",
      "epoch no.34 train no.1010  loss = 6.90181 avg_loss = 5.61997\n",
      "epoch no.34 train no.1020  loss = 5.39119 avg_loss = 5.64396\n",
      "epoch no.34 train no.1030  loss = 5.75671 avg_loss = 5.66630\n",
      "epoch no.34 train no.1040  loss = 3.83386 avg_loss = 5.60439\n",
      "epoch no.34 train no.1050  loss = 5.87726 avg_loss = 5.60151\n",
      "epoch no.34 train no.1060  loss = 5.09971 avg_loss = 5.61876\n",
      "epoch no.34 train no.1070  loss = 5.32753 avg_loss = 5.60785\n",
      "epoch no.34 train no.1080  loss = 6.56964 avg_loss = 5.62971\n",
      "epoch no.34 train no.1090  loss = 5.56567 avg_loss = 5.66158\n",
      "epoch no.34 train no.1100  loss = 5.09223 avg_loss = 5.62444\n",
      "epoch no.34 train no.1110  loss = 4.76672 avg_loss = 5.60451\n",
      "epoch no.34 train no.1120  loss = 5.57627 avg_loss = 5.60679\n",
      "epoch no.34 train no.1130  loss = 4.77984 avg_loss = 5.64926\n",
      "epoch no.34 train no.1140  loss = 6.08770 avg_loss = 5.66850\n",
      "epoch no.34 train no.1150  loss = 5.28977 avg_loss = 5.62597\n",
      "epoch no.34 train no.1160  loss = 6.64395 avg_loss = 5.64220\n",
      "epoch no.34 train no.1170  loss = 4.65878 avg_loss = 5.60428\n",
      "epoch no.34 train no.1180  loss = 5.98494 avg_loss = 5.62920\n",
      "epoch no.34 train no.1190  loss = 5.83798 avg_loss = 5.58149\n",
      "epoch no.34 train no.1200  loss = 5.36038 avg_loss = 5.60221\n",
      "epoch no.34 train no.1210  loss = 6.17251 avg_loss = 5.58652\n",
      "epoch no.34 train no.1220  loss = 4.97688 avg_loss = 5.60204\n",
      "epoch no.34 train no.1230  loss = 5.07563 avg_loss = 5.59347\n",
      "epoch no.34 train no.1240  loss = 6.69119 avg_loss = 5.59574\n",
      "epoch no.34 train no.1250  loss = 7.39328 avg_loss = 5.62824\n",
      "epoch no.34 train no.1260  loss = 4.19531 avg_loss = 5.60537\n",
      "epoch no.34 train no.1270  loss = 4.77532 avg_loss = 5.59759\n",
      "epoch no.34 train no.1280  loss = 5.46341 avg_loss = 5.59748\n",
      "epoch no.34 train no.1290  loss = 4.69563 avg_loss = 5.57293\n",
      "epoch no.34 train no.1300  loss = 6.14832 avg_loss = 5.57653\n",
      "epoch no.34 train no.1310  loss = 7.11130 avg_loss = 5.57237\n",
      "epoch no.34 train no.1320  loss = 6.14837 avg_loss = 5.58353\n",
      "epoch no.34 train no.1330  loss = 5.10430 avg_loss = 5.53491\n",
      "epoch no.34 train no.1340  loss = 5.96025 avg_loss = 5.52114\n",
      "epoch no.34 train no.1350  loss = 4.74275 avg_loss = 5.47803\n",
      "epoch no.34 train no.1360  loss = 5.50043 avg_loss = 5.47271\n",
      "epoch no.34 train no.1370  loss = 6.01633 avg_loss = 5.46711\n",
      "epoch no.34 train no.1380  loss = 5.75112 avg_loss = 5.48486\n",
      "epoch no.34 train no.1390  loss = 7.45661 avg_loss = 5.51779\n",
      "epoch no.34 train no.1400  loss = 7.56958 avg_loss = 5.54623\n",
      "epoch no.34 train no.1410  loss = 6.13786 avg_loss = 5.56563\n",
      "epoch no.34 train no.1420  loss = 6.74580 avg_loss = 5.60505\n",
      "epoch no.34 train no.1430  loss = 4.83517 avg_loss = 5.60245\n",
      "epoch no.34 train no.1440  loss = 4.53003 avg_loss = 5.53308\n",
      "epoch no.34 train no.1450  loss = 6.14074 avg_loss = 5.49448\n",
      "epoch no.34 train no.1460  loss = 4.98702 avg_loss = 5.48099\n",
      "epoch no.34 train no.1470  loss = 5.01149 avg_loss = 5.47267\n",
      "epoch no.34 train no.1480  loss = 5.32913 avg_loss = 5.50224\n",
      "epoch no.34 train no.1490  loss = 6.34883 avg_loss = 5.51504\n",
      "epoch no.34 train no.1500  loss = 6.38110 avg_loss = 5.52015\n",
      "epoch no.34 train no.1510  loss = 6.06648 avg_loss = 5.53921\n",
      "epoch no.34 train no.1520  loss = 5.79208 avg_loss = 5.54664\n",
      "epoch no.34 train no.1530  loss = 4.88850 avg_loss = 5.51426\n",
      "epoch no.34 train no.1540  loss = 6.86891 avg_loss = 5.55492\n",
      "epoch no.34 train no.1550  loss = 7.02552 avg_loss = 5.53857\n",
      "epoch no.34 train no.1560  loss = 3.92915 avg_loss = 5.53021\n",
      "epoch no.34 train no.1570  loss = 4.78209 avg_loss = 5.56162\n",
      "epoch no.34 train no.1580  loss = 7.76251 avg_loss = 5.59994\n",
      "epoch no.34 train no.1590  loss = 5.94613 avg_loss = 5.63056\n",
      "epoch no.34 train no.1600  loss = 4.64520 avg_loss = 5.62235\n",
      "epoch no.34 train no.1610  loss = 7.40281 avg_loss = 5.59970\n",
      "epoch no.34 train no.1620  loss = 5.14756 avg_loss = 5.54013\n",
      "epoch no.34 train no.1630  loss = 4.32600 avg_loss = 5.55164\n",
      "epoch no.34 train no.1640  loss = 5.58430 avg_loss = 5.58253\n",
      "epoch no.34 train no.1650  loss = 5.03221 avg_loss = 5.56514\n",
      "epoch no.34 train no.1660  loss = 5.71630 avg_loss = 5.60451\n",
      "epoch no.34 train no.1670  loss = 4.52174 avg_loss = 5.56660\n",
      "epoch no.34 train no.1680  loss = 5.43891 avg_loss = 5.58600\n",
      "epoch no.34 train no.1690  loss = 4.83395 avg_loss = 5.60660\n",
      "epoch no.34 train no.1700  loss = 5.62680 avg_loss = 5.56845\n",
      "epoch no.34 train no.1710  loss = 4.70824 avg_loss = 5.53858\n",
      "epoch no.34 train no.1720  loss = 6.00981 avg_loss = 5.54844\n",
      "epoch no.34 train no.1730  loss = 5.62285 avg_loss = 5.56767\n",
      "epoch no.34 train no.1740  loss = 6.13138 avg_loss = 5.56796\n",
      "epoch no.34 train no.1750  loss = 5.70272 avg_loss = 5.57059\n",
      "epoch no.35 train no.0  loss = 5.90138 avg_loss = 5.57249\n",
      "epoch no.35 train no.10  loss = 6.24361 avg_loss = 5.49418\n",
      "epoch no.35 train no.20  loss = 6.74562 avg_loss = 5.52095\n",
      "epoch no.35 train no.30  loss = 4.70496 avg_loss = 5.51555\n",
      "epoch no.35 train no.40  loss = 5.07383 avg_loss = 5.51382\n",
      "epoch no.35 train no.50  loss = 4.21326 avg_loss = 5.55118\n",
      "epoch no.35 train no.60  loss = 6.14752 avg_loss = 5.51238\n",
      "epoch no.35 train no.70  loss = 6.15652 avg_loss = 5.56185\n",
      "epoch no.35 train no.80  loss = 5.49801 avg_loss = 5.53172\n",
      "epoch no.35 train no.90  loss = 4.62581 avg_loss = 5.50722\n",
      "epoch no.35 train no.100  loss = 5.00884 avg_loss = 5.55504\n",
      "epoch no.35 train no.110  loss = 4.47524 avg_loss = 5.54832\n",
      "epoch no.35 train no.120  loss = 4.43843 avg_loss = 5.53781\n",
      "epoch no.35 train no.130  loss = 4.91907 avg_loss = 5.53198\n",
      "epoch no.35 train no.140  loss = 4.58974 avg_loss = 5.49526\n",
      "epoch no.35 train no.150  loss = 7.34289 avg_loss = 5.51627\n",
      "epoch no.35 train no.160  loss = 4.76566 avg_loss = 5.47560\n",
      "epoch no.35 train no.170  loss = 4.99679 avg_loss = 5.43554\n",
      "epoch no.35 train no.180  loss = 6.48352 avg_loss = 5.44522\n",
      "epoch no.35 train no.190  loss = 6.89828 avg_loss = 5.49999\n",
      "epoch no.35 train no.200  loss = 4.01530 avg_loss = 5.51497\n",
      "epoch no.35 train no.210  loss = 5.63878 avg_loss = 5.54960\n",
      "epoch no.35 train no.220  loss = 5.01876 avg_loss = 5.52805\n",
      "epoch no.35 train no.230  loss = 4.66818 avg_loss = 5.54926\n",
      "epoch no.35 train no.240  loss = 4.82354 avg_loss = 5.55854\n",
      "epoch no.35 train no.250  loss = 5.69662 avg_loss = 5.60737\n",
      "epoch no.35 train no.260  loss = 6.36470 avg_loss = 5.65357\n",
      "epoch no.35 train no.270  loss = 6.41919 avg_loss = 5.64597\n",
      "epoch no.35 train no.280  loss = 5.69045 avg_loss = 5.59871\n",
      "epoch no.35 train no.290  loss = 5.48052 avg_loss = 5.58954\n",
      "epoch no.35 train no.300  loss = 6.90558 avg_loss = 5.58969\n",
      "epoch no.35 train no.310  loss = 4.61489 avg_loss = 5.58213\n",
      "epoch no.35 train no.320  loss = 5.37165 avg_loss = 5.56871\n",
      "epoch no.35 train no.330  loss = 6.47935 avg_loss = 5.57532\n",
      "epoch no.35 train no.340  loss = 5.76445 avg_loss = 5.57427\n",
      "epoch no.35 train no.350  loss = 4.32055 avg_loss = 5.59785\n",
      "epoch no.35 train no.360  loss = 4.97641 avg_loss = 5.58003\n",
      "epoch no.35 train no.370  loss = 5.39394 avg_loss = 5.55369\n",
      "epoch no.35 train no.380  loss = 5.44467 avg_loss = 5.55229\n",
      "epoch no.35 train no.390  loss = 4.89660 avg_loss = 5.52781\n",
      "epoch no.35 train no.400  loss = 5.66081 avg_loss = 5.54155\n",
      "epoch no.35 train no.410  loss = 5.79343 avg_loss = 5.55006\n",
      "epoch no.35 train no.420  loss = 6.10424 avg_loss = 5.59196\n",
      "epoch no.35 train no.430  loss = 5.65945 avg_loss = 5.59694\n",
      "epoch no.35 train no.440  loss = 5.23275 avg_loss = 5.60678\n",
      "epoch no.35 train no.450  loss = 5.52722 avg_loss = 5.59431\n",
      "epoch no.35 train no.460  loss = 5.16967 avg_loss = 5.61368\n",
      "epoch no.35 train no.470  loss = 5.78110 avg_loss = 5.61739\n",
      "epoch no.35 train no.480  loss = 6.17387 avg_loss = 5.64787\n",
      "epoch no.35 train no.490  loss = 5.57298 avg_loss = 5.65365\n",
      "epoch no.35 train no.500  loss = 5.46463 avg_loss = 5.68294\n",
      "epoch no.35 train no.510  loss = 5.40909 avg_loss = 5.66146\n",
      "epoch no.35 train no.520  loss = 6.55546 avg_loss = 5.65159\n",
      "epoch no.35 train no.530  loss = 4.81349 avg_loss = 5.63140\n",
      "epoch no.35 train no.540  loss = 6.68754 avg_loss = 5.65257\n",
      "epoch no.35 train no.550  loss = 6.14174 avg_loss = 5.68176\n",
      "epoch no.35 train no.560  loss = 6.24377 avg_loss = 5.67457\n",
      "epoch no.35 train no.570  loss = 6.09503 avg_loss = 5.65244\n",
      "epoch no.35 train no.580  loss = 5.88995 avg_loss = 5.66676\n",
      "epoch no.35 train no.590  loss = 4.65624 avg_loss = 5.64089\n",
      "epoch no.35 train no.600  loss = 6.86372 avg_loss = 5.62231\n",
      "epoch no.35 train no.610  loss = 4.69316 avg_loss = 5.58300\n",
      "epoch no.35 train no.620  loss = 4.64680 avg_loss = 5.57709\n",
      "epoch no.35 train no.630  loss = 6.62071 avg_loss = 5.59449\n",
      "epoch no.35 train no.640  loss = 6.46555 avg_loss = 5.58389\n",
      "epoch no.35 train no.650  loss = 5.98733 avg_loss = 5.57397\n",
      "epoch no.35 train no.660  loss = 6.47156 avg_loss = 5.57347\n",
      "epoch no.35 train no.670  loss = 5.31833 avg_loss = 5.55680\n",
      "epoch no.35 train no.680  loss = 5.67882 avg_loss = 5.56055\n",
      "epoch no.35 train no.690  loss = 6.83703 avg_loss = 5.58885\n",
      "epoch no.35 train no.700  loss = 5.25133 avg_loss = 5.59283\n",
      "epoch no.35 train no.710  loss = 4.53289 avg_loss = 5.59143\n",
      "epoch no.35 train no.720  loss = 5.03835 avg_loss = 5.58177\n",
      "epoch no.35 train no.730  loss = 5.56570 avg_loss = 5.55706\n",
      "epoch no.35 train no.740  loss = 5.35193 avg_loss = 5.55910\n",
      "epoch no.35 train no.750  loss = 4.43907 avg_loss = 5.52472\n",
      "epoch no.35 train no.760  loss = 6.55625 avg_loss = 5.56921\n",
      "epoch no.35 train no.770  loss = 6.61497 avg_loss = 5.60717\n",
      "epoch no.35 train no.780  loss = 5.75421 avg_loss = 5.65013\n",
      "epoch no.35 train no.790  loss = 7.09354 avg_loss = 5.66947\n",
      "epoch no.35 train no.800  loss = 5.41720 avg_loss = 5.64764\n",
      "epoch no.35 train no.810  loss = 4.63422 avg_loss = 5.66884\n",
      "epoch no.35 train no.820  loss = 5.72184 avg_loss = 5.69642\n",
      "epoch no.35 train no.830  loss = 5.22007 avg_loss = 5.66587\n",
      "epoch no.35 train no.840  loss = 5.41040 avg_loss = 5.61186\n",
      "epoch no.35 train no.850  loss = 4.85543 avg_loss = 5.60761\n",
      "epoch no.35 train no.860  loss = 6.07355 avg_loss = 5.63298\n",
      "epoch no.35 train no.870  loss = 6.87371 avg_loss = 5.62005\n",
      "epoch no.35 train no.880  loss = 5.52123 avg_loss = 5.67353\n",
      "epoch no.35 train no.890  loss = 5.61874 avg_loss = 5.65844\n",
      "epoch no.35 train no.900  loss = 5.26393 avg_loss = 5.62130\n",
      "epoch no.35 train no.910  loss = 6.15616 avg_loss = 5.66387\n",
      "epoch no.35 train no.920  loss = 6.60496 avg_loss = 5.66487\n",
      "epoch no.35 train no.930  loss = 6.56376 avg_loss = 5.64260\n",
      "epoch no.35 train no.940  loss = 4.61677 avg_loss = 5.58938\n",
      "epoch no.35 train no.950  loss = 4.75071 avg_loss = 5.57667\n",
      "epoch no.35 train no.960  loss = 4.86820 avg_loss = 5.55986\n",
      "epoch no.35 train no.970  loss = 5.46150 avg_loss = 5.58311\n",
      "epoch no.35 train no.980  loss = 6.35677 avg_loss = 5.55180\n",
      "epoch no.35 train no.990  loss = 5.55043 avg_loss = 5.56807\n",
      "epoch no.35 train no.1000  loss = 5.48355 avg_loss = 5.59479\n"
     ]
    }
   ],
   "source": [
    "    for epoch in range(epoch):\n",
    "        count = 0\n",
    "        for data in post_data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            data = torch.stack(data)\n",
    "            data = data.transpose(1,0)\n",
    "            data = data.to(ctx)\n",
    "            model = model.to(ctx)\n",
    "\n",
    "            outputs = model(data, labels=data)\n",
    "            loss, logits = outputs[:2]\n",
    "            loss = loss.to(ctx)\n",
    "            loss.backward()\n",
    "            avg_loss = (avg_loss[0] * 0.99 + loss, avg_loss[1] * 0.99 + 1.0)\n",
    "            optimizer.step()\n",
    "            if count % 10 == 0:\n",
    "                print('epoch no.{0} train no.{1}  loss = {2:.5f} avg_loss = {3:.5f}' . format(epoch, count, loss, avg_loss[0] / avg_loss[1]))\n",
    "\n",
    "            if (count > 0 and count % 1000 == 0) or (len(data) < batch_size):\n",
    "                sent = sample_sequence(model.to(\"cpu\"), sentencepieceTokenizer, vocab, sent=\"뭐함\", text_size=100, temperature=0.7, top_p=0.8, top_k=40)\n",
    "                sent = sent.replace(\"<unused0>\", \"\\n\")\n",
    "                sent = auto_enter(sent)\n",
    "                print(sent)\n",
    "\n",
    "                if count > 500000:\n",
    "                    now = [int(n) for n in os.listdir(samples)]\n",
    "                    now = max(now)\n",
    "                    f = open(samples + str(now + 1), 'w', encoding=\"utf-8\")\n",
    "                    f.write(sent)\n",
    "                    f.close()\n",
    "            count += 1\n",
    "\n",
    "            if (count > 0 and count % 10000 == 0) or (len(data) < batch_size):\n",
    "                try:\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'train_no': count,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'loss': loss\n",
    "                    }, save_path+'korean_post_generator_checkpoint.tar')\n",
    "                except:\n",
    "                    pass"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "gpt2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
