{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposal\n",
    "Ian Park\n",
    "\n",
    "**Introduction**\n",
    "\n",
    "\n",
    "Ever since I watched Microsoft's Twitter bot, Tay, I was intrigued by AIs that can seamlessly imitate Humans. Although Tay was somewhat of a disaster causing Microsoft to pull it down shortly, the fact that it could generate a personality that represented a certain community based on various text responses was surprising. For this project, I wanted to do exactly that. I wanted to create models that generated text given a data set of posts from certain online communities.\n",
    "\n",
    "**Comparison of approaches**\n",
    "\n",
    "\n",
    "*Recurrent Neural Networks vs Markov Chain*\n",
    "\n",
    "There are multiple ways that large tech companies like Microsoft or Google generate seamless natural text. On the other hand, there is also a way to achieve this with Markov Chains without neural networks. As we've seen when implementing a variation in CS212, although fast, these are not so convincing. It turns out that they are good enough in the short term with smaller data but not so good compared to neural networks in the long term (https://pub.uni-bielefeld.de/download/2903474/2907910). Since I do plan on attaining large enough data, I will use neural networks to implement my text.\n",
    "\n",
    "*Recurrent Neural Networks vs Transformers*\n",
    "\n",
    "\n",
    "There is even a huge collection of general, modern, state-of-the-art NLG (Natural Language Generation) models that even do the pre-training for you (https://github.com/huggingface/transformers). These implementations are built on a completely different architecture called transformers, which eliminate recurrence and convolution to replace them with personal attention to establish dependencies between inputs and outputs (https://medium.com/analytics-vidhya/transformer-vs-rnn-and-cnn-18eeefa3602b). They seem to give the most realistic results, hence most widely used in recent systems. However, coming up with transformer models seemed to require much more research and was way out of the scope of this class. So, I've decided to go the more traditional route of generating texts using LSTM RNNs. To be specific, I will be implementing a character-level RNNs.\n",
    "\n",
    "*RNN with LSTM*\n",
    "\n",
    "Recurrent Neural Networks are powerful for modeling sequence data such as time series or natural language. Unlike Vanilla NNs and CNNs, RNNs combine the input vector with their state vector with a learned function to produce a new state vector. This gives RNNs the power to process sequential data well. However, traditional RNN or \"vanilla\" RNN suffers from the \"vanishing gradient\", in which the model cannot remember too far back in time (Sherstinsky). LSTM (Long Short Term Memory) networks can solve this by having cells and gates to emulate memory over arbitrary time intervals. This allows the model to remember what happened many time-steps ago. Overall, RNNs can learn how to generate languages because languages are sequential.\n",
    "\n",
    "**Tools and Implementation**\n",
    "\n",
    "For the implementation of the model with Keras, I will reference Chollet's notebook on LSTMs: https://nbviewer.jupyter.org/github/fchollet/deep-learning-with-python-notebooks/blob/master/8.1-text-generation-with-lstm.ipynb. I will also reference these tutorials that go more in-depth on character-level LSTM RNNs and apply them to working services: https://karpathy.github.io/2015/05/21/rnn-effectiveness/ and https://github.com/beneyal/data-science-rnn/blob/master/LSTM%20with%20Twitter%20API.md. \n",
    "\n",
    "I've received feedback on my previous proposal that it could be difficult to get a good data set. However, since the goal of my project is to generate text that is inherent to certain internet communities, I did not choose to get some text data from sites Kaggle. At the time of writing my proposal, I have successfully built a web scraper with the python library, Scrapy, to scrape text from the post on a couple of target sites. Similarly, one site, Reddit, had a separate API for its content, so I used that too. The number of posts to be scraped can be arbitrarily set, and there are plenty of new posts every minute. Furthermore, I have implemented basic automated data mapping for the text corpora similar to how the tutorials do it (One-Hot encoding). \n",
    "\n",
    "Now, I will have to implement and train the model with various datasets. And lastly, I will automate the uploading of the generated text to the sites.\n",
    "\n",
    "**Stretch Goals**\n",
    "\n",
    "One of my other goals was to have support for exploring Korean sites. This seems doable through separate Korean natural language processing libraries, which I found through this link: https://github.com/insikk/awesome-korean-nlp. However, this is not a core outcome. Likewise, if time allows, I would like to have a website that could interface my model and display results more nicely. The website could even make different architecture models be drop-in replacements to test them out and compare results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
